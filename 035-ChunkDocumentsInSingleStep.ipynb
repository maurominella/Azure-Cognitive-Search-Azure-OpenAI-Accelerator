{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58562652",
   "metadata": {},
   "source": [
    "# From files to separated chunk documents... in a single step\n",
    "\n",
    "In Notebook #1 we learnt how to create an index in Azure AI Search and leverage its \"pull\" ability to extract data from files located in storage account. As we saw, this \"extract and fill\" tecnique can be integrated by a \"skillset\" that \"enriches\" extracted data before writing them into the index. Such process is orchestrated by a so called \"indexer\" that maps each source or enriched piece of information against the proper field of the target index.<br/>\n",
    "If we need to split a long text field -typically, the *content* field- in chunks no longer than a predefined length -e.g. 5,000 characters-, we can leverage the **SplitSkill** skill of the skillset, which generates a multi-value field -*Collection(Edm.String)* type- containing the different chunks.<br/>\n",
    "This means, on the other hand, that if we need to retrieve the best ***chunks*** rather than the ***whole content*** of documents due to OpenAI token limits, or just because the whole document contains too much and potentially more confusing information, then we need to create a **secondary index** where we store **one chunk per index row**, possibly associating  specific embeddings, key phrases, emails/urls/persons and other entities to each search document (=chunk).<br/>\n",
    "That's exactly what we did in our Notebook #3; however, in this case we had to face with a few main challenges:\n",
    "- Filling the second index requires a set of manual steps, which we easily wrote in Python, but which creates a potential bottleneck when our scenario needs maintenance and scalability. For example, we had to manually vectorize each chunk and manually *push* the new search document into the secondary index\n",
    "- At the end we get two indexes to manage, where the first one becomes potentially useless\n",
    "- We need to define rules to keep both indexes in sync, and identify update policies. For example, in Notebook #3 we decided to *project and vectorize* only the chunks extracted by a first query, leaving the remaning chunks in their arrays within the first index.\n",
    "\n",
    "These were design decision led by some practical considerations and the general skill-up objective of this solution accelerator; however, they were also forced by some limits associated to Azure AI Search, *until a few days ago*.<br/>\n",
    "Yes, because with the release of the [latest stable version 2023-11-01 of Azure AI Search REST API](https://learn.microsoft.com/en-us/rest/api/searchservice/search-service-api-versions#stable-versions), we can now operate all the above actions with the following benefits:\n",
    "- **pull mathod**, e.g. no manual steps to upload search document to target index\n",
    "- **automatic embedding creation**, that can be performed by a specific skill\n",
    "- **automatic projection** of the array values into separated search documents of the target index\n",
    "\n",
    "As a matter of fact, we actually do not need two indexes any more. **Let's see how to do it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25369fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and assign variables\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "# Name of the container in your Blob Storage Datasource ( in credentials.env)\n",
    "BLOB_CONTAINER_NAME = \"arxivcs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22ae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for the data source, skillset, index and indexer\n",
    "datasource_name = \"cogsrch-datasource-files-onestep\"\n",
    "skillset_name   = \"cogsrch-skillset-files-onestep\"\n",
    "index_name      = \"cogsrch-index-files-vector-onestep\"\n",
    "indexer_name    = \"cogsrch-indexer-files-onestep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c260e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers     = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params      = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}\n",
    "params_old  = {'api-version': os.environ['AZURE_SEARCH_API_VERSION_OLD']} # needed for skillset creation with projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1bb04",
   "metadata": {},
   "source": [
    "## Create Data Source (Blob container with the Arxiv CS pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc983da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# The following code sends the json paylod to Azure Search engine to create the Datasource\n",
    "\n",
    "datasource_payload = {\n",
    "    \"name\": datasource_name,\n",
    "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
    "    \"type\": \"azureblob\",\n",
    "    \"credentials\": {\n",
    "        \"connectionString\": os.environ['BLOB_CONNECTION_STRING']\n",
    "    },\n",
    "    \"dataDeletionDetectionPolicy\" : {\n",
    "        \"@odata.type\" :\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\" # this makes sure that if the item is deleted from the source, it will be deleted from the index\n",
    "    },\n",
    "    \"container\": {\n",
    "        \"name\": BLOB_CONTAINER_NAME\n",
    "    }\n",
    "}\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/datasources/\" + datasource_name,\n",
    "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6d6df",
   "metadata": {},
   "source": [
    "## Create the *SINGLE* Index\n",
    "Since a [vectorizer](https://learn.microsoft.com/en-us/rest/api/searchservice/indexes/create?view=rest-searchservice-2023-10-01-preview&tabs=HTTP#vectorsearch) is used here, we need to use API 2023-10-01-Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f188d775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the single index\n",
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"fields\": [        \n",
    "        {\n",
    "          \"name\": \"id\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"key\": \"true\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\",\n",
    "          \"analyzer\": \"keyword\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"ParentKey\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"facetable\": \"false\",\n",
    "          \"filterable\": \"true\",\n",
    "          \"sortable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"title\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"chunk\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"name\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"location\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"false\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"language\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"true\",\n",
    "          \"filterable\": \"true\",\n",
    "          \"facetable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"persons\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"urls\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"emails\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"key_phrases\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"chunkVector\",\n",
    "          \"type\": \"Collection(Edm.Single)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"dimensions\": 1536,\n",
    "          \"vectorSearchProfile\": \"my_vectorSearch_profile\"\n",
    "        }\n",
    "    ],    \n",
    "  \n",
    "    \"vectorSearch\": {        \n",
    "        \"profiles\": [\n",
    "            {                \n",
    "                \"name\": \"my_vectorSearch_profile\",\n",
    "                \"algorithm\": \"my-vectorSearch-algorithm\",\n",
    "                \"vectorizer\": \"my-embeddings-vectorizer\"\n",
    "            }\n",
    "        ],        \n",
    "        \"algorithms\": [            \n",
    "            {\n",
    "                \"name\": \"my-vectorSearch-algorithm\",\n",
    "                \"kind\": \"hnsw\",\n",
    "                \"hnswParameters\": {                    \n",
    "                    \"m\": 4,\n",
    "                    \"metric\": \"cosine\",\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500\n",
    "                }          \n",
    "            }\n",
    "        ],        \n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"my-embeddings-vectorizer\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\": {\n",
    "                \"resourceUri\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "                \"apiKey\": os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                \"deploymentId\": os.environ['EMBEDDING_DEPLOYMENT']\n",
    "                }\n",
    "            }\n",
    "        ]        \n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params_old)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fef1b",
   "metadata": {},
   "source": [
    "## Create the Skillset\n",
    "### Please note, in this case we need to use version 2023-10-01-Preview of Azure AI Search REST API which currently offers the projection feature\n",
    "- [Reference guide version 2023-10-01-Preview](https://learn.microsoft.com/en-us/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-10-01-Preview&tabs=HTTP)\n",
    "- [Reference guide latest version (2023-11-01)](https://learn.microsoft.com/en-us/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-11-01&tabs=HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e1be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the skillset, including the projection settings into the single index created above\n",
    "skillset_payload = {    \n",
    "  \"name\": skillset_name,\n",
    "  \"description\": \"Detect language, run OCR, merge content+ocr_text, split in chunks, extract entities and key-phrases and embed the chunks\",\n",
    "  \"skills\": [\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
    "      \"name\": \"LanguageDetectionSkill\",\n",
    "      \"context\": \"/document\",\n",
    "      \"description\": \"If you have multilingual content, adding a language code is useful for filtering\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/content\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"targetName\": \"language\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
    "      \"name\": \"OcrSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/normalized_images/*\",\n",
    "      \"textExtractionAlgorithm\": \"\",\n",
    "      \"lineEnding\": \"Space\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"detectOrientation\": \"true\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"image\",\n",
    "          \"source\": \"/document/normalized_images/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"targetName\": \"text_from_ocr\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
    "      \"name\": \"MergeSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document\",\n",
    "      \"insertPreTag\": \" \",\n",
    "      \"insertPostTag\": \" \",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/content\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"itemsToInsert\",\n",
    "          \"source\": \"/document/normalized_images/*/text_from_ocr\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"offsets\",\n",
    "          \"source\": \"/document/normalized_images/*/contentOffset\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"mergedText\",\n",
    "          \"targetName\": \"merged_text\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
    "      \"name\": \"SplitSkill\",\n",
    "      \"context\": \"/document\",\n",
    "      \"textSplitMode\": \"pages\",\n",
    "      \"maximumPageLength\": 5000,\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/merged_text\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"source\": \"/document/language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"textItems\",\n",
    "          \"targetName\": \"chunks\"\n",
    "        }        \n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
    "      \"name\": \"LanguageDetectionSkill_by_chunk\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"description\": \"If you have multilingual content, adding a language code is useful for filtering\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"targetName\": \"chunk_language\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.V3.EntityRecognitionSkill\",\n",
    "      \"name\": \"EntityRecognitionSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"minimumPrecision\": 0.5,\n",
    "      \"modelVersion\": \"\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"languageCode\", \n",
    "            \"source\": \"/document/chunks/*/chunk_language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"persons\",\n",
    "          \"targetName\": \"persons\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"urls\",\n",
    "          \"targetName\": \"urls\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"emails\",\n",
    "          \"targetName\": \"emails\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.KeyPhraseExtractionSkill\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"modelVersion\": \"\",\n",
    "      \"name\": \"KeyPhraseExtractionSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"source\": \"/document/chunks/*/chunk_language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"keyPhrases\",\n",
    "          \"targetName\": \"key_phrases\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
    "      \"name\": \"AzureOpenAIEmbeddingSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"resourceUri\": \"https://mmopenai04.openai.azure.com\",\n",
    "      \"apiKey\": \"23b1db9a7f3a4eeb8a4f1c74bdd7d13d\",\n",
    "      \"deploymentId\": \"text-embedding-ada-002\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"embedding\",\n",
    "          \"targetName\": \"chunk_embedded\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"cognitiveServices\": {\n",
    "    \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
    "    \"description\": os.environ['COG_SERVICES_NAME'],\n",
    "    \"key\": os.environ['COG_SERVICES_KEY']\n",
    "  },\n",
    "  \"indexProjections\": {\n",
    "    \"selectors\": [\n",
    "      {\n",
    "        \"targetIndexName\": index_name,\n",
    "        \"parentKeyFieldName\": \"ParentKey\",\n",
    "        \"sourceContext\": \"/document/chunks/*\",\n",
    "        \"mappings\": [\n",
    "          {\n",
    "            \"name\": \"title\",\n",
    "            \"source\": \"/document/title\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"name\",\n",
    "            \"source\": \"/document/name\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"location\",\n",
    "            \"source\": \"/document/location\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"chunk\",\n",
    "            \"source\": \"/document/chunks/*\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"source\": \"/document/chunks/*/chunk_embedded\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"language\",\n",
    "            \"source\": \"/document/chunks/*/chunk_language\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"persons\",\n",
    "            \"source\": \"/document/chunks/*/persons\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"urls\",\n",
    "            \"source\": \"/document/chunks/*/urls\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"emails\",\n",
    "            \"source\": \"/document/chunks/*/emails\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"key_phrases\",\n",
    "            \"source\": \"/document/chunks/*/key_phrases\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/skillsets/\" + skillset_name,\n",
    "                 data=json.dumps(skillset_payload), headers=headers, params=params_old)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c357a",
   "metadata": {},
   "source": [
    "## Create and Run the Indexer\n",
    "[Reference guide](https://learn.microsoft.com/en-us/rest/api/searchservice/indexers?view=rest-searchservice-2023-11-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a36fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the skillset, including the projection settings into the single index created above\n",
    "indexer_payload = {    \n",
    "  \"name\": indexer_name,\n",
    "  \"dataSourceName\": datasource_name,\n",
    "  \"targetIndexName\": index_name,\n",
    "  \"skillsetName\": skillset_name,  \n",
    "  \"fieldMappings\": [\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_path\",\n",
    "      \"targetFieldName\": \"id\",\n",
    "      \"mappingFunction\": {\n",
    "        \"name\": \"base64Encode\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_title\",\n",
    "      \"targetFieldName\": \"title\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_name\",\n",
    "      \"targetFieldName\": \"name\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_path\",\n",
    "      \"targetFieldName\": \"location\"\n",
    "    }\n",
    "  ],\n",
    "  \"outputFieldMappings\": [],\n",
    "  \"parameters\": {\n",
    "    \"maxFailedItems\": -1,\n",
    "    \"maxFailedItemsPerBatch\": -1,\n",
    "    \"configuration\": {\n",
    "      \"dataToExtract\": \"contentAndMetadata\",\n",
    "      \"parsingMode\": \"default\",\n",
    "      \"imageAction\": \"generateNormalizedImages\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name,\n",
    "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabdd398",
   "metadata": {},
   "source": [
    "### Please wait for a few minutes after running the cell above to allow the Indexer to fill the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9e256",
   "metadata": {},
   "source": [
    "## And now... let's search!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55f265",
   "metadata": {},
   "source": [
    "Let's start with a text search. Recall:\n",
    "- The **value** key contains the ***sequence of results*** returned by the query\n",
    "- The **@search.answers** key contain the the ***answers*** query results for the search operation. They include three pieces of information:\n",
    "  1. **key**: the search document key\n",
    "  2. **score**: the score associated to that semantic answer\n",
    "  3. **text**: the so called ***captions*** that contain the most representative passages from the document relatively to the search query. They are often used as document summary. Captions are only returned for queries of type semantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39c95a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 313,\n",
      "  \"@search.answers\": [\n",
      "    {\n",
      "      \"key\": \"8cac12fadbc1_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"text\": \"A meaning function is a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings; typically, some set-theoretic objects like lists of features or functions.\",\n",
      "      \"highlights\": \"A meaning function is<em> a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings;</em> typically, some set-theoretic objects like lists of features or functions.\",\n",
      "      \"score\": 0.9970703125\n",
      "    },\n",
      "    {\n",
      "      \"key\": \"8cac12fadbc1_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_4\",\n",
      "      \"text\": \"A meaning function \\u00b5 is a compositional semantics for the set S if its domain is contained in S, and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "      \"highlights\": \"A meaning function \\u00b5 is<em> a compositional semantics for the set S if its domain is contained in S,</em> and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "      \"score\": 0.8466796875\n",
      "    }\n",
      "  ],\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 8.24891,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"A meaning function is a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings; typically, some set-theoretic objects like lists of features or functions.\",\n",
      "          \"highlights\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 3.868259906768799,\n",
      "      \"id\": \"8cac12fadbc1_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 8.7842,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"A meaning function \\u00b5 is a compositional semantics for the set S if its domain is contained in S, and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "          \"highlights\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 3.6001837253570557,\n",
      "      \"id\": \"8cac12fadbc1_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_4\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"grammar \\u2014 its total length is less than 300 symbols (vs. 18,000);\\nbut it assumes an existence of a language generator. Interestingly,\\nthe grammar resembles the compositional semantics, as usually\\ngiven. The rule with V (1) and N(1) describes the compositional\\npart of the corpus; the rule with v0 and n0 \\u2013 the idiomatic; other\\nrules are in between.\\n\\n3.4 Variations on the MDL algorithm\\n\\nA similar result is obtained when we do not insist that all instances\\nof merging classes are replaced by the result of the merge. Starting\\nwith the grammar\\n\\nGrammar GV (1):\\n\\nV (1) = {v1| ... |v9} (21)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{V (1)}{n1}{action}{V (1)}{object}{n1} (18)\\n...\\n\\n{V (1)}{n99}{action}{V (1)}{object}{n99} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n{v0}{n1}{action}{v0}{object}{n1} (18)\\n... (18)\\n{v0}{n99}{action}{v0}{object}{n99} (18)\\n\\n14\\n\\n\\n\\nWe can see that the merge V (0) = {v0|V (1)} will decrease the size\\nof the grammar by 99 rules and result in:\\n\\nGrammar GV (0):\\n\\nV (1) = {v1| ... |v9} (21)\\nV (0) = {v0|V (1)} (7)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{V (0)}{n1}{action}{V (0)}{object}{n1} (18)\\n...\\n\\n{V (0)}{n99}{action}{V (0)}{object}{n99} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nThe successive merging of nouns will then produce\\n\\nGrammar GV (0)N(1):\\n\\nV (0) = {v0|V (1)} (7)\\nV (1) = {v1| ... |v9} (21)\\nN(1) = {n1| ... |n99} (201)\\n{V (0)}{N(1)}{action}{V (0)}{object}{N(1)} (18)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nIf, however we do not do the V (0) = {v0|V (1)} merge, and\\nproceed with the merging of the nouns (e.g. if there were reasons\\nto modify the algorithm), we get:\\n\\n15\\n\\n\\n\\nGrammar GV (1)N(0):\\n\\nV (1) = {v1| ... |v9} (21)\\nN(1) = {n1| ... |n99} (201)\\nN(0) = {n0|N(1)} (7)\\n{V (1)}{N(0)}{action}{V (1)}{object}{N(0)} (18)\\n{v0}{N(1)}{action}{v0}{object}{N(1)} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nFinally, if we allow some overgeneralization, we can replace the\\nabove grammars with an even shorter grammar:\\n\\nGrammar GV (0)N(0):\\n\\nV = {v0| ... |v9} (23)\\nN = {n0| ... |n99} (203)\\n{V }{N}{action}{V }{object}{N} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nHere, clearly v0 is the idiomatic element. However, both idiomatic\\nand non-idiomatic reading of kick bucket is allowed. (In the pre-\\nviously defined grammars, we can also see the distinction between\\nthe idiomatic and non-idiomatic elements).\\n\\n4 A non-vacuous definition of composition-\\n\\nality\\n\\nThe fact that that the MDL principle can produce an object re-\\nsembling a compositional semantics is crucial. It allows us to argue\\nfor a non-vacuous definition of compositionality.\\n\\nAssume that we have a corpus S of sentences and their parts,\\ngiven either as a set or generated by a grammar. Let sentences\\n\\n16\\n\\n\\n\\nand their parts be collections of symbols put together by some\\noperations; in the simplest and most important case, by concate-\\nnation \\u201d.\\u201d.\\n\\nDefinition. A meaning function \\u00b5 is a compositional semantics\\nfor the set S if its domain is contained in S, and\\na. it satisfies the postulate of compositionality: for all s, t in its\\ndomain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nb. it is the shortest, in the sense of the Minimum Description\\nLength principle, such an encoding.\\nc. it is maximal, i.e. there is no \\u00b5\\u2032 with a larger domain that\\nsatisfies a and b.\\n\\nTo see better what this definition entails, let us consider our\\nsemantic corpus again. The set S consists of the 10 verbs and\\n100 nouns and all noun-verb combinations. The compositional\\nfunction \\u00b5 assigns to each word its category e.g. [n17, noun].\\nThe question is how to define the operator \\u2295. Because of the\\nidiom, it cannot be a total function; hence we have to exclude\\nfrom the domain of \\u2295 the pair [[v0, verb], [n0, noun]]. The short-\\nest description of \\u2295 can be given by translating the grammar of\\nSection 3.2. First, map non-idiomatic verbs and nouns into pairs\\n\\u00b5(vi) = [vi, verbnonid], \\u00b5(nj) = [nj , nounnonid], i, j > 0. Then,\\nput\\n\\n\\u2295([[v, verbnonid], [n, nounnonid]]) = [action.v, object.n]\\n\\nThus defined \\u00b5 and \\u2295 correspond to the grammar obtained by the\\nalgorithm of Section 3.2 and to the tables of Section 2. This cor-\\nrespondence is not exact, because functions \\u00b5 and \\u2295 encode only\\nthe systematic, compositional part of the corpus. (But please note\\nthis clear distinction between the idiomatic and the compositional\\nparts of the lexicon and the corpus).\\n\\n17\\n\\n\\n\\nHowever this description of the two functions is not maximal.\\nWe obtain the maximal compositional semantics for S by extend-\\ning the above defined mapping to all nouns \\u00b5(nj) = [nj, noun],\\nj \\u2265 0, and extending the domain of \\u2295\\n\\n\\u2295([[v, verbnonid], [n, noun]]) = [action.v, object.n]\\n\\nIt is easily checked that this is the shortest (in the sense of the\\nMDL) and maximal assignment of meaning to the elements of set\\nS.4 Please compare this mapping with GV (1)N(0), and also note\\nthat now we have a formal basis for saying that (for this corpus)\\nit is the verb kick, and not the noun bucket, that is idiomatic.\\n\\n\",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 11.708195,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"Since this chapter deals with sizes of objects, we compute them for the meaning functions: the size of the first function is 90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79.\",\n",
      "          \"highlights\": \"Since this chapter deals with sizes of objects, we compute them for the<em> meaning functions:</em> the size of the first function is 90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79.\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 2.895526885986328,\n",
      "      \"id\": \"8cac12fadbc1_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_2\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"If our corpus were to be the 10 \\u00d7 100 table consisting\\nof all verb-noun combinations:\\n\\nv0n0 + v1n0 + ... + vjni + ...\\n\\nwe could quickly use the previous example to write a simple finite\\nstate grammar that describes the corpus:\\n\\n{v0|v1|...}{n0|n1...} (21 + 201)\\n\\nBut in this subsection we are supposed to introduce some seman-\\ntics. Thus, let our corpus consist of all those 1,000 sentences to-\\ngether with their meanings, which, to keep things as simple as\\npossible, will be simplified to two attributes. Also, for the reason\\nof simplicity, we assume that only \\u201dkick bucket\\u201d has an idiomatic\\nmeaning, and all other entries are assigned the meaning consisting\\nof the two attribute expression [[action, vj ], [object, ni]]. Hence,\\nour corpus will look as follows:\\n\\nkick bucket action die object nil\\n\\nv1 bucket action v1 object bucket\\n\\n...\\n\\nvj ni action vj object ni\\n\\n...\\n\\nNow, notice that this corpus cannot be encoded by means of a\\nshort finite state grammar, because of the dependence of the mean-\\nings (i.e. the pair [action, ..., object, ...]) on the first two elements\\nof each sentence. We will have to extend our grammar formalism\\nto address this dependence (Section 3).\\n\\n7\\n\\n\\n\\n2.4 On meaning functions\\n\\nEven though we cannot encode the corpus by a short, finite state\\ngrammar, we can easily provide for it a compositional semantics.\\nTo avoid the complications of type raising, we will build a homo-\\nmorphic mapping from syntax to semantics. To do it, it is enough\\nto build meaning functions in a manner ensuring that the meaning\\nof each vjni is composed from the meaning of vj and the mean-\\ning of ni. Since our corpus is simple, these meaning functions are\\nsimple, too: For the verbs the meaning function is given by the\\ntable:\\n\\n[v0, [verb, v0]]; [v1, [verb, v1]] ... [v9, [verb, v9]] (90)\\n\\nFor the nouns:\\n\\n[n0, [noun, n0]]; [n1, [noun, n1]] ... [n99, [noun, n99]] (900)\\n\\nWe have represented both meaning functions as tables of sym-\\nbols. Since this chapter deals with sizes of objects, we compute\\nthem for the meaning functions: the size of the first function is\\n90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79. Therefore,\\nthe meaning function for the whole corpus could be represented\\nas a table with 1,000 entries:\\n\\n[[[verb, v9], [noun, n99]], [[action, v9], [object, n99]]]\\n. . .\\n[[[verb, vj ], [noun, ni]], [[action, vj ], [object, ni]]]\\n. . .\\n[[[verb, v1], [noun, bucket]], [[action, v1 ], [object, bucket]]]\\n[[[verb, kick], [noun, bucket]], [[action, die], [object, nil]]\\n\\nand the size of this table is 29\\u00d71000. Finally, the total size of the\\ntables that describe the compositional interpretation of the corpus\\nis 29000 + 900 + 90, i.e. roughly 30, 000. Notice that if we had\\nmore verbs and nouns, the tables describing the meaning functions\\n\\n8\\n\\n\\n\\nwould be even larger.2 Also, note that we have not counted the\\ncost of encoding the positions of elements of the table, which would\\nbe the log of the total number of symbols in the table. This\\nsimplifying assumption does not change anything in the strength\\nof our arguments (as larger tables have longer encodings).\\n\\n3 Compositional semantics through the Min-\\n\\nimum Description Length principle\\n\\nIn this section we first extend our notation to deal with semantic\\ngrammars. Then we apply the minimum description length princi-\\nple to construct a compact representation of our example corpus.\\nThis experience will motivate our new, non-vacuous definition of\\nthe notion of compositional semantics given in Section 4.\\n\\n3.1 Representations\\n\\nWe have seen that it is impossible to efficiently encode our se-\\nmantic corpus using a finite state grammar. Therefore, we have\\nto make our representation of grammars more expressive (at the\\nprice of a slightly bigger interpreter). Namely, we will allow a sim-\\nple form of unification.\\n\\n2 The reader familiar with [12] should notice that the meaning functions\\nobtained by the solution lemma also consist of tables of element-value pairs. It\\nis easy to see that for the corpus we are encoding the solution lemma produces\\nthe same meaning functions.\\n\\nIn the other direction, the method for deriving compositional semantics us-\\ning the minimum description length principle (Sections 3 and 4) are directly\\napplicable to meaning functions obtained by the solution lemma in [12], pro-\\nvided they are finite (which covers the practically interesting cases); and it\\nseems applicable to the infinite case, if it has a finite representation. However,\\nwe will not pursue this connection any further.\\n\\n9\\n\\n\\n\\nExample. Assume we do not want {a|b}{a|b|d} to generate ab.\\nWe can do it by changing the notation:\\n\\nX = {a|b}\\n{X}{X|d}\\n\\nThe intention is simple: first, we define a class variable (X) for the\\nclass consisting of elements a and b; then, we generate all strings\\nusing the rule with variable X: XX and Xd; and finally we substi-\\ntute for X all its possible values, which produces aa, ad, bb and bd.\\n\\nMore generally, let us assume that we have an alphabet a1, a2, ...,\\n\",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Three full text answers plus two semantic answers\n",
    "results = 3\n",
    "answers = 2\n",
    "QUESTION = \"What is a meaning function?\"\n",
    "\n",
    "import requests, json\n",
    "\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params  = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']} # NEW VERSION\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"search\": QUESTION,\n",
    "  \"count\": \"true\",\n",
    "  \"top\": results,\n",
    "  \"select\": \"id, name, title, chunk, language\",\n",
    "  \"queryType\": \"semantic\",\n",
    "  \"semanticConfiguration\": \"my-semantic-config\",\n",
    "  \"captions\": \"extractive\",\n",
    "  \"answers\": f\"extractive|count-{answers}\"\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25613c4",
   "metadata": {},
   "source": [
    "### Now let's do a *vector* query. \n",
    "We can use two possible ***vectorQueries*** settings in the search payload:\n",
    "1. [**\"kind\": \"vector\"**](https://learn.microsoft.com/en-us/rest/api/searchservice/documents/search-post?view=rest-searchservice-2023-11-01&tabs=HTTP#rawvectorquery), when a raw vector value is provided, such as an Azure OpenAI Embedding. In this case, of course, we need to **manually** convert and pass the query embedding.\n",
    "2. [**\"kind\": \"text\"**](https://learn.microsoft.com/en-us/rest/api/searchservice/documents/search-post?view=rest-searchservice-2023-10-01-Preview&tabs=HTTP#rawvectorquery), which accepts a text that is **automatically** converted into an embedding thanks to the **vectorized** parameter associated to the \"chunkVector\" field in the indexer. **Currently, only REST API 2023-10-01-Preview supports this feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7e9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 2,\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 0.84926814,\n",
      "      \"id\": \"b21f0042cdca_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0_chunks_3\",\n",
      "      \"title\": \"Microsoft Word - LP99-sub.htm\",\n",
      "      \"chunk\": \"requirement that the meaning of an expression be functionally determined by its structure and the \\n\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\n\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\n\\n \\n\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\n\\nMEANING \\n\\n \\n\\n    The importance which has been assigned to compositionality in semantic theory can be \\n\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\n\\n                                                                                                                                                                                           \\nsense of inter-substitutivity.  \\n\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\nconstraints upon the set possible models into the definition of a compositional meaning function. \\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\n\\n\\n\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\n\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\n\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\n\\n \\n\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\n\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\n\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\n\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\n\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\n\\n \\n\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\n\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\n\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\n\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\n\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\n\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\n\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\n\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\n\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\n\\nsyntactic derivation tree.  \\n\\n \\n\\n\\n\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\n\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\n\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\n\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\n\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\n\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\n\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\n\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\n\\n \\n\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\n\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\n\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\n\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\n\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\n\\nsingle disambiguated interpretation. \",\n",
      "      \"name\": \"0001006v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001006v1.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 0.8394798,\n",
      "      \"id\": \"8cac12fadbc1_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001002v1.pdf\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# \"kind\": \"vector\"\n",
    "\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "embedder = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\", skip_empty=True)\n",
    "VECTORIZED_QUESTION = embedder.embed_query(QUESTION)\n",
    "results = 3\n",
    "answers = 2\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"count\": \"true\",\n",
    "  \"select\": \"id, name, title, location, chunk\",\n",
    "  \"top\": results,\n",
    "  \"vectorQueries\": [\n",
    "    {\n",
    "      \"kind\": \"vector\",\n",
    "      \"k\": answers,\n",
    "      \"fields\": \"chunkVector\",\n",
    "      \"vector\": VECTORIZED_QUESTION\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f0c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 2,\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 0.84926814,\n",
      "      \"id\": \"b21f0042cdca_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0_chunks_3\",\n",
      "      \"title\": \"Microsoft Word - LP99-sub.htm\",\n",
      "      \"chunk\": \"requirement that the meaning of an expression be functionally determined by its structure and the \\n\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\n\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\n\\n \\n\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\n\\nMEANING \\n\\n \\n\\n    The importance which has been assigned to compositionality in semantic theory can be \\n\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\n\\n                                                                                                                                                                                           \\nsense of inter-substitutivity.  \\n\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\nconstraints upon the set possible models into the definition of a compositional meaning function. \\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\n\\n\\n\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\n\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\n\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\n\\n \\n\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\n\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\n\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\n\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\n\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\n\\n \\n\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\n\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\n\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\n\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\n\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\n\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\n\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\n\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\n\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\n\\nsyntactic derivation tree.  \\n\\n \\n\\n\\n\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\n\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\n\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\n\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\n\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\n\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\n\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\n\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\n\\n \\n\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\n\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\n\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\n\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\n\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\n\\nsingle disambiguated interpretation. \",\n",
      "      \"name\": \"0001006v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001006v1.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 0.8394798,\n",
      "      \"id\": \"8cac12fadbc1_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001002v1.pdf\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# \"kind\": \"text\" - requires 2023-10-01-Preview version of Azure AI HTTP REST API\n",
    "\n",
    "results = 3\n",
    "answers = 2\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"count\": \"true\",\n",
    "  \"select\": \"id, name, title, location, chunk\",\n",
    "  \"top\": results,\n",
    "  \"vectorQueries\": [\n",
    "    {\n",
    "      \"kind\": \"text\",\n",
    "      \"k\": answers,\n",
    "      \"fields\": \"chunkVector\", \n",
    "      \"text\": QUESTION\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params_old) # 2023-10-01-Preview version\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc66a19",
   "metadata": {},
   "source": [
    "## ...et *voil*, the two answers are 100% identical\n",
    "You want to confirm the differences, go [here](https://www.diffchecker.com/text-compare/) ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3d535",
   "metadata": {},
   "source": [
    "# BONUS cell: using Azure OpenAI Extensions to make a single OpenAI call that implicitly contacts Azure AI Search\n",
    "Finally, we make a single call to OpenAI, passing all the necessary information to access to Azure Search (its endpoint, authentication method, index name) in the payload to extract the top chunks from the vector index we built in Azure AI Search.\n",
    "\n",
    "## At this point, the question arises: Who calls Who?\n",
    "and the answer is that **OpenAI calls Azure Search**, theres no doubt about it as weve only invoked the Azure OpenAI endpoint. So heres how to implement this second method, which is certainly more practical than the previous one, even if potentially less flexible and controllable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54baf406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"55eb15d5-8169-4f72-8a8c-25114c5be1a1\",\n",
      "  \"model\": \"gpt-4-32k\",\n",
      "  \"created\": 1703298017,\n",
      "  \"object\": \"extensions.chat.completion\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"A meaning function, denoted as \\u00b5, is a concept used in semantics to assign meanings to sentences and their parts. It is a compositional semantics for a set if its domain is contained in the set, and it satisfies the postulate of compositionality. This means that for all s, t in its domain, \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t), where s and t are sentences or parts of sentences. The meaning function should be the shortest, in the sense of the Minimum Description Length principle, and it should be maximal, meaning there is no \\u00b5' with a larger domain that satisfies the same conditions[doc2]. \\n\\nIn the context of a language, a meaning function applies to expressions with fully specified syntactic representations and yields unique semantic values. Therefore, syntactic and semantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous lexical items are separated into words that stand in a one-to-one correspondence with the distinct senses of the original terms. Similarly, the different scope readings of a phrase are obtained from distinct syntactic sources[doc4]. \\n\\nHowever, it's important to note that a meaning function can also be non-compositional and systematic, taking the meaning of a syntactically complex expression to be the set of values of a relation on the meanings of the expression's syntactic constituents rather than the value of a function[doc4].\",\n",
      "        \"end_turn\": true,\n",
      "        \"context\": {\n",
      "          \"messages\": [\n",
      "            {\n",
      "              \"role\": \"tool\",\n",
      "              \"content\": \"{\\\"citations\\\": [{\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\nThe assignement  Xa=\\u00b5(a), Xb=\\u00b5(b), Xs=\\u00b5(s) for s in L\\\\\\\\{a,b}, together with X$=\\u00b5($), \\\\n\\\\nXs$=\\u00b5(s.$)=m(s), for all s in L.  is a solution (by the definition of the meaning function \\u00b5). \\\\n\\\\nHowever, Xb=\\u00b5(a), Xa=\\u00b5(b), Xs=\\u00b5(s) for s in L\\\\\\\\{a,b} also produces a solution. Thus, by its \\\\n\\\\nuniqueness, \\u00b5(a)=\\u00b5(b).  \\\\n\\\\n \\\\n\\\\n\\\\n\\\\nAs in Zadrozny 1994 we check that compositionality holds,  \\u00b5(s.t) = \\u00b5(s)(\\u00b5(t)), that is, it follows \\\\n\\\\nimmediately from the fact that   <Xt, Xs.t >  is in Xs. Finally, the recoverability of meanings \\\\n\\\\n\\u00b5(s.$) = m(s) holds true,  because \\u00b5(s.$) = m(s) = \\u00b5(s)(\\u00b5($)), again by direct lookup. \\\\n\\\\nQED. \\\\n\\\\n \\\\n\\\\nRemark 1. $ doesn\\u2019t have to extend the language (as it did in Proposition 3). We can use any \\\\n\\\\nelement which does not make a distinction between the meaning of a and b, and which is outside \\\\n\\\\nof the range of  m. For instance, without extending the language by $ we could recover the \\\\n\\\\nmeanings by requiring \\u00b5(s)(\\u00b5) = m(s). However, we could not use  \\\\n\\\\nXa = { <a,m(a)>} 4 {  <Xt, Xa.t : a.t c L } \\\\n\\\\nXb = { <b,m(b)>} 4 {  <Xt, Xb.t : a.t c L } \\\\n\\\\nbecause  this (or similar construction) would encode the orthographic difference between a and b \\\\n\\\\nas part of their meanings \\u2013 which would effectively deny the synonymy of a and b. \\\\n\\\\n \\\\n\\\\nRemark 2. \\u00b5($) is Omega, the simplest non-well founded set, i.e. the canonical set that satisfies \\\\n\\\\nX={X}. (cf. Aczel, 1988 p.6). \\\\n\\\\n \\\\n\\\\n\\\\n\\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=634. Scores=5.499957Org Highlight count=11.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0\\\\ngrammar \\u2014 its total length is less than 300 symbols (vs. 18,000);\\\\nbut it assumes an existence of a language generator. Interestingly,\\\\nthe grammar resembles the compositional semantics, as usually\\\\ngiven. The rule with V (1) and N(1) describes the compositional\\\\npart of the corpus; the rule with v0 and n0 \\u2013 the idiomatic; other\\\\nrules are in between.\\\\n\\\\n3.4 Variations on the MDL algorithm\\\\n\\\\nA similar result is obtained when we do not insist that all instances\\\\nof merging classes are replaced by the result of the merge. Starting\\\\nwith the grammar\\\\n\\\\nGrammar GV (1):\\\\n\\\\nV (1) = {v1| ... |v9} (21)\\\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\\\n{V (1)}{n1}{action}{V (1)}{object}{n1} (18)\\\\n...\\\\n\\\\n{V (1)}{n99}{action}{V (1)}{object}{n99} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n{v0}{n1}{action}{v0}{object}{n1} (18)\\\\n... (18)\\\\n{v0}{n99}{action}{v0}{object}{n99} (18)\\\\n\\\\n14\\\\n\\\\n\\\\n\\\\nWe can see that the merge V (0) = {v0|V (1)} will decrease the size\\\\nof the grammar by 99 rules and result in:\\\\n\\\\nGrammar GV (0):\\\\n\\\\nV (1) = {v1| ... |v9} (21)\\\\nV (0) = {v0|V (1)} (7)\\\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\\\n{V (0)}{n1}{action}{V (0)}{object}{n1} (18)\\\\n...\\\\n\\\\n{V (0)}{n99}{action}{V (0)}{object}{n99} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nThe successive merging of nouns will then produce\\\\n\\\\nGrammar GV (0)N(1):\\\\n\\\\nV (0) = {v0|V (1)} (7)\\\\nV (1) = {v1| ... |v9} (21)\\\\nN(1) = {n1| ... |n99} (201)\\\\n{V (0)}{N(1)}{action}{V (0)}{object}{N(1)} (18)\\\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nIf, however we do not do the V (0) = {v0|V (1)} merge, and\\\\nproceed with the merging of the nouns (e.g. if there were reasons\\\\nto modify the algorithm), we get:\\\\n\\\\n15\\\\n\\\\n\\\\n\\\\nGrammar GV (1)N(0):\\\\n\\\\nV (1) = {v1| ... |v9} (21)\\\\nN(1) = {n1| ... |n99} (201)\\\\nN(0) = {n0|N(1)} (7)\\\\n{V (1)}{N(0)}{action}{V (1)}{object}{N(0)} (18)\\\\n{v0}{N(1)}{action}{v0}{object}{N(1)} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nFinally, if we allow some overgeneralization, we can replace the\\\\nabove grammars with an even shorter grammar:\\\\n\\\\nGrammar GV (0)N(0):\\\\n\\\\nV = {v0| ... |v9} (23)\\\\nN = {n0| ... |n99} (203)\\\\n{V }{N}{action}{V }{object}{N} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nHere, clearly v0 is the idiomatic element. However, both idiomatic\\\\nand non-idiomatic reading of kick bucket is allowed. (In the pre-\\\\nviously defined grammars, we can also see the distinction between\\\\nthe idiomatic and non-idiomatic elements).\\\\n\\\\n4 A non-vacuous definition of composition-\\\\n\\\\nality\\\\n\\\\nThe fact that that the MDL principle can produce an object re-\\\\nsembling a compositional semantics is crucial. It allows us to argue\\\\nfor a non-vacuous definition of compositionality.\\\\n\\\\nAssume that we have a corpus S of sentences and their parts,\\\\ngiven either as a set or generated by a grammar. Let sentences\\\\n\\\\n16\\\\n\\\\n\\\\n\\\\nand their parts be collections of symbols put together by some\\\\noperations; in the simplest and most important case, by concate-\\\\nnation \\u201d.\\u201d.\\\\n\\\\nDefinition. A meaning function \\u00b5 is a compositional semantics\\\\nfor the set S if its domain is contained in S, and\\\\na. it satisfies the postulate of compositionality: for all s, t in its\\\\ndomain:\\\\n\\\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\\\n\\\\nb. it is the shortest, in the sense of the Minimum Description\\\\nLength principle, such an encoding.\\\\nc. it is maximal, i.e. there is no \\u00b5\\u2032 with a larger domain that\\\\nsatisfies a and b.\\\\n\\\\nTo see better what this definition entails, let us consider our\\\\nsemantic corpus again. The set S consists of the 10 verbs and\\\\n100 nouns and all noun-verb combinations. The compositional\\\\nfunction \\u00b5 assigns to each word its category e.g. [n17, noun].\\\\nThe question is how to define the operator \\u2295. Because of the\\\\nidiom, it cannot be a total function; hence we have to exclude\\\\nfrom the domain of \\u2295 the pair [[v0, verb], [n0, noun]]. The short-\\\\nest description of \\u2295 can be given by translating the grammar of\\\\nSection 3.2. First, map non-idiomatic verbs and nouns into pairs\\\\n\\u00b5(vi) = [vi, verbnonid], \\u00b5(nj) = [nj , nounnonid], i, j > 0. Then,\\\\nput\\\\n\\\\n\\u2295([[v, verbnonid], [n, nounnonid]]) = [action.v, object.n]\\\\n\\\\nThus defined \\u00b5 and \\u2295 correspond to the grammar obtained by the\\\\nalgorithm of Section 3.2 and to the tables of Section 2. This cor-\\\\nrespondence is not exact, because functions \\u00b5 and \\u2295 encode only\\\\nthe systematic, compositional part of the corpus. (But please note\\\\nthis clear distinction between the idiomatic and the compositional\\\\nparts of the lexicon and the corpus).\\\\n\\\\n17\\\\n\\\\n\\\\n\\\\nHowever this description of the two functions is not maximal.\\\\nWe obtain the maximal compositional semantics for S by extend-\\\\ning the above defined mapping to all nouns \\u00b5(nj) = [nj, noun],\\\\nj \\u2265 0, and extending the domain of \\u2295\\\\n\\\\n\\u2295([[v, verbnonid], [n, noun]]) = [action.v, object.n]\\\\n\\\\nIt is easily checked that this is the shortest (in the sense of the\\\\nMDL) and maximal assignment of meaning to the elements of set\\\\nS.4 Please compare this mapping with GV (1)N(0), and also note\\\\nthat now we have a formal basis for saying that (for this corpus)\\\\nit is the verb kick, and not the noun bucket, that is idiomatic.\\\\n\\\\n\\\\nen\\\\n0001002v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1854. Scores=5.2807636Org Highlight count=37.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0\\\\nWhat are the advantages of defining compositionality using\\\\nthe Minimum Description Length principle? 1. It brings us back\\\\nto the original definition of compositionality, but makes it non-\\\\nvacuous. 2. It encodes the postulate that the meaning functions\\\\nshould be simple. 3. It allows us to distinguish between composi-\\\\ntional and non-compositional semantics by means of systematicity,\\\\ni.e. the minimality of encodings, as e.g. Hirst [5] wanted. 4. It\\\\ndoes not make a reference to non-intrinsic properties of mean-\\\\ning functions (like being a polynomial). 5. It works for different\\\\nmodels of language understanding: pipeline (syntax, semantics,\\\\npragmatics), construction grammars (cf. [3]), and even semantic\\\\ngrammars. 6. It allows us to compare different meaning functions\\\\nwith respect to how compositional they are \\u2014 we can measure\\\\nthe size of their domains and the length of the encodings. Finally,\\\\nthis definition might even satisfy those philosophers of language\\\\nwho regard compositionality not as a formal property but as an\\\\nunattainable ideal worth striving for. This hope is based on the\\\\nfact that, given an appropriately rich model of language, its min-\\\\nimum description length is, in general, non-computable, and can\\\\n\\\\n4We are assuming that we have to assign the noun and verb categories to\\\\nthe lexical symbols of the corpus.\\\\n\\\\n18\\\\n\\\\n\\\\n\\\\nonly be approximated but never exactly computed.\\\\n\\\\n5 Discussion and Conclusions\\\\n\\\\nLambdas, approximations, and the minimum description\\\\n\\\\nlength\\\\n\\\\nAssuming that we have a \\u03bb-expressions interpreter (e.g. a lisp\\\\nprogram), we could describe the meaning functions of Section 3\\\\nas:\\\\n\\\\n\\u03bbX.[noun,X]\\\\n\\u03bbY.[verb, Y ]\\\\n\\u03bb[verb, Y ][noun,X].[[action, Y ], [object,X]]\\\\n\\u03bb[verb, kick][noun, bucket].[[action, die], [object, nil]]\\\\n\\\\nThe approximate total size of this description is size(\\u03bb\\u2212interpreter)\\\\n+ 66 (the above definitions) + 110 (to describe the domains of the\\\\nfirst two functions).\\\\n\\\\nClearly, the last lambda expression corresponds to an idiomatic\\\\nmeaning. But, note that this definition assigns also the non-\\\\nidiomatic meaning to \\u201dkick bucket\\u201d. Thus, although much simpler,\\\\nit does not exactly correspond to the original meaning function.\\\\nIt does however correspond to grammar GV (0)N(0) of the previous\\\\nsection. Also, representations that ignore exceptions are more of-\\\\nten found in the literature. This point may be worth pursuing:\\\\nSavitch in [9] argues that approximate representation in a more\\\\nexpressive language can be more compact. For approximate rep-\\\\nresentations that overgeneralize, the idiomaticity of an expression\\\\ncan be defined as the existence of a more specific definition of its\\\\nmeaning.\\\\n\\\\nBridging linguistic and probabilistic approaches to natu-\\\\n\\\\nral language\\\\n\\\\nThe relationship between linguistics principles and the MDL\\\\nmethod is not completely surprising. We used the MDL principle\\\\n\\\\n19\\\\n\\\\n\\\\n\\\\nin [13] to argue for a construction-based approach to language un-\\\\nderstanding (cf. [3]). After setting up a formal model based on\\\\nlinguistic and computational evidence, we applied the MDL prin-\\\\nciple to prove that construction-based representations are at least\\\\nan order of magnitude more compact that the corresponding lexi-\\\\ncalized representations of the same linguistic data. The argument\\\\npresented there suggests that in building compositional semantics\\\\nwe might be better off when the language is build by means of\\\\nreach combinatorics (constructions), than by the concatenation of\\\\nlexical items. However, this hypothesis remains to be proved.\\\\n\\\\nIt is known that the most important rules of statistical rea-\\\\nsoning, the maximum likelihood method, the maximum entropy\\\\nmethod, the Bayes rule and the minimum description length, are\\\\nall closely related (cf. pp. 275-321 of [7]). From the material\\\\nof Sections 3 and 4 we can see that compositionality is closely\\\\nrelated to the MDL principle; thus, it is possible to imagine bring-\\\\ning together linguistic and statistical methods for natural language\\\\nunderstanding. For example, starting with semantic classes of [2]\\\\ncontinue derivation of semantic model for a large corpus using the\\\\nmethod of Section 3 with the computational implementation along\\\\nthe lines of [1].\\\\n\\\\nConclusion\\\\n\\\\nWe have redefined the linguistic concept of compositionality as\\\\nthe simplest maximal description of data that satisfies the postu-\\\\nlate that the meaning of the whole is a function of the meaning\\\\nof its parts. By justifying compositionality by the minimum de-\\\\nscription length principle, we have placed the intuitive idea that\\\\nthe meaning of a sentence is a combination of the meanings of its\\\\nconstituents on a firm mathematical foundation.\\\\n\\\\nThis new, non-vacuous definition of compositionality is intu-\\\\nitive and allows us to distinguish between compositional and non-\\\\n\\\\n20\\\\n\\\\n\\\\n\\\\ncompositional semantics, and between idiomatic and non-idiomatic\\\\nexpressions. It is not ad hoc, since it does not make any references\\\\nto non-intrinsic properties of meaning functions (like being a poly-\\\\nnomial). \\\\nen\\\\n0001002v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1275. Scores=9.108858Org Highlight count=58.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\nThis set is, in effect, the disjunction of interpretations which \\\\n\\\\ncan be assigned to E in particular contexts. A relational theory of meaning represents the \\\\n\\\\ninterpretation of a sentence as underspecified to the extent that it defines its semantic value as a \\\\n\\\\nset of possibly incompatible alternative interpretations. \\\\n\\\\n \\\\n\\\\n    An early instance of such a relational theory of meaning is Cooper\\u2019s (1983) use of quantifier \\\\n\\\\nstorage to associate alternative quantifier scope readings with a single syntactic structure. In the \\\\n\\\\n                                                           \\\\n5See Nerbonne (1996) for a non-compositional approach to semantics in a constraint-based \\\\nframework that focuses on lexical and syntactic ambiguity. Lappin (forthcoming) gives an \\\\noverview of non-compositional representations of scope. \\\\n \\\\n\\\\n\\\\n\\\\ncomputational model of storage presented in Pereira and Shieber (1987) and Pereira (1990) the \\\\n\\\\nsyntactic structure ST of a sentence which contains quantified NP\\u2019s is assigned a set of scope \\\\n\\\\ninterpretations where each element of this set imposes a relative scope ordering on the semantic \\\\n\\\\nrepresentations of the quantifiers in ST.  \\\\n\\\\n \\\\n\\\\n    Recent work in underspecified semantics, such as Reyle (1993), Copestake et al. (1995), \\\\n\\\\nCrouch and van Genabith (1997), Richter and Sailer (1997), Lappin (1999), and Pollard (1999) \\\\n\\\\nextends and generalizes this approach. In each case, the meaning of a sentence is a set of possible \\\\n\\\\ninterpretations I such that each element of I is obtained by (i) ordering the scope relations of \\\\n\\\\nscope taking elements (quantifiers, modals, adverbs, certain connectives, etc.), and/or (ii) \\\\n\\\\nassigning values to parameters in the semantic representations of constituents of the sentence. \\\\n\\\\nThe elements of I are the resolved interpretations of the sentence that are selected in conjunction \\\\n\\\\nwith information supplied by the discourse context. For each syntactically complex constituent C \\\\n\\\\nof a sentence, the constraints in the semantic theory determine the elements of C\\u2019s semantic \\\\n\\\\nvalue sv(C) by defining the relation R which maps the semantic values of C\\u2019s immediate \\\\n\\\\nconstituents into the possible interpretations of C.  \\\\n\\\\n \\\\n\\\\n5. CONCLUSION  \\\\n\\\\n \\\\n\\\\n    We have argued that the objections raised by both K&P and Westerst\\u00e5hl to Zadrozny\\u2019s \\\\n\\\\ntheorem concerning compositionality do not hold. Specifically, the main criticism to the effect \\\\n\\\\nthat the encoding function \\u00b5 on which the theorem depends does not preserve the synonymy \\\\n\\\\nrelations in L specified by the original meaning function m implies an additional constraint on \\\\n\\\\n\\\\n\\\\ncompositionality that is not part of the original  homomorphism requirement. We have also \\\\n\\\\nshown that the construction of  the compositional function used in Zadrozny(1994) can preserve \\\\n\\\\nsynonymy relations if that is desired.  \\\\n\\\\n Recent work in underspecified semantics has shown that compositionality is not a necessary \\\\n\\\\ncondition for a systematic semantic theory. This work indicates that it is possible to develop a \\\\n\\\\nrelational theory of meaning which does not satisfy the homomorphism constraint but does \\\\n\\\\nassign semantic values to phrases that are systematically computed from the meanings of their \\\\n\\\\nsyntactic components. Given this result, there is no reason to take compositionality to be a \\\\n\\\\ncondition of adequacy on a semantic theory. Therefore, the fact that any meaning function for a \\\\n\\\\nlanguage L can be encoded by a mapping from the syntax to the semantics of L that is a \\\\n\\\\nhomomorphism is not a particularly dramatic result. Even if one adds constraints to the definition \\\\n\\\\nof compositionality to exclude functional encodings of the sort employed in the proof of \\\\n\\\\nZadrozny\\u2019s theorem, such a strengthened condition is of limited interest, given that we can \\\\n\\\\ndispense with the homomorphism condition on semantic theory.  \\\\n\\\\n \\\\n\\\\nREFERENCES \\\\n\\\\n \\\\n\\\\nAczel, P.(1988), Non-well-founded-sets, Center for the Study of Language and Information, \\\\n\\\\nStanford, CA. \\\\n\\\\nCooper, R. (1983), Quantification and Syntactic Theory, Reidel, Dordrecht. \\\\n\\\\nCopestake, A., D. Flickinger, R. Malouf, S. Riehman, and I.A. Sag (1995), \\u201cTranslation Using \\\\n\\\\n    Minimal Recursion Semantics\\u201d in Proceedings of the Sixth International Conference on \\\\n\\\\n    Theoretical Issues in Machine Translation, Leuven, Belgium. \\\\n\\\\n\\\\n\\\\nCrouch, R. and J. van Genabith (1997), Context Change, Underspecification and the Structure \\\\n\\\\n    of Glue Language Derivations, ms., University of Nottingham and Dublin City University. \\\\n\\\\nDavidson, D. (1984), \\u201cMeaning and Learnable Languages\\u201d in D. Davidson, Inquiries into \\\\n\\\\n    Truth and Interpretation, Oxford University Press, Oxford, pp. 3-15.  \\\\n\\\\nFulop, S. and E. Keenan (forthcoming), \\u201cCompositionality: A Global Perpsective\\u201d, Sonderheft \\\\n\\\\n    Semantik (a special issue of Linguistische Bericht). \\\\n\\\\nJanssen, T. (1997), \\u201cCompositionality\\u201d in J. van Benthem and A. ter Meulen (eds.), Handbook  \\\\n\\\\n    of Logic and Language, Elsevier, Amsterdam, pp. 417-473. \\\\n\\\\n\\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1433. Scores=7.7507205Org Highlight count=54.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\nrequirement that the meaning of an expression be functionally determined by its structure and the \\\\n\\\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\\\n\\\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\\\n\\\\n \\\\n\\\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\\\n\\\\nMEANING \\\\n\\\\n \\\\n\\\\n    The importance which has been assigned to compositionality in semantic theory can be \\\\n\\\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\\\n\\\\n                                                                                                                                                                                           \\\\nsense of inter-substitutivity.  \\\\n\\\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\\\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\\\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\\\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\\\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\\\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\\\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\\\nconstraints upon the set possible models into the definition of a compositional meaning function. \\\\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\\\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\\\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\\\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\\\n\\\\n\\\\n\\\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\\\n\\\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\\\n\\\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\\\n\\\\n \\\\n\\\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\\\n\\\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\\\n\\\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\\\n\\\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\\\n\\\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\\\n\\\\n \\\\n\\\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\\\n\\\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\\\n\\\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\\\n\\\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\\\n\\\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\\\n\\\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\\\n\\\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\\\n\\\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\\\n\\\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\\\n\\\\nsyntactic derivation tree.  \\\\n\\\\n \\\\n\\\\n\\\\n\\\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\\\n\\\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\\\n\\\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\\\n\\\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\\\n\\\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\\\n\\\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\\\n\\\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\\\n\\\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\\\n\\\\n \\\\n\\\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\\\n\\\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\\\n\\\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\\\n\\\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\\\n\\\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\\\n\\\\nsingle disambiguated interpretation. \\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1481. Scores=6.4710183Org Highlight count=75.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}], \\\"intent\\\": \\\"[\\\\\\\"Definition of meaning function\\\\\\\"]\\\"}\",\n",
      "              \"end_turn\": false\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 10108,\n",
      "    \"completion_tokens\": 288,\n",
      "    \"total_tokens\": 10396\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = 3\n",
    "answers = 2\n",
    "\n",
    "openaicall_headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_OPENAI_API_KEY']}\n",
    "openaicall_params  = {'api-version': '2023-12-01-preview'}\n",
    "\n",
    "search_payload = {\n",
    "    \"count\": \"true\",\n",
    "    \"select\": \"id, name, title, location, chunk\",\n",
    "    \"top\": results,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": QUESTION\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1.0,    \n",
    "    \"max_tokens\": 1000,\n",
    "    \"dataSources\": [\n",
    "        {            \n",
    "            \"type\": \"AzureCognitiveSearch\",\n",
    "            \"parameters\": {                \n",
    "                \"endpoint\": os.environ['AZURE_SEARCH_ENDPOINT'],\n",
    "                \"key\": os.environ['AZURE_SEARCH_KEY'],\n",
    "                \"indexName\": index_name\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.post(\n",
    "    f\"{os.environ['AZURE_OPENAI_ENDPOINT']}openai/deployments/{os.environ['GPT4_32K_DEPLOYMENT']}/extensions/chat/completions\",\n",
    "    data=json.dumps(search_payload), headers=openaicall_headers, params=openaicall_params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivbd_20231215",
   "language": "python",
   "name": "openaivbd_20231215"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
