{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58562652",
   "metadata": {},
   "source": [
    "# From files to separated chunk documents... in a single step\n",
    "\n",
    "In Notebook #1 we learnt how to create an index in Azure AI Search and leverage its \"pull\" ability to extract data from files located in storage account. As we saw, this \"extract and fill\" tecnique can be integrated by a \"skillset\" that \"enriches\" extracted data before writing them into the index. Such process is orchestrated by a so called \"indexer\" that maps each source or enriched piece of information against the proper field of the target index.<br/>\n",
    "If we need to split a long text field -typically, the *content* field- in chunks no longer than a predefined length -e.g. 5,000 characters-, we can leverage the **SplitSkill** skill of the skillset, which generates a multi-value field -*Collection(Edm.String)* type- containing the different chunks.<br/>\n",
    "This means, on the other hand, that if we need to retrieve the best ***chunks*** rather than the ***whole content*** of documents due to OpenAI token limits, or just because the whole document contains too much and potentially more confusing information, then we need to create a **secondary index** where we store **one chunk per index row**, possibly associating  specific embeddings, key phrases, emails/urls/persons and other entities to each search document (=chunk).<br/>\n",
    "That's exactly what we did in our Notebook #3; however, in this case we had to face with a few main challenges:\n",
    "- Filling the second index requires a set of manual steps, which we easily wrote in Python, but which creates a potential bottleneck when our scenario needs maintenance and scalability. For example, we had to manually vectorize each chunk and manually *push* the new search document into the secondary index\n",
    "- At the end we get two indexes to manage, where the first one becomes potentially useless\n",
    "- We need to define rules to keep both indexes in sync, and identify update policies. For example, in Notebook #3 we decided to *project and vectorize* only the chunks extracted by a first query, leaving the remaning chunks in their arrays within the first index.\n",
    "\n",
    "These were design decision led by some practical considerations and the general skill-up objective of this solution accelerator; however, they were also forced by some limits associated to Azure AI Search, *until a few days ago*.<br/>\n",
    "Yes, because with the release of the [latest stable version 2023-11-01 of Azure AI Search REST API](https://learn.microsoft.com/en-us/rest/api/searchservice/search-service-api-versions#stable-versions), we can now operate all the above actions with the following benefits:\n",
    "- **pull mathod**, e.g. no manual steps to upload search document to target index\n",
    "- **automatic embedding creation**, that can be performed by a specific skill\n",
    "- **automatic projection** of the array values into separated search documents of the target index\n",
    "\n",
    "As a matter of fact, we actually do not need two indexes any more. **Let's see how to do it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25369fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and assign variables\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials_my.env\")\n",
    "\n",
    "# Name of the container in your Blob Storage Datasource ( in credentials.env)\n",
    "BLOB_CONTAINER_NAME = \"arxivcs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22ae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for the data source, skillset, index and indexer\n",
    "datasource_name = \"cogsrch-datasource-files-onestep\"\n",
    "skillset_name   = \"cogsrch-skillset-files-onestep\"\n",
    "index_name      = \"cogsrch-index-files-vector-onestep\"\n",
    "indexer_name    = \"cogsrch-indexer-files-onestep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c260e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers     = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params      = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}\n",
    "params_old  = {'api-version': os.environ['AZURE_SEARCH_API_VERSION_OLD']} # needed for skillset creation with projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1bb04",
   "metadata": {},
   "source": [
    "## Create Data Source (Blob container with the Arxiv CS pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc983da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# The following code sends the json paylod to Azure Search engine to create the Datasource\n",
    "\n",
    "datasource_payload = {\n",
    "    \"name\": datasource_name,\n",
    "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
    "    \"type\": \"azureblob\",\n",
    "    \"credentials\": {\n",
    "        \"connectionString\": os.environ['BLOB_CONNECTION_STRING']\n",
    "    },\n",
    "    \"dataDeletionDetectionPolicy\" : {\n",
    "        \"@odata.type\" :\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\" # this makes sure that if the item is deleted from the source, it will be deleted from the index\n",
    "    },\n",
    "    \"container\": {\n",
    "        \"name\": BLOB_CONTAINER_NAME\n",
    "    }\n",
    "}\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/datasources/\" + datasource_name,\n",
    "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6d6df",
   "metadata": {},
   "source": [
    "## Create the *SINGLE* Index\n",
    "Since a [vectorizer](https://learn.microsoft.com/en-us/rest/api/searchservice/indexes/create?view=rest-searchservice-2023-10-01-preview&tabs=HTTP#vectorsearch) is used here, we need to use API 2023-10-01-Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f188d775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the single index\n",
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"fields\": [        \n",
    "        {\n",
    "          \"name\": \"id\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"key\": \"true\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\",\n",
    "          \"analyzer\": \"keyword\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"ParentKey\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"facetable\": \"false\",\n",
    "          \"filterable\": \"true\",\n",
    "          \"sortable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"title\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"chunk\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"name\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"location\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"false\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"language\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"true\",\n",
    "          \"filterable\": \"true\",\n",
    "          \"facetable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"persons\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"urls\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"emails\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"key_phrases\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"chunkVector\",\n",
    "          \"type\": \"Collection(Edm.Single)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"dimensions\": 1536,\n",
    "          \"vectorSearchProfile\": \"my_vectorSearch_profile\"\n",
    "        }\n",
    "    ],    \n",
    "  \n",
    "    \"vectorSearch\": {        \n",
    "        \"profiles\": [\n",
    "            {                \n",
    "                \"name\": \"my_vectorSearch_profile\",\n",
    "                \"algorithm\": \"my-vectorSearch-algorithm\",\n",
    "                \"vectorizer\": \"my-embeddings-vectorizer\"\n",
    "            }\n",
    "        ],        \n",
    "        \"algorithms\": [            \n",
    "            {\n",
    "                \"name\": \"my-vectorSearch-algorithm\",\n",
    "                \"kind\": \"hnsw\",\n",
    "                \"hnswParameters\": {                    \n",
    "                    \"m\": 4,\n",
    "                    \"metric\": \"cosine\",\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500\n",
    "                }          \n",
    "            }\n",
    "        ],        \n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"my-embeddings-vectorizer\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\": {\n",
    "                \"resourceUri\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "                \"apiKey\": os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                \"deploymentId\": os.environ['EMBEDDING_DEPLOYMENT']\n",
    "                }\n",
    "            }\n",
    "        ]        \n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params_old)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fef1b",
   "metadata": {},
   "source": [
    "## Create the Skillset\n",
    "### Please note, in this case we need to use version 2023-10-01-Preview of Azure AI Search REST API which currently offers the projection feature\n",
    "- [Reference guide version 2023-10-01-Preview](https://learn.microsoft.com/en-us/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-10-01-Preview&tabs=HTTP)\n",
    "- [Reference guide latest version (2023-11-01)](https://learn.microsoft.com/en-us/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-11-01&tabs=HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e1be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the skillset, including the projection settings into the single index created above\n",
    "skillset_payload = {    \n",
    "  \"name\": skillset_name,\n",
    "  \"description\": \"Detect language, run OCR, merge content+ocr_text, split in chunks, extract entities and key-phrases and embed the chunks\",\n",
    "  \"skills\": [\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
    "      \"name\": \"LanguageDetectionSkill\",\n",
    "      \"context\": \"/document\",\n",
    "      \"description\": \"If you have multilingual content, adding a language code is useful for filtering\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/content\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"targetName\": \"language\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
    "      \"name\": \"OcrSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/normalized_images/*\",\n",
    "      \"textExtractionAlgorithm\": \"\",\n",
    "      \"lineEnding\": \"Space\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"detectOrientation\": \"true\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"image\",\n",
    "          \"source\": \"/document/normalized_images/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"targetName\": \"text_from_ocr\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
    "      \"name\": \"MergeSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document\",\n",
    "      \"insertPreTag\": \" \",\n",
    "      \"insertPostTag\": \" \",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/content\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"itemsToInsert\",\n",
    "          \"source\": \"/document/normalized_images/*/text_from_ocr\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"offsets\",\n",
    "          \"source\": \"/document/normalized_images/*/contentOffset\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"mergedText\",\n",
    "          \"targetName\": \"merged_text\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
    "      \"name\": \"SplitSkill\",\n",
    "      \"context\": \"/document\",\n",
    "      \"textSplitMode\": \"pages\",\n",
    "      \"maximumPageLength\": 5000,\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/merged_text\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"source\": \"/document/language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"textItems\",\n",
    "          \"targetName\": \"chunks\"\n",
    "        }        \n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
    "      \"name\": \"LanguageDetectionSkill_by_chunk\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"description\": \"If you have multilingual content, adding a language code is useful for filtering\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"targetName\": \"chunk_language\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.V3.EntityRecognitionSkill\",\n",
    "      \"name\": \"EntityRecognitionSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"minimumPrecision\": 0.5,\n",
    "      \"modelVersion\": \"\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"languageCode\", \n",
    "            \"source\": \"/document/chunks/*/chunk_language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"persons\",\n",
    "          \"targetName\": \"persons\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"urls\",\n",
    "          \"targetName\": \"urls\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"emails\",\n",
    "          \"targetName\": \"emails\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.KeyPhraseExtractionSkill\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"modelVersion\": \"\",\n",
    "      \"name\": \"KeyPhraseExtractionSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"source\": \"/document/chunks/*/chunk_language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"keyPhrases\",\n",
    "          \"targetName\": \"key_phrases\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
    "      \"name\": \"AzureOpenAIEmbeddingSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"resourceUri\": \"https://mmopenai04.openai.azure.com\",\n",
    "      \"apiKey\": \"23b1db9a7f3a4eeb8a4f1c74bdd7d13d\",\n",
    "      \"deploymentId\": \"text-embedding-ada-002\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"embedding\",\n",
    "          \"targetName\": \"chunk_embedded\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"cognitiveServices\": {\n",
    "    \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
    "    \"description\": os.environ['COG_SERVICES_NAME'],\n",
    "    \"key\": os.environ['COG_SERVICES_KEY']\n",
    "  },\n",
    "  \"indexProjections\": {\n",
    "    \"selectors\": [\n",
    "      {\n",
    "        \"targetIndexName\": index_name,\n",
    "        \"parentKeyFieldName\": \"ParentKey\",\n",
    "        \"sourceContext\": \"/document/chunks/*\",\n",
    "        \"mappings\": [\n",
    "          {\n",
    "            \"name\": \"title\",\n",
    "            \"source\": \"/document/title\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"name\",\n",
    "            \"source\": \"/document/name\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"location\",\n",
    "            \"source\": \"/document/location\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"chunk\",\n",
    "            \"source\": \"/document/chunks/*\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"source\": \"/document/chunks/*/chunk_embedded\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"language\",\n",
    "            \"source\": \"/document/chunks/*/chunk_language\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"persons\",\n",
    "            \"source\": \"/document/chunks/*/persons\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"urls\",\n",
    "            \"source\": \"/document/chunks/*/urls\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"emails\",\n",
    "            \"source\": \"/document/chunks/*/emails\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"key_phrases\",\n",
    "            \"source\": \"/document/chunks/*/key_phrases\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/skillsets/\" + skillset_name,\n",
    "                 data=json.dumps(skillset_payload), headers=headers, params=params_old)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c357a",
   "metadata": {},
   "source": [
    "## Create and Run the Indexer\n",
    "[Reference guide](https://learn.microsoft.com/en-us/rest/api/searchservice/indexers?view=rest-searchservice-2023-11-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a36fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the skillset, including the projection settings into the single index created above\n",
    "indexer_payload = {    \n",
    "  \"name\": indexer_name,\n",
    "  \"dataSourceName\": datasource_name,\n",
    "  \"targetIndexName\": index_name,\n",
    "  \"skillsetName\": skillset_name,  \n",
    "  \"fieldMappings\": [\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_path\",\n",
    "      \"targetFieldName\": \"id\",\n",
    "      \"mappingFunction\": {\n",
    "        \"name\": \"base64Encode\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_title\",\n",
    "      \"targetFieldName\": \"title\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_name\",\n",
    "      \"targetFieldName\": \"name\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_path\",\n",
    "      \"targetFieldName\": \"location\"\n",
    "    }\n",
    "  ],\n",
    "  \"outputFieldMappings\": [],\n",
    "  \"parameters\": {\n",
    "    \"maxFailedItems\": -1,\n",
    "    \"maxFailedItemsPerBatch\": -1,\n",
    "    \"configuration\": {\n",
    "      \"dataToExtract\": \"contentAndMetadata\",\n",
    "      \"parsingMode\": \"default\",\n",
    "      \"imageAction\": \"generateNormalizedImages\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name,\n",
    "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabdd398",
   "metadata": {},
   "source": [
    "### Please wait for a few minutes after running the cell above to allow the Indexer to fill the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9e256",
   "metadata": {},
   "source": [
    "## And now... let's search!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55f265",
   "metadata": {},
   "source": [
    "Let's start with a text search. Recall:\n",
    "- The **value** key contains the ***sequence of results*** returned by the query\n",
    "- The **@search.answers** key contain the the ***answers*** query results for the search operation. They include three pieces of information:\n",
    "  1. **key**: the search document key\n",
    "  2. **score**: the score associated to that semantic answer\n",
    "  3. **text**: the so called ***captions*** that contain the most representative passages from the document relatively to the search query. They are often used as document summary. Captions are only returned for queries of type semantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39c95a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 313,\n",
      "  \"@search.answers\": [\n",
      "    {\n",
      "      \"key\": \"14597253b55e_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"text\": \"A meaning function is a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings; typically, some set-theoretic objects like lists of features or functions.\",\n",
      "      \"highlights\": \"A meaning function is<em> a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings;</em> typically, some set-theoretic objects like lists of features or functions.\",\n",
      "      \"score\": 0.9970703125\n",
      "    },\n",
      "    {\n",
      "      \"key\": \"14597253b55e_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_4\",\n",
      "      \"text\": \"A meaning function \\u00b5 is a compositional semantics for the set S if its domain is contained in S, and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "      \"highlights\": \"A meaning function \\u00b5 is<em> a compositional semantics for the set S if its domain is contained in S,</em> and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "      \"score\": 0.8466796875\n",
      "    }\n",
      "  ],\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 9.681817,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"A meaning function is a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings; typically, some set-theoretic objects like lists of features or functions.\",\n",
      "          \"highlights\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 3.868259906768799,\n",
      "      \"id\": \"14597253b55e_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 10.012405,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"A meaning function \\u00b5 is a compositional semantics for the set S if its domain is contained in S, and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "          \"highlights\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 3.6001837253570557,\n",
      "      \"id\": \"14597253b55e_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_4\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"grammar \\u2014 its total length is less than 300 symbols (vs. 18,000);\\nbut it assumes an existence of a language generator. Interestingly,\\nthe grammar resembles the compositional semantics, as usually\\ngiven. The rule with V (1) and N(1) describes the compositional\\npart of the corpus; the rule with v0 and n0 \\u2013 the idiomatic; other\\nrules are in between.\\n\\n3.4 Variations on the MDL algorithm\\n\\nA similar result is obtained when we do not insist that all instances\\nof merging classes are replaced by the result of the merge. Starting\\nwith the grammar\\n\\nGrammar GV (1):\\n\\nV (1) = {v1| ... |v9} (21)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{V (1)}{n1}{action}{V (1)}{object}{n1} (18)\\n...\\n\\n{V (1)}{n99}{action}{V (1)}{object}{n99} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n{v0}{n1}{action}{v0}{object}{n1} (18)\\n... (18)\\n{v0}{n99}{action}{v0}{object}{n99} (18)\\n\\n14\\n\\n\\n\\nWe can see that the merge V (0) = {v0|V (1)} will decrease the size\\nof the grammar by 99 rules and result in:\\n\\nGrammar GV (0):\\n\\nV (1) = {v1| ... |v9} (21)\\nV (0) = {v0|V (1)} (7)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{V (0)}{n1}{action}{V (0)}{object}{n1} (18)\\n...\\n\\n{V (0)}{n99}{action}{V (0)}{object}{n99} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nThe successive merging of nouns will then produce\\n\\nGrammar GV (0)N(1):\\n\\nV (0) = {v0|V (1)} (7)\\nV (1) = {v1| ... |v9} (21)\\nN(1) = {n1| ... |n99} (201)\\n{V (0)}{N(1)}{action}{V (0)}{object}{N(1)} (18)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nIf, however we do not do the V (0) = {v0|V (1)} merge, and\\nproceed with the merging of the nouns (e.g. if there were reasons\\nto modify the algorithm), we get:\\n\\n15\\n\\n\\n\\nGrammar GV (1)N(0):\\n\\nV (1) = {v1| ... |v9} (21)\\nN(1) = {n1| ... |n99} (201)\\nN(0) = {n0|N(1)} (7)\\n{V (1)}{N(0)}{action}{V (1)}{object}{N(0)} (18)\\n{v0}{N(1)}{action}{v0}{object}{N(1)} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nFinally, if we allow some overgeneralization, we can replace the\\nabove grammars with an even shorter grammar:\\n\\nGrammar GV (0)N(0):\\n\\nV = {v0| ... |v9} (23)\\nN = {n0| ... |n99} (203)\\n{V }{N}{action}{V }{object}{N} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nHere, clearly v0 is the idiomatic element. However, both idiomatic\\nand non-idiomatic reading of kick bucket is allowed. (In the pre-\\nviously defined grammars, we can also see the distinction between\\nthe idiomatic and non-idiomatic elements).\\n\\n4 A non-vacuous definition of composition-\\n\\nality\\n\\nThe fact that that the MDL principle can produce an object re-\\nsembling a compositional semantics is crucial. It allows us to argue\\nfor a non-vacuous definition of compositionality.\\n\\nAssume that we have a corpus S of sentences and their parts,\\ngiven either as a set or generated by a grammar. Let sentences\\n\\n16\\n\\n\\n\\nand their parts be collections of symbols put together by some\\noperations; in the simplest and most important case, by concate-\\nnation \\u201d.\\u201d.\\n\\nDefinition. A meaning function \\u00b5 is a compositional semantics\\nfor the set S if its domain is contained in S, and\\na. it satisfies the postulate of compositionality: for all s, t in its\\ndomain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nb. it is the shortest, in the sense of the Minimum Description\\nLength principle, such an encoding.\\nc. it is maximal, i.e. there is no \\u00b5\\u2032 with a larger domain that\\nsatisfies a and b.\\n\\nTo see better what this definition entails, let us consider our\\nsemantic corpus again. The set S consists of the 10 verbs and\\n100 nouns and all noun-verb combinations. The compositional\\nfunction \\u00b5 assigns to each word its category e.g. [n17, noun].\\nThe question is how to define the operator \\u2295. Because of the\\nidiom, it cannot be a total function; hence we have to exclude\\nfrom the domain of \\u2295 the pair [[v0, verb], [n0, noun]]. The short-\\nest description of \\u2295 can be given by translating the grammar of\\nSection 3.2. First, map non-idiomatic verbs and nouns into pairs\\n\\u00b5(vi) = [vi, verbnonid], \\u00b5(nj) = [nj , nounnonid], i, j > 0. Then,\\nput\\n\\n\\u2295([[v, verbnonid], [n, nounnonid]]) = [action.v, object.n]\\n\\nThus defined \\u00b5 and \\u2295 correspond to the grammar obtained by the\\nalgorithm of Section 3.2 and to the tables of Section 2. This cor-\\nrespondence is not exact, because functions \\u00b5 and \\u2295 encode only\\nthe systematic, compositional part of the corpus. (But please note\\nthis clear distinction between the idiomatic and the compositional\\nparts of the lexicon and the corpus).\\n\\n17\\n\\n\\n\\nHowever this description of the two functions is not maximal.\\nWe obtain the maximal compositional semantics for S by extend-\\ning the above defined mapping to all nouns \\u00b5(nj) = [nj, noun],\\nj \\u2265 0, and extending the domain of \\u2295\\n\\n\\u2295([[v, verbnonid], [n, noun]]) = [action.v, object.n]\\n\\nIt is easily checked that this is the shortest (in the sense of the\\nMDL) and maximal assignment of meaning to the elements of set\\nS.4 Please compare this mapping with GV (1)N(0), and also note\\nthat now we have a formal basis for saying that (for this corpus)\\nit is the verb kick, and not the noun bucket, that is idiomatic.\\n\\n\",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 8.051845,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"Since this chapter deals with sizes of objects, we compute them for the meaning functions: the size of the first function is 90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79.\",\n",
      "          \"highlights\": \"Since this chapter deals with sizes of objects, we compute them for the<em> meaning functions:</em> the size of the first function is 90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79.\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 2.895526885986328,\n",
      "      \"id\": \"14597253b55e_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_2\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"If our corpus were to be the 10 \\u00d7 100 table consisting\\nof all verb-noun combinations:\\n\\nv0n0 + v1n0 + ... + vjni + ...\\n\\nwe could quickly use the previous example to write a simple finite\\nstate grammar that describes the corpus:\\n\\n{v0|v1|...}{n0|n1...} (21 + 201)\\n\\nBut in this subsection we are supposed to introduce some seman-\\ntics. Thus, let our corpus consist of all those 1,000 sentences to-\\ngether with their meanings, which, to keep things as simple as\\npossible, will be simplified to two attributes. Also, for the reason\\nof simplicity, we assume that only \\u201dkick bucket\\u201d has an idiomatic\\nmeaning, and all other entries are assigned the meaning consisting\\nof the two attribute expression [[action, vj ], [object, ni]]. Hence,\\nour corpus will look as follows:\\n\\nkick bucket action die object nil\\n\\nv1 bucket action v1 object bucket\\n\\n...\\n\\nvj ni action vj object ni\\n\\n...\\n\\nNow, notice that this corpus cannot be encoded by means of a\\nshort finite state grammar, because of the dependence of the mean-\\nings (i.e. the pair [action, ..., object, ...]) on the first two elements\\nof each sentence. We will have to extend our grammar formalism\\nto address this dependence (Section 3).\\n\\n7\\n\\n\\n\\n2.4 On meaning functions\\n\\nEven though we cannot encode the corpus by a short, finite state\\ngrammar, we can easily provide for it a compositional semantics.\\nTo avoid the complications of type raising, we will build a homo-\\nmorphic mapping from syntax to semantics. To do it, it is enough\\nto build meaning functions in a manner ensuring that the meaning\\nof each vjni is composed from the meaning of vj and the mean-\\ning of ni. Since our corpus is simple, these meaning functions are\\nsimple, too: For the verbs the meaning function is given by the\\ntable:\\n\\n[v0, [verb, v0]]; [v1, [verb, v1]] ... [v9, [verb, v9]] (90)\\n\\nFor the nouns:\\n\\n[n0, [noun, n0]]; [n1, [noun, n1]] ... [n99, [noun, n99]] (900)\\n\\nWe have represented both meaning functions as tables of sym-\\nbols. Since this chapter deals with sizes of objects, we compute\\nthem for the meaning functions: the size of the first function is\\n90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79. Therefore,\\nthe meaning function for the whole corpus could be represented\\nas a table with 1,000 entries:\\n\\n[[[verb, v9], [noun, n99]], [[action, v9], [object, n99]]]\\n. . .\\n[[[verb, vj ], [noun, ni]], [[action, vj ], [object, ni]]]\\n. . .\\n[[[verb, v1], [noun, bucket]], [[action, v1 ], [object, bucket]]]\\n[[[verb, kick], [noun, bucket]], [[action, die], [object, nil]]\\n\\nand the size of this table is 29\\u00d71000. Finally, the total size of the\\ntables that describe the compositional interpretation of the corpus\\nis 29000 + 900 + 90, i.e. roughly 30, 000. Notice that if we had\\nmore verbs and nouns, the tables describing the meaning functions\\n\\n8\\n\\n\\n\\nwould be even larger.2 Also, note that we have not counted the\\ncost of encoding the positions of elements of the table, which would\\nbe the log of the total number of symbols in the table. This\\nsimplifying assumption does not change anything in the strength\\nof our arguments (as larger tables have longer encodings).\\n\\n3 Compositional semantics through the Min-\\n\\nimum Description Length principle\\n\\nIn this section we first extend our notation to deal with semantic\\ngrammars. Then we apply the minimum description length princi-\\nple to construct a compact representation of our example corpus.\\nThis experience will motivate our new, non-vacuous definition of\\nthe notion of compositional semantics given in Section 4.\\n\\n3.1 Representations\\n\\nWe have seen that it is impossible to efficiently encode our se-\\nmantic corpus using a finite state grammar. Therefore, we have\\nto make our representation of grammars more expressive (at the\\nprice of a slightly bigger interpreter). Namely, we will allow a sim-\\nple form of unification.\\n\\n2 The reader familiar with [12] should notice that the meaning functions\\nobtained by the solution lemma also consist of tables of element-value pairs. It\\nis easy to see that for the corpus we are encoding the solution lemma produces\\nthe same meaning functions.\\n\\nIn the other direction, the method for deriving compositional semantics us-\\ning the minimum description length principle (Sections 3 and 4) are directly\\napplicable to meaning functions obtained by the solution lemma in [12], pro-\\nvided they are finite (which covers the practically interesting cases); and it\\nseems applicable to the infinite case, if it has a finite representation. However,\\nwe will not pursue this connection any further.\\n\\n9\\n\\n\\n\\nExample. Assume we do not want {a|b}{a|b|d} to generate ab.\\nWe can do it by changing the notation:\\n\\nX = {a|b}\\n{X}{X|d}\\n\\nThe intention is simple: first, we define a class variable (X) for the\\nclass consisting of elements a and b; then, we generate all strings\\nusing the rule with variable X: XX and Xd; and finally we substi-\\ntute for X all its possible values, which produces aa, ad, bb and bd.\\n\\nMore generally, let us assume that we have an alphabet a1, a2, ...,\\n\",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Three full text answers plus two semantic answers\n",
    "results = 3\n",
    "answers = 2\n",
    "QUESTION = \"What is a meaning function?\"\n",
    "\n",
    "import requests, json\n",
    "\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params  = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']} # NEW VERSION\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"search\": QUESTION,\n",
    "  \"count\": \"true\",\n",
    "  \"top\": results,\n",
    "  \"select\": \"id, name, title, chunk, language\",\n",
    "  \"queryType\": \"semantic\",\n",
    "  \"semanticConfiguration\": \"my-semantic-config\",\n",
    "  \"captions\": \"extractive\",\n",
    "  \"answers\": f\"extractive|count-{answers}\"\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25613c4",
   "metadata": {},
   "source": [
    "### Now let's do a *vector* query. \n",
    "We can use two possible ***vectorQueries*** settings in the search payload:\n",
    "1. [**\"kind\": \"vector\"**](https://learn.microsoft.com/en-us/rest/api/searchservice/documents/search-post?view=rest-searchservice-2023-11-01&tabs=HTTP#rawvectorquery), when a raw vector value is provided, such as an Azure OpenAI Embedding. In this case, of course, we need to **manually** convert and pass the query embedding.\n",
    "2. [**\"kind\": \"text\"**](https://learn.microsoft.com/en-us/rest/api/searchservice/documents/search-post?view=rest-searchservice-2023-10-01-Preview&tabs=HTTP#rawvectorquery), which accepts a text that is **automatically** converted into an embedding thanks to the **vectorized** parameter associated to the \"chunkVector\" field in the indexer. **Currently, only REST API 2023-10-01-Preview supports this feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7e9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 2,\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 0.84926814,\n",
      "      \"id\": \"7d798d7a32af_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0_chunks_3\",\n",
      "      \"title\": \"Microsoft Word - LP99-sub.htm\",\n",
      "      \"chunk\": \"requirement that the meaning of an expression be functionally determined by its structure and the \\n\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\n\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\n\\n \\n\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\n\\nMEANING \\n\\n \\n\\n    The importance which has been assigned to compositionality in semantic theory can be \\n\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\n\\n                                                                                                                                                                                           \\nsense of inter-substitutivity.  \\n\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\nconstraints upon the set possible models into the definition of a compositional meaning function. \\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\n\\n\\n\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\n\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\n\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\n\\n \\n\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\n\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\n\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\n\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\n\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\n\\n \\n\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\n\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\n\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\n\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\n\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\n\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\n\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\n\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\n\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\n\\nsyntactic derivation tree.  \\n\\n \\n\\n\\n\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\n\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\n\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\n\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\n\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\n\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\n\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\n\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\n\\n \\n\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\n\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\n\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\n\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\n\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\n\\nsingle disambiguated interpretation. \",\n",
      "      \"name\": \"0001006v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001006v1.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 0.8394798,\n",
      "      \"id\": \"14597253b55e_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001002v1.pdf\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# \"kind\": \"vector\"\n",
    "\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "embedder = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\", skip_empty=True)\n",
    "VECTORIZED_QUESTION = embedder.embed_query(QUESTION)\n",
    "results = 3\n",
    "answers = 2\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"count\": \"true\",\n",
    "  \"select\": \"id, name, title, location, chunk\",\n",
    "  \"top\": results,\n",
    "  \"vectorQueries\": [\n",
    "    {\n",
    "      \"kind\": \"vector\",\n",
    "      \"k\": answers,\n",
    "      \"fields\": \"chunkVector\",\n",
    "      \"vector\": VECTORIZED_QUESTION\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f0c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 2,\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 0.84926814,\n",
      "      \"id\": \"7d798d7a32af_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0_chunks_3\",\n",
      "      \"title\": \"Microsoft Word - LP99-sub.htm\",\n",
      "      \"chunk\": \"requirement that the meaning of an expression be functionally determined by its structure and the \\n\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\n\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\n\\n \\n\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\n\\nMEANING \\n\\n \\n\\n    The importance which has been assigned to compositionality in semantic theory can be \\n\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\n\\n                                                                                                                                                                                           \\nsense of inter-substitutivity.  \\n\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\nconstraints upon the set possible models into the definition of a compositional meaning function. \\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\n\\n\\n\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\n\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\n\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\n\\n \\n\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\n\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\n\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\n\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\n\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\n\\n \\n\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\n\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\n\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\n\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\n\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\n\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\n\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\n\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\n\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\n\\nsyntactic derivation tree.  \\n\\n \\n\\n\\n\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\n\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\n\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\n\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\n\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\n\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\n\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\n\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\n\\n \\n\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\n\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\n\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\n\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\n\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\n\\nsingle disambiguated interpretation. \",\n",
      "      \"name\": \"0001006v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001006v1.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 0.8394798,\n",
      "      \"id\": \"14597253b55e_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001002v1.pdf\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# \"kind\": \"text\" - requires 2023-10-01-Preview version of Azure AI HTTP REST API\n",
    "\n",
    "results = 3\n",
    "answers = 2\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"count\": \"true\",\n",
    "  \"select\": \"id, name, title, location, chunk\",\n",
    "  \"top\": results,\n",
    "  \"vectorQueries\": [\n",
    "    {\n",
    "      \"kind\": \"text\",\n",
    "      \"k\": answers,\n",
    "      \"fields\": \"chunkVector\", \n",
    "      \"text\": QUESTION\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params_old) # 2023-10-01-Preview version\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc66a19",
   "metadata": {},
   "source": [
    "## ...et *voilà*, the two answers are 100% identical\n",
    "You want to confirm the differences, go [here](https://www.diffchecker.com/text-compare/) ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3d535",
   "metadata": {},
   "source": [
    "# BONUS cell: using Azure OpenAI Extensions to make a single OpenAI call that implicitly contacts Azure AI Search\n",
    "Finally, we make a single call to OpenAI, passing all the necessary information to access to Azure Search (its endpoint, authentication method, index name…) in the payload to extract the top chunks from the vector index we built in Azure AI Search.\n",
    "\n",
    "## At this point, the question arises: Who calls Who?\n",
    "…and the answer is that **OpenAI calls Azure Search**, there’s no doubt about it as we’ve only invoked the Azure OpenAI endpoint. So here’s how to implement this second method, which is certainly more practical than the previous one, even if potentially less flexible and controllable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54baf406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"8af72856-f6b5-4c34-902d-78c404c6d9f1\",\n",
      "  \"model\": \"gpt-4\",\n",
      "  \"created\": 1703689909,\n",
      "  \"object\": \"extensions.chat.completion\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"A meaning function is a (possibly partial) function that maps sentences (and their parts) into (a representation of) their meanings, which are typically some set-theoretic objects like lists of features or functions[doc5]. It is compositional if for all elements in its domain, it satisfies the postulate of compositionality, meaning that the meaning of a sentence is functionally determined by the meanings of its parts[doc5]. In formal terms, for any sentences `s` and `t` in its domain, a compositional meaning function \\\\(\\\\mu\\\\) would satisfy the equation \\\\(\\\\mu(s.t) = \\\\mu(s) \\\\otimes \\\\mu(t)\\\\), where `.` denotes the concatenation of symbols and \\\\(\\\\otimes\\\\) is a function of two arguments[doc5].\",\n",
      "        \"end_turn\": true,\n",
      "        \"context\": {\n",
      "          \"messages\": [\n",
      "            {\n",
      "              \"role\": \"tool\",\n",
      "              \"content\": \"{\\\"citations\\\": [{\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\nrequirement that the meaning of an expression be functionally determined by its structure and the \\\\n\\\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\\\n\\\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\\\n\\\\n \\\\n\\\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\\\n\\\\nMEANING \\\\n\\\\n \\\\n\\\\n    The importance which has been assigned to compositionality in semantic theory can be \\\\n\\\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\\\n\\\\n                                                                                                                                                                                           \\\\nsense of inter-substitutivity.  \\\\n\\\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\\\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\\\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\\\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\\\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\\\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\\\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\\\nconstraints upon the set possible models into the definition of a compositional meaning function. \\\\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\\\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\\\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\\\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\\\n\\\\n\\\\n\\\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\\\n\\\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\\\n\\\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\\\n\\\\n \\\\n\\\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\\\n\\\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\\\n\\\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\\\n\\\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\\\n\\\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\\\n\\\\n \\\\n\\\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\\\n\\\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\\\n\\\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\\\n\\\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\\\n\\\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\\\n\\\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\\\n\\\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\\\n\\\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\\\n\\\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\\\n\\\\nsyntactic derivation tree.  \\\\n\\\\n \\\\n\\\\n\\\\n\\\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\\\n\\\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\\\n\\\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\\\n\\\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\\\n\\\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\\\n\\\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\\\n\\\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\\\n\\\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\\\n\\\\n \\\\n\\\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\\\n\\\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\\\n\\\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\\\n\\\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\\\n\\\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\\\n\\\\nsingle disambiguated interpretation. \\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1481. Scores=7.4181857Org Highlight count=23.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0\\\\ngrammar \\u2014 its total length is less than 300 symbols (vs. 18,000);\\\\nbut it assumes an existence of a language generator. Interestingly,\\\\nthe grammar resembles the compositional semantics, as usually\\\\ngiven. The rule with V (1) and N(1) describes the compositional\\\\npart of the corpus; the rule with v0 and n0 \\u2013 the idiomatic; other\\\\nrules are in between.\\\\n\\\\n3.4 Variations on the MDL algorithm\\\\n\\\\nA similar result is obtained when we do not insist that all instances\\\\nof merging classes are replaced by the result of the merge. Starting\\\\nwith the grammar\\\\n\\\\nGrammar GV (1):\\\\n\\\\nV (1) = {v1| ... |v9} (21)\\\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\\\n{V (1)}{n1}{action}{V (1)}{object}{n1} (18)\\\\n...\\\\n\\\\n{V (1)}{n99}{action}{V (1)}{object}{n99} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n{v0}{n1}{action}{v0}{object}{n1} (18)\\\\n... (18)\\\\n{v0}{n99}{action}{v0}{object}{n99} (18)\\\\n\\\\n14\\\\n\\\\n\\\\n\\\\nWe can see that the merge V (0) = {v0|V (1)} will decrease the size\\\\nof the grammar by 99 rules and result in:\\\\n\\\\nGrammar GV (0):\\\\n\\\\nV (1) = {v1| ... |v9} (21)\\\\nV (0) = {v0|V (1)} (7)\\\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\\\n{V (0)}{n1}{action}{V (0)}{object}{n1} (18)\\\\n...\\\\n\\\\n{V (0)}{n99}{action}{V (0)}{object}{n99} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nThe successive merging of nouns will then produce\\\\n\\\\nGrammar GV (0)N(1):\\\\n\\\\nV (0) = {v0|V (1)} (7)\\\\nV (1) = {v1| ... |v9} (21)\\\\nN(1) = {n1| ... |n99} (201)\\\\n{V (0)}{N(1)}{action}{V (0)}{object}{N(1)} (18)\\\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nIf, however we do not do the V (0) = {v0|V (1)} merge, and\\\\nproceed with the merging of the nouns (e.g. if there were reasons\\\\nto modify the algorithm), we get:\\\\n\\\\n15\\\\n\\\\n\\\\n\\\\nGrammar GV (1)N(0):\\\\n\\\\nV (1) = {v1| ... |v9} (21)\\\\nN(1) = {n1| ... |n99} (201)\\\\nN(0) = {n0|N(1)} (7)\\\\n{V (1)}{N(0)}{action}{V (1)}{object}{N(0)} (18)\\\\n{v0}{N(1)}{action}{v0}{object}{N(1)} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nFinally, if we allow some overgeneralization, we can replace the\\\\nabove grammars with an even shorter grammar:\\\\n\\\\nGrammar GV (0)N(0):\\\\n\\\\nV = {v0| ... |v9} (23)\\\\nN = {n0| ... |n99} (203)\\\\n{V }{N}{action}{V }{object}{N} (18)\\\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\\\n\\\\nHere, clearly v0 is the idiomatic element. However, both idiomatic\\\\nand non-idiomatic reading of kick bucket is allowed. (In the pre-\\\\nviously defined grammars, we can also see the distinction between\\\\nthe idiomatic and non-idiomatic elements).\\\\n\\\\n4 A non-vacuous definition of composition-\\\\n\\\\nality\\\\n\\\\nThe fact that that the MDL principle can produce an object re-\\\\nsembling a compositional semantics is crucial. It allows us to argue\\\\nfor a non-vacuous definition of compositionality.\\\\n\\\\nAssume that we have a corpus S of sentences and their parts,\\\\ngiven either as a set or generated by a grammar. Let sentences\\\\n\\\\n16\\\\n\\\\n\\\\n\\\\nand their parts be collections of symbols put together by some\\\\noperations; in the simplest and most important case, by concate-\\\\nnation \\u201d.\\u201d.\\\\n\\\\nDefinition. A meaning function \\u00b5 is a compositional semantics\\\\nfor the set S if its domain is contained in S, and\\\\na. it satisfies the postulate of compositionality: for all s, t in its\\\\ndomain:\\\\n\\\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\\\n\\\\nb. it is the shortest, in the sense of the Minimum Description\\\\nLength principle, such an encoding.\\\\nc. it is maximal, i.e. there is no \\u00b5\\u2032 with a larger domain that\\\\nsatisfies a and b.\\\\n\\\\nTo see better what this definition entails, let us consider our\\\\nsemantic corpus again. The set S consists of the 10 verbs and\\\\n100 nouns and all noun-verb combinations. The compositional\\\\nfunction \\u00b5 assigns to each word its category e.g. [n17, noun].\\\\nThe question is how to define the operator \\u2295. Because of the\\\\nidiom, it cannot be a total function; hence we have to exclude\\\\nfrom the domain of \\u2295 the pair [[v0, verb], [n0, noun]]. The short-\\\\nest description of \\u2295 can be given by translating the grammar of\\\\nSection 3.2. First, map non-idiomatic verbs and nouns into pairs\\\\n\\u00b5(vi) = [vi, verbnonid], \\u00b5(nj) = [nj , nounnonid], i, j > 0. Then,\\\\nput\\\\n\\\\n\\u2295([[v, verbnonid], [n, nounnonid]]) = [action.v, object.n]\\\\n\\\\nThus defined \\u00b5 and \\u2295 correspond to the grammar obtained by the\\\\nalgorithm of Section 3.2 and to the tables of Section 2. This cor-\\\\nrespondence is not exact, because functions \\u00b5 and \\u2295 encode only\\\\nthe systematic, compositional part of the corpus. (But please note\\\\nthis clear distinction between the idiomatic and the compositional\\\\nparts of the lexicon and the corpus).\\\\n\\\\n17\\\\n\\\\n\\\\n\\\\nHowever this description of the two functions is not maximal.\\\\nWe obtain the maximal compositional semantics for S by extend-\\\\ning the above defined mapping to all nouns \\u00b5(nj) = [nj, noun],\\\\nj \\u2265 0, and extending the domain of \\u2295\\\\n\\\\n\\u2295([[v, verbnonid], [n, noun]]) = [action.v, object.n]\\\\n\\\\nIt is easily checked that this is the shortest (in the sense of the\\\\nMDL) and maximal assignment of meaning to the elements of set\\\\nS.4 Please compare this mapping with GV (1)N(0), and also note\\\\nthat now we have a formal basis for saying that (for this corpus)\\\\nit is the verb kick, and not the noun bucket, that is idiomatic.\\\\n\\\\n\\\\nen\\\\n0001002v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1854. Scores=6.594784Org Highlight count=9.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\n\\\\nar\\\\nX\\\\n\\\\niv\\\\n:c\\\\n\\\\ns.\\\\nC\\\\n\\\\nL\\\\n/0\\\\n\\\\n00\\\\n10\\\\n\\\\n06\\\\n   \\\\n\\\\n9 \\\\nJa\\\\n\\\\nn \\\\n20\\\\n\\\\n00\\\\n\\\\nCOMPOSITIONALITY, SYNONYMY, AND THE SYSTEMATIC REPRESENTATION \\\\nOF MEANING  \\\\n \\\\n                     Shalom Lappin                   and          Wlodek Zadrozny \\\\n                     King\\u2019s College London                       IBM T.J. Watson Research Center \\\\n                     shalom.lappin@kcl.ac.uk                   wlodz@us.ibm.com \\\\n \\\\n \\\\n\\\\n \\\\n\\\\n1. INTRODUCTION     \\\\n\\\\n     In a recent issue of Linguistics and Philosophy Kasmi and Pelletier (1998) (K&P), and \\\\n\\\\nWesterst\\u00e5hl (1998) criticize Zadrozny's (1994) argument that any semantics can be represented \\\\n\\\\ncompositionally. The argument is based upon Zadrozny's theorem that every meaning function m \\\\n\\\\ncan be encoded by a function \\u00b5 such that (i) for any expression E of a specified language L, m(E) \\\\n\\\\ncan be recovered from \\u00b5(E), and (ii) \\u00b5 is a homomorphism from the syntactic structures of L to \\\\n\\\\ninterpretations of L. In both cases, the primary motivation for the objections brought against \\\\n\\\\nZadrozny\\u2019s argument is the view that his encoding of the original meaning function does not \\\\n\\\\nproperly reflect the synonymy relations posited for the language.1 \\\\n\\\\n \\\\n\\\\n In this paper we will look at both the technical issues that K&P and Westerst\\u00e5hl raise, \\\\n\\\\nand the relation between synonymy and compositionality. We will argue that their technical \\\\n\\\\ncriticisms do not go through, while the objection from synonymy assumes an additional \\\\n\\\\nconstraint on compositionality, which they do not make explicit. We also correct some \\\\n\\\\nmisconceptions about the function \\u00b5, e.g. Janssen (1997). We prove that \\u00b5 properly encodes \\\\n\\\\nsynonymy relations, i.e. if two expressions are synonymous, then their compositional meanings \\\\n\\\\nare identical. In Sections 2 and 3 we take up K&P's and Westerst\\u00e5hl's formal objections. Section \\\\n\\\\n\\\\n\\\\n4 considers the relation between compositionality and the systematic representation of meaning. \\\\n\\\\nWe suggest that the reason that semanticists have been anxious to preserve compositionality as a \\\\n\\\\nsignificant constraint on semantic theory is that it has been mistakenly regarded as a condition \\\\n\\\\nthat must be satisfied by any theory that sustains a systematic connection between the meaning \\\\n\\\\nof an expression and the meanings of its parts. Recent developments in formal and computational \\\\n\\\\nsemantics show that systematic theories of meanings need not be compositional. \\\\n\\\\n \\\\n\\\\n Zadrozny\\u2019s argument is based on the observation that a function is defined as a collection \\\\n\\\\nof pairs <argument, value>, such that each argument has only one value in the collection (in \\\\n\\\\ncontrast to a relation in which an argument can be associated with more than one value). The \\\\n\\\\nclassical version of compositionality is defined as a functional dependence of the meaning of an \\\\n\\\\nexpression on the meaning of its parts. Since many semantic theories represent meanings as \\\\n\\\\nfunctions, the requirement of compositionality can be expressed as a condition on the <argument, \\\\n\\\\nvalue> pairs that define the function which assigns meanings to the expressions of a language. \\\\n\\\\nSpecifically, \\u00b5(s.t) = \\u00b5(\\u00b5(s),\\u00b5(t)) means that \\u00b5 must have among its elements the pairs <s.t, \\\\n\\\\n\\u00b5(\\u00b5(s),\\u00b5(t))>, <s,\\u00b5(s)>, and <t,\\u00b5(t)>. Using this approach, the following theorem is proven. \\\\n\\\\n \\\\n\\\\nTHEOREM 1: Let M be an arbitrary set. Let A be an arbitrary alphabet. Let \\u2018.\\u2019 be a binary \\\\n\\\\noperation (concatenation), and let S be the set closure of A under \\u2018.\\u2019. Let m: S dM be an arbitrary \\\\n\\\\nfunction. Then there is a set of functions M* and a unique map \\u00b5: Sd M* such that for all s,t in S, \\\\n\\\\n\\u00b5(s.t) = \\u00b5(\\u00b5(s),\\u00b5(t)) = \\u00b5(s)(\\u00b5(t)) , and \\u00b5(\\u00b5(s)) = m(s).  \\\\n\\\\n \\\\n\\\\n                                                                                                                                                                                           \\\\n1Janssen (1997) makes a similar claim. \\\\n\\\\n\\\\n\\\\nTheorem 1 entails that even if for the original meaning function m of a language L, m(A) \\\\n\\\\n= m(B) and m(C.A) g m(C.B), there is a function \\u00b5 such that \\u00b5(C.A) = \\u00b5(C)(\\u00b5(A)), and \\u00b5(C.B) = \\\\n\\\\n\\u00b5(C)(\\u00b5(B)).2  \\\\n\\\\n \\\\n\\\\n In the same paper, it is shown (Corollary 2) that the language L doesn\\u2019t have to be closed \\\\n\\\\nunder concatenation, and (Proposition 3) that the original meanings given by m(s) can be \\\\n\\\\nrecovered in other ways, e.g. by having  \\u00b5(s)($) = m(s), for some element $. We will use the last \\\\n\\\\nfact (and method) in the proof of the Synonymy Theorem in Section 3.    \\\\n\\\\n \\\\n\\\\n2. FUNCTIONALITY, TYPE RAISING  and  K&P\\u2019s OBJECTIONS  \\\\n\\\\n \\\\n\\\\n    Kasmi and Pelletier (1998) (K&P) raise two main objections. First, they observe that \\\\n\\\\nZadrozny\\u2019s encoding of L\\u2019s semantics substitutes functions for the original meanings that m \\\\n\\\\nassigns to the expressions of L. So, for example, \\u00b5(A) = f1 rather than m(A), and \\u00b5(B) = f2 rather \\\\n\\\\nthan m(B), where m(A) = m(B). They conclude that \\u00b5 is not a meaning function in the sense that \\\\n\\\\nm is. Therefore, while \\u00b5 is indeed compositional, it does not encode the semantics of L. \\\\n\\\\n \\\\n\\\\n\\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1851. Scores=6.759312Org Highlight count=19.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\nThis set is, in effect, the disjunction of interpretations which \\\\n\\\\ncan be assigned to E in particular contexts. A relational theory of meaning represents the \\\\n\\\\ninterpretation of a sentence as underspecified to the extent that it defines its semantic value as a \\\\n\\\\nset of possibly incompatible alternative interpretations. \\\\n\\\\n \\\\n\\\\n    An early instance of such a relational theory of meaning is Cooper\\u2019s (1983) use of quantifier \\\\n\\\\nstorage to associate alternative quantifier scope readings with a single syntactic structure. In the \\\\n\\\\n                                                           \\\\n5See Nerbonne (1996) for a non-compositional approach to semantics in a constraint-based \\\\nframework that focuses on lexical and syntactic ambiguity. Lappin (forthcoming) gives an \\\\noverview of non-compositional representations of scope. \\\\n \\\\n\\\\n\\\\n\\\\ncomputational model of storage presented in Pereira and Shieber (1987) and Pereira (1990) the \\\\n\\\\nsyntactic structure ST of a sentence which contains quantified NP\\u2019s is assigned a set of scope \\\\n\\\\ninterpretations where each element of this set imposes a relative scope ordering on the semantic \\\\n\\\\nrepresentations of the quantifiers in ST.  \\\\n\\\\n \\\\n\\\\n    Recent work in underspecified semantics, such as Reyle (1993), Copestake et al. (1995), \\\\n\\\\nCrouch and van Genabith (1997), Richter and Sailer (1997), Lappin (1999), and Pollard (1999) \\\\n\\\\nextends and generalizes this approach. In each case, the meaning of a sentence is a set of possible \\\\n\\\\ninterpretations I such that each element of I is obtained by (i) ordering the scope relations of \\\\n\\\\nscope taking elements (quantifiers, modals, adverbs, certain connectives, etc.), and/or (ii) \\\\n\\\\nassigning values to parameters in the semantic representations of constituents of the sentence. \\\\n\\\\nThe elements of I are the resolved interpretations of the sentence that are selected in conjunction \\\\n\\\\nwith information supplied by the discourse context. For each syntactically complex constituent C \\\\n\\\\nof a sentence, the constraints in the semantic theory determine the elements of C\\u2019s semantic \\\\n\\\\nvalue sv(C) by defining the relation R which maps the semantic values of C\\u2019s immediate \\\\n\\\\nconstituents into the possible interpretations of C.  \\\\n\\\\n \\\\n\\\\n5. CONCLUSION  \\\\n\\\\n \\\\n\\\\n    We have argued that the objections raised by both K&P and Westerst\\u00e5hl to Zadrozny\\u2019s \\\\n\\\\ntheorem concerning compositionality do not hold. Specifically, the main criticism to the effect \\\\n\\\\nthat the encoding function \\u00b5 on which the theorem depends does not preserve the synonymy \\\\n\\\\nrelations in L specified by the original meaning function m implies an additional constraint on \\\\n\\\\n\\\\n\\\\ncompositionality that is not part of the original  homomorphism requirement. We have also \\\\n\\\\nshown that the construction of  the compositional function used in Zadrozny(1994) can preserve \\\\n\\\\nsynonymy relations if that is desired.  \\\\n\\\\n Recent work in underspecified semantics has shown that compositionality is not a necessary \\\\n\\\\ncondition for a systematic semantic theory. This work indicates that it is possible to develop a \\\\n\\\\nrelational theory of meaning which does not satisfy the homomorphism constraint but does \\\\n\\\\nassign semantic values to phrases that are systematically computed from the meanings of their \\\\n\\\\nsyntactic components. Given this result, there is no reason to take compositionality to be a \\\\n\\\\ncondition of adequacy on a semantic theory. Therefore, the fact that any meaning function for a \\\\n\\\\nlanguage L can be encoded by a mapping from the syntax to the semantics of L that is a \\\\n\\\\nhomomorphism is not a particularly dramatic result. Even if one adds constraints to the definition \\\\n\\\\nof compositionality to exclude functional encodings of the sort employed in the proof of \\\\n\\\\nZadrozny\\u2019s theorem, such a strengthened condition is of limited interest, given that we can \\\\n\\\\ndispense with the homomorphism condition on semantic theory.  \\\\n\\\\n \\\\n\\\\nREFERENCES \\\\n\\\\n \\\\n\\\\nAczel, P.(1988), Non-well-founded-sets, Center for the Study of Language and Information, \\\\n\\\\nStanford, CA. \\\\n\\\\nCooper, R. (1983), Quantification and Syntactic Theory, Reidel, Dordrecht. \\\\n\\\\nCopestake, A., D. Flickinger, R. Malouf, S. Riehman, and I.A. Sag (1995), \\u201cTranslation Using \\\\n\\\\n    Minimal Recursion Semantics\\u201d in Proceedings of the Sixth International Conference on \\\\n\\\\n    Theoretical Issues in Machine Translation, Leuven, Belgium. \\\\n\\\\n\\\\n\\\\nCrouch, R. and J. van Genabith (1997), Context Change, Underspecification and the Structure \\\\n\\\\n    of Glue Language Derivations, ms., University of Nottingham and Dublin City University. \\\\n\\\\nDavidson, D. (1984), \\u201cMeaning and Learnable Languages\\u201d in D. Davidson, Inquiries into \\\\n\\\\n    Truth and Interpretation, Oxford University Press, Oxford, pp. 3-15.  \\\\n\\\\nFulop, S. and E. Keenan (forthcoming), \\u201cCompositionality: A Global Perpsective\\u201d, Sonderheft \\\\n\\\\n    Semantik (a special issue of Linguistische Bericht). \\\\n\\\\nJanssen, T. (1997), \\u201cCompositionality\\u201d in J. van Benthem and A. ter Meulen (eds.), Handbook  \\\\n\\\\n    of Logic and Language, Elsevier, Amsterdam, pp. 417-473. \\\\n\\\\n\\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1433. Scores=6.085874Org Highlight count=12.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0\\\\nalso all have length 1; but we will not count semicolons\\\\n\\\\n3\\\\n\\\\n\\\\n\\\\nwhich we will occasionally use as a typographical device standing\\\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\\\n\\\\nWe define a (finite state) grammar rule as a sequence of classes.\\\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\\\nWe will go beyond finite state grammars when we discuss compo-\\\\nsitional semantics, and we introduce an extension of this notation\\\\nthen.\\\\n\\\\nThe reader should always remember that, mathematically, a\\\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\\\nfunction does not have to be given by a formula. A formula is not\\\\na function, although it might define one: e.g. a description of one\\\\nentity, like energy, depending on another, e.g. velocity, is typically\\\\ngiven as a formula, which defines a function (a set of pairs).\\\\n\\\\nA meaning function is a (possibly partial) function that maps\\\\nsentences (and their parts) into (a representation of) their mean-\\\\nings; typically, some set-theoretic objects like lists of features or\\\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\\\nin its domain:\\\\n\\\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\\\n\\\\nWe are restricting our interest to two argument functions: . de-\\\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\\\narguments. However, the same concept can be defined if expres-\\\\nsions are put together by other, not necessarily binary, operations.\\\\nIn literature, \\u2295 is often taken as a composition of functions; but\\\\nin this chapter it will mostly be used as an operator for construct-\\\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\\\nThis has the advantage of being both conceptually simpler (no\\\\nneed for type raising), and closer to the practice of computational\\\\nlinguistics.\\\\n\\\\n4\\\\n\\\\n\\\\n\\\\n2.1.2 Minimum description length\\\\n\\\\nThe minimum description length (MDL) principle was proposed\\\\nby Rissanen [10]. It states that the best theory to explain a set of\\\\ndata is the one which minimizes the sum of\\\\n\\\\n\\u2022 the length, in bits, of the description of the theory, and\\\\n\\\\n\\u2022 the length, in bits, of data when encoded with the help of\\\\nthe theory.\\\\n\\\\nIn our case, the data is the language we want to describe,\\\\nand the the encoding theory is its grammar (which includes the\\\\nlexicon). The MDL principle justifies the intuition that a more\\\\ncompact grammatical description is better. At issue is what is\\\\nthe best encoding. To address it, we will be simply comparing\\\\nclasses of encodings. The formal side of the argument will be kept\\\\nto the minimum; and the mathematics will be simple \\u2014 counting\\\\nsymbols1. Counting symbols instead of bits does not change the\\\\nline of MDL arguments, given an alternative formulation of the\\\\nMDL principle: (p.310 of [7]):\\\\n\\\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\\\nesis H such that the length of the shortest encoding of D [i.e.\\\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\\\nent applications, the hypothesis H can be about different things.\\\\nFor example, decision trees, finite automata, Boolean formulas, or\\\\npolynomials.\\u201d\\\\n\\\\nThe important aspect of the MDL method has to do with\\\\nthe fact that this complexity measure is invariant with respect\\\\nto the representation language (because of the invariance of the\\\\nKolmogorov complexity on which it is based). The existence of\\\\nsuch invariant complexity measures is not obvious; for example,\\\\n\\\\n1 We assume that the corpus contains no errors (noise), so we do not have\\\\nto worry about defining prior distributions.\\\\n\\\\n5\\\\n\\\\n\\\\n\\\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\\\nture is depends critically upon the way in which we describe it.\\\\nMost of the complex structures found in the world are enormously\\\\nredundant, and we can use this redundancy to simplify their de-\\\\nscription. But to use it, to achieve this simplification, we must\\\\nfind the right representation\\u201d.\\\\n\\\\n2.2 Encoding a corpus of sentences\\\\n\\\\nAssume that we are given a text in an unknown language (con-\\\\ntaining lower and uppercase letters and numbers):\\\\n\\\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\\\n\\\\n(We use the pluses to separate utterances, so there is no order\\\\nimplied.) We are interested in building a grammar describing the\\\\ntext. For a short text, the simplest grammar might in fact be the\\\\ngrammar consisting of the list of all valid sentences:\\\\n\\\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\\\n\\\\nThis grammar has only 25 symbols. However, if a new corpus is\\\\npresented\\\\n\\\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\\\n\\\\nThe listing grammar would have 49 symbols, and a shorter gram-\\\\nmar, with only 39 symbols, could be found:\\\\n\\\\n{X|Y |Z|W}{a|b}{0} (17)\\\\n{Y }{c}{1} (9)\\\\n{X|Z|W}{c}{0} (13)\\\\n\\\\n2.3 How to encode semantics?\\\\n\\\\nWe will now examine a similar example that includes some simple\\\\nsemantics.\\\\n\\\\n6\\\\n\\\\n\\\\n\\\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\\\nmeanings. \\\\nen\\\\n0001002v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1468. Scores=4.9356823Org Highlight count=10.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}], \\\"intent\\\": \\\"[\\\\\\\"meaning function definition\\\\\\\", \\\\\\\"what is a meaning function\\\\\\\", \\\\\\\"explanation of meaning function\\\\\\\"]\\\"}\",\n",
      "              \"end_turn\": false\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 10702,\n",
      "    \"completion_tokens\": 182,\n",
      "    \"total_tokens\": 10884\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = 3\n",
    "answers = 2\n",
    "\n",
    "openaicall_headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_OPENAI_API_KEY']}\n",
    "openaicall_params  = {'api-version': '2023-12-01-preview'}\n",
    "\n",
    "search_payload = {\n",
    "    \"count\": \"true\",\n",
    "    \"select\": \"id, name, title, location, chunk\",\n",
    "    \"top\": results,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": QUESTION\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1.0,    \n",
    "    \"max_tokens\": 1000,\n",
    "    \"dataSources\": [\n",
    "        {            \n",
    "            \"type\": \"AzureCognitiveSearch\",\n",
    "            \"parameters\": {                \n",
    "                \"endpoint\": os.environ['AZURE_SEARCH_ENDPOINT'],\n",
    "                \"key\": os.environ['AZURE_SEARCH_KEY'],\n",
    "                \"indexName\": index_name\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# VERSION 1106 IS NEEDED FOR COMPLETIONS. For more information: https://learn.microsoft.com/en-us/azure/ai-services/openai/overview\n",
    "r = requests.post(\n",
    "    f\"{os.environ['AZURE_OPENAI_ENDPOINT']}openai/deployments/{os.environ['GPT4-1106-128k']}/extensions/chat/completions\",\n",
    "    data=json.dumps(search_payload), headers=openaicall_headers, params=openaicall_params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivbd_20231215",
   "language": "python",
   "name": "openaivbd_20231215"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
