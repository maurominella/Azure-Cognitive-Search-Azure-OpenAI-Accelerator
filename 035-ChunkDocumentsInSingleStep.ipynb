{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58562652",
   "metadata": {},
   "source": [
    "# From files to separated chunk documents... in a single step\n",
    "\n",
    "In Notebook #1 we learnt how to create an index in Azure AI Search and leverage its \"pull\" ability to extract data from files located in storage account. As we saw, this \"extract and fill\" tecnique can be integrated by a \"skillset\" that \"enriches\" extracted data before writing them into the index. Such process is orchestrated by a so called \"indexer\" that maps each source or enriched piece of information against the proper field of the target index.<br/>\n",
    "If we need to split a long text field -typically, the *content* field- in chunks no longer than a predefined length -e.g. 5,000 characters-, we can leverage the **SplitSkill** skill of the skillset, which generates a multi-value field -*Collection(Edm.String)* type- containing the different chunks.<br/>\n",
    "This means, on the other hand, that if we need to retrieve the best ***chunks*** rather than the ***whole content*** of documents due to OpenAI token limits, or just because the whole document contains too much and potentially more confusing information, then we need to create a **secondary index** where we store **one chunk per index row**, possibly associating  specific embeddings, key phrases, emails/urls/persons and other entities to each search document (=chunk).<br/>\n",
    "That's exactly what we did in our Notebook #3; however, in this case we had to face with a few main challenges:\n",
    "- Filling the second index requires a set of manual steps, which we easily wrote in Python, but which creates a potential bottleneck when our scenario needs maintenance and scalability. For example, we had to manually vectorize each chunk and manually *push* the new search document into the secondary index\n",
    "- At the end we get two indexes to manage, where the first one becomes potentially useless\n",
    "- We need to define rules to keep both indexes in sync, and identify update policies. For example, in Notebook #3 we decided to *project and vectorize* only the chunks extracted by a first query, leaving the remaning chunks in their arrays within the first index.\n",
    "\n",
    "These were design decision led by some practical considerations and the general skill-up objective of this solution accelerator; however, they were also forced by some limits associated to Azure AI Search, *until a few days ago*.<br/>\n",
    "Yes, because with the release of the [latest stable version 2023-11-01 of Azure AI Search REST API](https://learn.microsoft.com/en-us/rest/api/searchservice/search-service-api-versions#stable-versions), we can now operate all the above actions with the following benefits:\n",
    "- **pull mathod**, e.g. no manual steps to upload search document to target index\n",
    "- **automatic embedding creation**, that can be performed by a specific skill\n",
    "- **automatic projection** of the array values into separated search documents of the target index\n",
    "\n",
    "As a matter of fact, we actually do not need two indexes any more. **Let's see how to do it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25369fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and assign variables\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials_my.env\")\n",
    "\n",
    "# Name of the container in your Blob Storage Datasource (included in credentials.env)\n",
    "BLOB_CONTAINER_NAME = \"arxivcs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22ae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for the data source, skillset, index and indexer\n",
    "datasource_name = \"cogsrch-datasource-files-onestep\"\n",
    "skillset_name   = \"cogsrch-skillset-files-onestep\"\n",
    "index_name      = \"cogsrch-index-files-vector-onestep\"\n",
    "indexer_name    = \"cogsrch-indexer-files-onestep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c260e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers     = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params      = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}\n",
    "params_prj  = {'api-version': os.environ['AZURE_SEARCH_API_VERSION_OLD']} # needed for skillset creation with projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1bb04",
   "metadata": {},
   "source": [
    "## Create Data Source (Blob container with the Arxiv CS pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc983da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# The following code sends the json paylod to Azure Search engine to create the Datasource\n",
    "\n",
    "datasource_payload = {\n",
    "    \"name\": datasource_name,\n",
    "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
    "    \"type\": \"azureblob\",\n",
    "    \"credentials\": {\n",
    "        \"connectionString\": os.environ['BLOB_CONNECTION_STRING']\n",
    "    },\n",
    "    \"dataDeletionDetectionPolicy\" : {\n",
    "        \"@odata.type\" :\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\" # this makes sure that if the item is deleted from the source, it will be deleted from the index\n",
    "    },\n",
    "    \"container\": {\n",
    "        \"name\": BLOB_CONTAINER_NAME\n",
    "    }\n",
    "}\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/datasources/\" + datasource_name,\n",
    "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6d6df",
   "metadata": {},
   "source": [
    "## Create the *SINGLE* Index\n",
    "Since a [vectorizer](https://learn.microsoft.com/en-us/rest/api/searchservice/indexes/create?view=rest-searchservice-2023-10-01-preview&tabs=HTTP#vectorsearch) is used here, we need to use API 2023-10-01-Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f188d775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the single index\n",
    "index_payload = {\n",
    "    \"name\": index_name,\n",
    "    \"fields\": [        \n",
    "        {\n",
    "          \"name\": \"id\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"key\": \"true\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\",\n",
    "          \"analyzer\": \"keyword\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"ParentKey\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"facetable\": \"false\",\n",
    "          \"filterable\": \"true\",\n",
    "          \"sortable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"title\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"chunk\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"name\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"location\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"false\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"language\",\n",
    "          \"type\": \"Edm.String\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"true\",\n",
    "          \"filterable\": \"true\",\n",
    "          \"facetable\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"persons\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"urls\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"emails\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"key_phrases\",\n",
    "          \"type\": \"Collection(Edm.String)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"sortable\": \"false\",\n",
    "          \"filterable\": \"false\",\n",
    "          \"facetable\": \"false\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"chunkVector\",\n",
    "          \"type\": \"Collection(Edm.Single)\",\n",
    "          \"searchable\": \"true\",\n",
    "          \"retrievable\": \"true\",\n",
    "          \"dimensions\": 1536,\n",
    "          \"vectorSearchProfile\": \"my_vectorSearch_profile\"\n",
    "        }\n",
    "    ],    \n",
    "  \n",
    "    \"vectorSearch\": {        \n",
    "        \"profiles\": [\n",
    "            {                \n",
    "                \"name\": \"my_vectorSearch_profile\",\n",
    "                \"algorithm\": \"my-vectorSearch-algorithm\",\n",
    "                \"vectorizer\": \"my-embeddings-vectorizer\"\n",
    "            }\n",
    "        ],        \n",
    "        \"algorithms\": [            \n",
    "            {\n",
    "                \"name\": \"my-vectorSearch-algorithm\",\n",
    "                \"kind\": \"hnsw\",\n",
    "                \"hnswParameters\": {                    \n",
    "                    \"m\": 4,\n",
    "                    \"metric\": \"cosine\",\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500\n",
    "                }          \n",
    "            }\n",
    "        ],        \n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"my-embeddings-vectorizer\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\": {\n",
    "                \"resourceUri\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "                \"apiKey\": os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                \"deploymentId\": os.environ['EMBEDDING_DEPLOYMENT']\n",
    "                }\n",
    "            }\n",
    "        ]        \n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params_prj)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fef1b",
   "metadata": {},
   "source": [
    "## Create the Skillset\n",
    "### Please note, in this case we need to use version 2023-10-01-Preview of Azure AI Search REST API which currently offers the projection feature\n",
    "- [Reference guide version 2023-10-01-Preview](https://learn.microsoft.com/en-us/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-10-01-Preview&tabs=HTTP)\n",
    "- [Reference guide latest version (2023-11-01)](https://learn.microsoft.com/en-us/rest/api/searchservice/skillsets/create-or-update?view=rest-searchservice-2023-11-01&tabs=HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e1be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the skillset, including the projection settings into the single index created above\n",
    "skillset_payload = {    \n",
    "  \"name\": skillset_name,\n",
    "  \"description\": \"Detect language, run OCR, merge content+ocr_text, split in chunks, extract entities and key-phrases and embed the chunks\",\n",
    "  \"skills\": [\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
    "      \"name\": \"LanguageDetectionSkill\",\n",
    "      \"context\": \"/document\",\n",
    "      \"description\": \"If you have multilingual content, adding a language code is useful for filtering\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/content\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"targetName\": \"language\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
    "      \"name\": \"OcrSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/normalized_images/*\",\n",
    "      \"textExtractionAlgorithm\": \"\",\n",
    "      \"lineEnding\": \"Space\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"detectOrientation\": \"true\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"image\",\n",
    "          \"source\": \"/document/normalized_images/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"targetName\": \"text_from_ocr\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
    "      \"name\": \"MergeSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document\",\n",
    "      \"insertPreTag\": \" \",\n",
    "      \"insertPostTag\": \" \",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/content\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"itemsToInsert\",\n",
    "          \"source\": \"/document/normalized_images/*/text_from_ocr\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"offsets\",\n",
    "          \"source\": \"/document/normalized_images/*/contentOffset\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"mergedText\",\n",
    "          \"targetName\": \"merged_text\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
    "      \"name\": \"SplitSkill\",\n",
    "      \"context\": \"/document\",\n",
    "      \"textSplitMode\": \"pages\",\n",
    "      \"maximumPageLength\": 5000,\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/merged_text\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"source\": \"/document/language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"textItems\",\n",
    "          \"targetName\": \"chunks\"\n",
    "        }        \n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.LanguageDetectionSkill\",\n",
    "      \"name\": \"LanguageDetectionSkill_by_chunk\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"description\": \"If you have multilingual content, adding a language code is useful for filtering\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"targetName\": \"chunk_language\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.V3.EntityRecognitionSkill\",\n",
    "      \"name\": \"EntityRecognitionSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"minimumPrecision\": 0.5,\n",
    "      \"modelVersion\": \"\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"languageCode\", \n",
    "            \"source\": \"/document/chunks/*/chunk_language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"persons\",\n",
    "          \"targetName\": \"persons\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"urls\",\n",
    "          \"targetName\": \"urls\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"emails\",\n",
    "          \"targetName\": \"emails\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.KeyPhraseExtractionSkill\",\n",
    "      \"defaultLanguageCode\": \"en\",\n",
    "      \"modelVersion\": \"\",\n",
    "      \"name\": \"KeyPhraseExtractionSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"languageCode\",\n",
    "          \"source\": \"/document/chunks/*/chunk_language\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"keyPhrases\",\n",
    "          \"targetName\": \"key_phrases\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
    "      \"name\": \"AzureOpenAIEmbeddingSkill\",\n",
    "      \"description\": \"\",\n",
    "      \"context\": \"/document/chunks/*\",\n",
    "      \"resourceUri\": \"https://mmopenai04.openai.azure.com\",\n",
    "      \"apiKey\": \"23b1db9a7f3a4eeb8a4f1c74bdd7d13d\",\n",
    "      \"deploymentId\": \"text-embedding-ada-002\",\n",
    "      \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"text\",\n",
    "          \"source\": \"/document/chunks/*\"\n",
    "        }\n",
    "      ],\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"name\": \"embedding\",\n",
    "          \"targetName\": \"chunk_embedded\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"cognitiveServices\": {\n",
    "    \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
    "    \"description\": os.environ['COG_SERVICES_NAME'],\n",
    "    \"key\": os.environ['COG_SERVICES_KEY']\n",
    "  },\n",
    "  \"indexProjections\": {\n",
    "    \"selectors\": [\n",
    "      {\n",
    "        \"targetIndexName\": index_name,\n",
    "        \"parentKeyFieldName\": \"ParentKey\",\n",
    "        \"sourceContext\": \"/document/chunks/*\",\n",
    "        \"mappings\": [\n",
    "          {\n",
    "            \"name\": \"title\",\n",
    "            \"source\": \"/document/title\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"name\",\n",
    "            \"source\": \"/document/name\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"location\",\n",
    "            \"source\": \"/document/location\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"chunk\",\n",
    "            \"source\": \"/document/chunks/*\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"source\": \"/document/chunks/*/chunk_embedded\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"language\",\n",
    "            \"source\": \"/document/chunks/*/chunk_language\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"persons\",\n",
    "            \"source\": \"/document/chunks/*/persons\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"urls\",\n",
    "            \"source\": \"/document/chunks/*/urls\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"emails\",\n",
    "            \"source\": \"/document/chunks/*/emails\"\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"key_phrases\",\n",
    "            \"source\": \"/document/chunks/*/key_phrases\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/skillsets/\" + skillset_name,\n",
    "                 data=json.dumps(skillset_payload), headers=headers, params=params_prj)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c357a",
   "metadata": {},
   "source": [
    "## Create and Run the Indexer\n",
    "[Reference guide](https://learn.microsoft.com/en-us/rest/api/searchservice/indexers?view=rest-searchservice-2023-11-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a36fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the skillset, including the projection settings into the single index created above\n",
    "indexer_payload = {    \n",
    "  \"name\": indexer_name,\n",
    "  \"dataSourceName\": datasource_name,\n",
    "  \"targetIndexName\": index_name,\n",
    "  \"skillsetName\": skillset_name,  \n",
    "  \"fieldMappings\": [\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_path\",\n",
    "      \"targetFieldName\": \"id\",\n",
    "      \"mappingFunction\": {\n",
    "        \"name\": \"base64Encode\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_title\",\n",
    "      \"targetFieldName\": \"title\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_name\",\n",
    "      \"targetFieldName\": \"name\"\n",
    "    },\n",
    "    {\n",
    "      \"sourceFieldName\": \"metadata_storage_path\",\n",
    "      \"targetFieldName\": \"location\"\n",
    "    }\n",
    "  ],\n",
    "  \"outputFieldMappings\": [],\n",
    "  \"parameters\": {\n",
    "    \"maxFailedItems\": -1,\n",
    "    \"maxFailedItemsPerBatch\": -1,\n",
    "    \"configuration\": {\n",
    "      \"dataToExtract\": \"contentAndMetadata\",\n",
    "      \"parsingMode\": \"default\",\n",
    "      \"imageAction\": \"generateNormalizedImages\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name,\n",
    "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabdd398",
   "metadata": {},
   "source": [
    "### Please wait for a few minutes after running the cell above to allow the Indexer to fill the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9e256",
   "metadata": {},
   "source": [
    "## And now... let's search!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55f265",
   "metadata": {},
   "source": [
    "Let's start with a text search. Recall:\n",
    "- The **value** key contains the ***sequence of results*** returned by the query. The amount of results here is controlled via the **top** parameter.\n",
    "- The **@search.answers** key contain the **semantic query results** for the search operation, whose amount depends on the **answers** parameter. Such semantic query results include three pieces of information:\n",
    "  1. **key**: the search document key\n",
    "  2. **score**: the score associated to that semantic answer\n",
    "  3. **text**: the so called ***captions*** that contain the most representative passages from the document relatively to the search query. They are often used as document summary. Captions are only returned for queries of type semantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd235538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is needed just if we start running this notebook from here\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials_my.env\")\n",
    "\n",
    "index_name  = \"cogsrch-index-files-vector-onestep\"\n",
    "params_prj  = {'api-version': os.environ['AZURE_SEARCH_API_VERSION_OLD']} # needed for skillset creation with projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39c95a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 313,\n",
      "  \"@search.answers\": [\n",
      "    {\n",
      "      \"key\": \"15e667d8b7d3_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"text\": \"A meaning function is a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings; typically, some set-theoretic objects like lists of features or functions.\",\n",
      "      \"highlights\": \"A meaning function is<em> a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings;</em> typically, some set-theoretic objects like lists of features or functions.\",\n",
      "      \"score\": 0.9970703125\n",
      "    },\n",
      "    {\n",
      "      \"key\": \"15e667d8b7d3_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_4\",\n",
      "      \"text\": \"A meaning function \\u00b5 is a compositional semantics for the set S if its domain is contained in S, and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "      \"highlights\": \"A meaning function \\u00b5 is<em> a compositional semantics for the set S if its domain is contained in S,</em> and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "      \"score\": 0.8466796875\n",
      "    }\n",
      "  ],\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 7.8717155,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"A meaning function is a (possibly partial) function that maps sentences (and their parts) into (a representation of) their mean- ings; typically, some set-theoretic objects like lists of features or functions.\",\n",
      "          \"highlights\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 3.868259906768799,\n",
      "      \"id\": \"15e667d8b7d3_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 9.094685,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"A meaning function \\u00b5 is a compositional semantics for the set S if its domain is contained in S, and a. it satisfies the postulate of compositionality: for all s, t in its domain:  \\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)  b. it is the shortest, in the sense of the Minimum Description Length principle, such an encoding. c.\",\n",
      "          \"highlights\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 3.6001837253570557,\n",
      "      \"id\": \"15e667d8b7d3_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_4\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"grammar \\u2014 its total length is less than 300 symbols (vs. 18,000);\\nbut it assumes an existence of a language generator. Interestingly,\\nthe grammar resembles the compositional semantics, as usually\\ngiven. The rule with V (1) and N(1) describes the compositional\\npart of the corpus; the rule with v0 and n0 \\u2013 the idiomatic; other\\nrules are in between.\\n\\n3.4 Variations on the MDL algorithm\\n\\nA similar result is obtained when we do not insist that all instances\\nof merging classes are replaced by the result of the merge. Starting\\nwith the grammar\\n\\nGrammar GV (1):\\n\\nV (1) = {v1| ... |v9} (21)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{V (1)}{n1}{action}{V (1)}{object}{n1} (18)\\n...\\n\\n{V (1)}{n99}{action}{V (1)}{object}{n99} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n{v0}{n1}{action}{v0}{object}{n1} (18)\\n... (18)\\n{v0}{n99}{action}{v0}{object}{n99} (18)\\n\\n14\\n\\n\\n\\nWe can see that the merge V (0) = {v0|V (1)} will decrease the size\\nof the grammar by 99 rules and result in:\\n\\nGrammar GV (0):\\n\\nV (1) = {v1| ... |v9} (21)\\nV (0) = {v0|V (1)} (7)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{V (0)}{n1}{action}{V (0)}{object}{n1} (18)\\n...\\n\\n{V (0)}{n99}{action}{V (0)}{object}{n99} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nThe successive merging of nouns will then produce\\n\\nGrammar GV (0)N(1):\\n\\nV (0) = {v0|V (1)} (7)\\nV (1) = {v1| ... |v9} (21)\\nN(1) = {n1| ... |n99} (201)\\n{V (0)}{N(1)}{action}{V (0)}{object}{N(1)} (18)\\n{V (1)}{n0}{action}{V (1)}{object}{n0} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nIf, however we do not do the V (0) = {v0|V (1)} merge, and\\nproceed with the merging of the nouns (e.g. if there were reasons\\nto modify the algorithm), we get:\\n\\n15\\n\\n\\n\\nGrammar GV (1)N(0):\\n\\nV (1) = {v1| ... |v9} (21)\\nN(1) = {n1| ... |n99} (201)\\nN(0) = {n0|N(1)} (7)\\n{V (1)}{N(0)}{action}{V (1)}{object}{N(0)} (18)\\n{v0}{N(1)}{action}{v0}{object}{N(1)} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nFinally, if we allow some overgeneralization, we can replace the\\nabove grammars with an even shorter grammar:\\n\\nGrammar GV (0)N(0):\\n\\nV = {v0| ... |v9} (23)\\nN = {n0| ... |n99} (203)\\n{V }{N}{action}{V }{object}{N} (18)\\n{v0}{n0}{action}{v0}{object}{nil} (18)\\n\\nHere, clearly v0 is the idiomatic element. However, both idiomatic\\nand non-idiomatic reading of kick bucket is allowed. (In the pre-\\nviously defined grammars, we can also see the distinction between\\nthe idiomatic and non-idiomatic elements).\\n\\n4 A non-vacuous definition of composition-\\n\\nality\\n\\nThe fact that that the MDL principle can produce an object re-\\nsembling a compositional semantics is crucial. It allows us to argue\\nfor a non-vacuous definition of compositionality.\\n\\nAssume that we have a corpus S of sentences and their parts,\\ngiven either as a set or generated by a grammar. Let sentences\\n\\n16\\n\\n\\n\\nand their parts be collections of symbols put together by some\\noperations; in the simplest and most important case, by concate-\\nnation \\u201d.\\u201d.\\n\\nDefinition. A meaning function \\u00b5 is a compositional semantics\\nfor the set S if its domain is contained in S, and\\na. it satisfies the postulate of compositionality: for all s, t in its\\ndomain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nb. it is the shortest, in the sense of the Minimum Description\\nLength principle, such an encoding.\\nc. it is maximal, i.e. there is no \\u00b5\\u2032 with a larger domain that\\nsatisfies a and b.\\n\\nTo see better what this definition entails, let us consider our\\nsemantic corpus again. The set S consists of the 10 verbs and\\n100 nouns and all noun-verb combinations. The compositional\\nfunction \\u00b5 assigns to each word its category e.g. [n17, noun].\\nThe question is how to define the operator \\u2295. Because of the\\nidiom, it cannot be a total function; hence we have to exclude\\nfrom the domain of \\u2295 the pair [[v0, verb], [n0, noun]]. The short-\\nest description of \\u2295 can be given by translating the grammar of\\nSection 3.2. First, map non-idiomatic verbs and nouns into pairs\\n\\u00b5(vi) = [vi, verbnonid], \\u00b5(nj) = [nj , nounnonid], i, j > 0. Then,\\nput\\n\\n\\u2295([[v, verbnonid], [n, nounnonid]]) = [action.v, object.n]\\n\\nThus defined \\u00b5 and \\u2295 correspond to the grammar obtained by the\\nalgorithm of Section 3.2 and to the tables of Section 2. This cor-\\nrespondence is not exact, because functions \\u00b5 and \\u2295 encode only\\nthe systematic, compositional part of the corpus. (But please note\\nthis clear distinction between the idiomatic and the compositional\\nparts of the lexicon and the corpus).\\n\\n17\\n\\n\\n\\nHowever this description of the two functions is not maximal.\\nWe obtain the maximal compositional semantics for S by extend-\\ning the above defined mapping to all nouns \\u00b5(nj) = [nj, noun],\\nj \\u2265 0, and extending the domain of \\u2295\\n\\n\\u2295([[v, verbnonid], [n, noun]]) = [action.v, object.n]\\n\\nIt is easily checked that this is the shortest (in the sense of the\\nMDL) and maximal assignment of meaning to the elements of set\\nS.4 Please compare this mapping with GV (1)N(0), and also note\\nthat now we have a formal basis for saying that (for this corpus)\\nit is the verb kick, and not the noun bucket, that is idiomatic.\\n\\n\",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 10.975723,\n",
      "      \"@search.captions\": [\n",
      "        {\n",
      "          \"text\": \"Since this chapter deals with sizes of objects, we compute them for the meaning functions: the size of the first function is 90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79.\",\n",
      "          \"highlights\": \"Since this chapter deals with sizes of objects, we compute them for the<em> meaning functions:</em> the size of the first function is 90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79.\"\n",
      "        }\n",
      "      ],\n",
      "      \"@search.rerankerScore\": 2.895526885986328,\n",
      "      \"id\": \"15e667d8b7d3_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_2\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"If our corpus were to be the 10 \\u00d7 100 table consisting\\nof all verb-noun combinations:\\n\\nv0n0 + v1n0 + ... + vjni + ...\\n\\nwe could quickly use the previous example to write a simple finite\\nstate grammar that describes the corpus:\\n\\n{v0|v1|...}{n0|n1...} (21 + 201)\\n\\nBut in this subsection we are supposed to introduce some seman-\\ntics. Thus, let our corpus consist of all those 1,000 sentences to-\\ngether with their meanings, which, to keep things as simple as\\npossible, will be simplified to two attributes. Also, for the reason\\nof simplicity, we assume that only \\u201dkick bucket\\u201d has an idiomatic\\nmeaning, and all other entries are assigned the meaning consisting\\nof the two attribute expression [[action, vj ], [object, ni]]. Hence,\\nour corpus will look as follows:\\n\\nkick bucket action die object nil\\n\\nv1 bucket action v1 object bucket\\n\\n...\\n\\nvj ni action vj object ni\\n\\n...\\n\\nNow, notice that this corpus cannot be encoded by means of a\\nshort finite state grammar, because of the dependence of the mean-\\nings (i.e. the pair [action, ..., object, ...]) on the first two elements\\nof each sentence. We will have to extend our grammar formalism\\nto address this dependence (Section 3).\\n\\n7\\n\\n\\n\\n2.4 On meaning functions\\n\\nEven though we cannot encode the corpus by a short, finite state\\ngrammar, we can easily provide for it a compositional semantics.\\nTo avoid the complications of type raising, we will build a homo-\\nmorphic mapping from syntax to semantics. To do it, it is enough\\nto build meaning functions in a manner ensuring that the meaning\\nof each vjni is composed from the meaning of vj and the mean-\\ning of ni. Since our corpus is simple, these meaning functions are\\nsimple, too: For the verbs the meaning function is given by the\\ntable:\\n\\n[v0, [verb, v0]]; [v1, [verb, v1]] ... [v9, [verb, v9]] (90)\\n\\nFor the nouns:\\n\\n[n0, [noun, n0]]; [n1, [noun, n1]] ... [n99, [noun, n99]] (900)\\n\\nWe have represented both meaning functions as tables of sym-\\nbols. Since this chapter deals with sizes of objects, we compute\\nthem for the meaning functions: the size of the first function is\\n90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79. Therefore,\\nthe meaning function for the whole corpus could be represented\\nas a table with 1,000 entries:\\n\\n[[[verb, v9], [noun, n99]], [[action, v9], [object, n99]]]\\n. . .\\n[[[verb, vj ], [noun, ni]], [[action, vj ], [object, ni]]]\\n. . .\\n[[[verb, v1], [noun, bucket]], [[action, v1 ], [object, bucket]]]\\n[[[verb, kick], [noun, bucket]], [[action, die], [object, nil]]\\n\\nand the size of this table is 29\\u00d71000. Finally, the total size of the\\ntables that describe the compositional interpretation of the corpus\\nis 29000 + 900 + 90, i.e. roughly 30, 000. Notice that if we had\\nmore verbs and nouns, the tables describing the meaning functions\\n\\n8\\n\\n\\n\\nwould be even larger.2 Also, note that we have not counted the\\ncost of encoding the positions of elements of the table, which would\\nbe the log of the total number of symbols in the table. This\\nsimplifying assumption does not change anything in the strength\\nof our arguments (as larger tables have longer encodings).\\n\\n3 Compositional semantics through the Min-\\n\\nimum Description Length principle\\n\\nIn this section we first extend our notation to deal with semantic\\ngrammars. Then we apply the minimum description length princi-\\nple to construct a compact representation of our example corpus.\\nThis experience will motivate our new, non-vacuous definition of\\nthe notion of compositional semantics given in Section 4.\\n\\n3.1 Representations\\n\\nWe have seen that it is impossible to efficiently encode our se-\\nmantic corpus using a finite state grammar. Therefore, we have\\nto make our representation of grammars more expressive (at the\\nprice of a slightly bigger interpreter). Namely, we will allow a sim-\\nple form of unification.\\n\\n2 The reader familiar with [12] should notice that the meaning functions\\nobtained by the solution lemma also consist of tables of element-value pairs. It\\nis easy to see that for the corpus we are encoding the solution lemma produces\\nthe same meaning functions.\\n\\nIn the other direction, the method for deriving compositional semantics us-\\ning the minimum description length principle (Sections 3 and 4) are directly\\napplicable to meaning functions obtained by the solution lemma in [12], pro-\\nvided they are finite (which covers the practically interesting cases); and it\\nseems applicable to the infinite case, if it has a finite representation. However,\\nwe will not pursue this connection any further.\\n\\n9\\n\\n\\n\\nExample. Assume we do not want {a|b}{a|b|d} to generate ab.\\nWe can do it by changing the notation:\\n\\nX = {a|b}\\n{X}{X|d}\\n\\nThe intention is simple: first, we define a class variable (X) for the\\nclass consisting of elements a and b; then, we generate all strings\\nusing the rule with variable X: XX and Xd; and finally we substi-\\ntute for X all its possible values, which produces aa, ad, bb and bd.\\n\\nMore generally, let us assume that we have an alphabet a1, a2, ...,\\n\",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"language\": \"en\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Three full text answers plus two semantic answers\n",
    "results_total = 3\n",
    "semantic_answers = 2\n",
    "QUESTION = \"What is a meaning function?\"\n",
    "\n",
    "import requests, json\n",
    "\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params  = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']} # NEW VERSION\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"count\": \"true\",\n",
    "  \"select\": \"id, name, title, chunk, language\",\n",
    "  \"top\": results_total,\n",
    "  \"search\": QUESTION,\n",
    "  \"queryType\": \"semantic\",\n",
    "  \"semanticConfiguration\": \"my-semantic-config\",\n",
    "  \"captions\": \"extractive\",\n",
    "  \"answers\": f\"extractive|count-{semantic_answers}\"\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25613c4",
   "metadata": {},
   "source": [
    "### Now let's do a *vector* query. \n",
    "We can use two possible ***vectorQueries*** settings in the search payload:\n",
    "1. [**\"kind\": \"vector\"**](https://learn.microsoft.com/en-us/rest/api/searchservice/documents/search-post?view=rest-searchservice-2023-11-01&tabs=HTTP#rawvectorquery), when a raw vector value is provided, such as an Azure OpenAI Embedding. In this case, of course, we need to **manually** convert and pass the query embedding.\n",
    "2. [**\"kind\": \"text\"**](https://learn.microsoft.com/en-us/rest/api/searchservice/documents/search-post?view=rest-searchservice-2023-10-01-Preview&tabs=HTTP#rawvectorquery), which accepts a text that is **automatically** converted into an embedding thanks to the **vectorized** parameter associated to the \"chunkVector\" field in the indexer. **Currently, only REST API 2023-10-01-Preview supports this feature**.<br/>\n",
    "\n",
    "In both cases, please consider the following settings:\n",
    "- **\"top\"**: The number of search results *per page* to retrieve. This can be used in conjunction with **skip** to implement client-side paging of search results.\n",
    "- **\"skip\"**: The number of search results to skip.\n",
    "- **\"k\"**: the number of total results to retrieve through the vector query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7e9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 2,\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 0.84926814,\n",
      "      \"id\": \"3f663ffc93ae_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0_chunks_3\",\n",
      "      \"title\": \"Microsoft Word - LP99-sub.htm\",\n",
      "      \"chunk\": \"requirement that the meaning of an expression be functionally determined by its structure and the \\n\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\n\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\n\\n \\n\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\n\\nMEANING \\n\\n \\n\\n    The importance which has been assigned to compositionality in semantic theory can be \\n\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\n\\n                                                                                                                                                                                           \\nsense of inter-substitutivity.  \\n\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\nconstraints upon the set possible models into the definition of a compositional meaning function. \\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\n\\n\\n\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\n\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\n\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\n\\n \\n\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\n\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\n\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\n\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\n\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\n\\n \\n\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\n\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\n\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\n\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\n\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\n\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\n\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\n\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\n\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\n\\nsyntactic derivation tree.  \\n\\n \\n\\n\\n\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\n\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\n\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\n\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\n\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\n\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\n\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\n\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\n\\n \\n\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\n\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\n\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\n\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\n\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\n\\nsingle disambiguated interpretation. \",\n",
      "      \"name\": \"0001006v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001006v1.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 0.8394798,\n",
      "      \"id\": \"15e667d8b7d3_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001002v1.pdf\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# \"kind\": \"vector\"\n",
    "\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "embedder = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\", skip_empty=True)\n",
    "VECTORIZED_QUESTION = embedder.embed_query(QUESTION)\n",
    "results_total = 2\n",
    "results_per_page = results_total # we return ALL results in a single call\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "  \"count\": \"true\",\n",
    "  \"select\": \"id, name, title, location, chunk\",\n",
    "  \"top\": results_per_page, \"skip\": 0, # both can be omitted if results_per_page == results_total\n",
    "  \"vectorQueries\": [\n",
    "    {\n",
    "      \"kind\": \"vector\",\n",
    "      \"k\": results_total,\n",
    "      \"fields\": \"chunkVector\",\n",
    "      \"vector\": VECTORIZED_QUESTION\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f0c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"@odata.context\": \"https://cog-search-y24bi577jszf6.search.windows.net/indexes('cogsrch-index-files-vector-onestep')/$metadata#docs(*)\",\n",
      "  \"@odata.count\": 2,\n",
      "  \"value\": [\n",
      "    {\n",
      "      \"@search.score\": 0.84926814,\n",
      "      \"id\": \"3f663ffc93ae_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0_chunks_3\",\n",
      "      \"title\": \"Microsoft Word - LP99-sub.htm\",\n",
      "      \"chunk\": \"requirement that the meaning of an expression be functionally determined by its structure and the \\n\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\n\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\n\\n \\n\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\n\\nMEANING \\n\\n \\n\\n    The importance which has been assigned to compositionality in semantic theory can be \\n\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\n\\n                                                                                                                                                                                           \\nsense of inter-substitutivity.  \\n\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\nconstraints upon the set possible models into the definition of a compositional meaning function. \\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\n\\n\\n\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\n\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\n\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\n\\n \\n\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\n\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\n\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\n\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\n\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\n\\n \\n\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\n\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\n\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\n\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\n\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\n\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\n\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\n\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\n\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\n\\nsyntactic derivation tree.  \\n\\n \\n\\n\\n\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\n\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\n\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\n\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\n\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\n\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\n\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\n\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\n\\n \\n\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\n\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\n\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\n\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\n\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\n\\nsingle disambiguated interpretation. \",\n",
      "      \"name\": \"0001006v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001006v1.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"@search.score\": 0.8394798,\n",
      "      \"id\": \"15e667d8b7d3_aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0_chunks_1\",\n",
      "      \"title\": \"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\",\n",
      "      \"chunk\": \"also all have length 1; but we will not count semicolons\\n\\n3\\n\\n\\n\\nwhich we will occasionally use as a typographical device standing\\nfor \\u201dend of line\\u201d. In several cases, we will give the length (in\\nparentheses) together with an expression, e.g. {a|b|ac} (8).\\n\\nWe define a (finite state) grammar rule as a sequence of classes.\\nE.g. the rule {a|b}{c|d} describes all the combinations ac, ad, bc, bd.\\nWe will go beyond finite state grammars when we discuss compo-\\nsitional semantics, and we introduce an extension of this notation\\nthen.\\n\\nThe reader should always remember that, mathematically, a\\nfunction is defined as a set of pairs [argument, value]. Thus, a\\nfunction does not have to be given by a formula. A formula is not\\na function, although it might define one: e.g. a description of one\\nentity, like energy, depending on another, e.g. velocity, is typically\\ngiven as a formula, which defines a function (a set of pairs).\\n\\nA meaning function is a (possibly partial) function that maps\\nsentences (and their parts) into (a representation of) their mean-\\nings; typically, some set-theoretic objects like lists of features or\\nfunctions. A meaning function \\u00b5 is compositional if for all elements\\nin its domain:\\n\\n\\u00b5(s.t) = \\u00b5(s) \\u2295 \\u00b5(t)\\n\\nWe are restricting our interest to two argument functions: . de-\\nnotes the concatenation of symbols, and \\u2295 is a function of two\\narguments. However, the same concept can be defined if expres-\\nsions are put together by other, not necessarily binary, operations.\\nIn literature, \\u2295 is often taken as a composition of functions; but\\nin this chapter it will mostly be used as an operator for construct-\\ning a list, where some new attributes are added to \\u00b5(s) and \\u00b5(t).\\nThis has the advantage of being both conceptually simpler (no\\nneed for type raising), and closer to the practice of computational\\nlinguistics.\\n\\n4\\n\\n\\n\\n2.1.2 Minimum description length\\n\\nThe minimum description length (MDL) principle was proposed\\nby Rissanen [10]. It states that the best theory to explain a set of\\ndata is the one which minimizes the sum of\\n\\n\\u2022 the length, in bits, of the description of the theory, and\\n\\n\\u2022 the length, in bits, of data when encoded with the help of\\nthe theory.\\n\\nIn our case, the data is the language we want to describe,\\nand the the encoding theory is its grammar (which includes the\\nlexicon). The MDL principle justifies the intuition that a more\\ncompact grammatical description is better. At issue is what is\\nthe best encoding. To address it, we will be simply comparing\\nclasses of encodings. The formal side of the argument will be kept\\nto the minimum; and the mathematics will be simple \\u2014 counting\\nsymbols1. Counting symbols instead of bits does not change the\\nline of MDL arguments, given an alternative formulation of the\\nMDL principle: (p.310 of [7]):\\n\\n\\u201dGiven a hypothesis space H, we want to select the hypoth-\\nesis H such that the length of the shortest encoding of D [i.e.\\nthe data] together with the hypothesis H is minimal. \\u201dIn differ-\\nent applications, the hypothesis H can be about different things.\\nFor example, decision trees, finite automata, Boolean formulas, or\\npolynomials.\\u201d\\n\\nThe important aspect of the MDL method has to do with\\nthe fact that this complexity measure is invariant with respect\\nto the representation language (because of the invariance of the\\nKolmogorov complexity on which it is based). The existence of\\nsuch invariant complexity measures is not obvious; for example,\\n\\n1 We assume that the corpus contains no errors (noise), so we do not have\\nto worry about defining prior distributions.\\n\\n5\\n\\n\\n\\nH.Simon (in [11], p.228), wrote \\u201dHow complex or simple a struc-\\nture is depends critically upon the way in which we describe it.\\nMost of the complex structures found in the world are enormously\\nredundant, and we can use this redundancy to simplify their de-\\nscription. But to use it, to achieve this simplification, we must\\nfind the right representation\\u201d.\\n\\n2.2 Encoding a corpus of sentences\\n\\nAssume that we are given a text in an unknown language (con-\\ntaining lower and uppercase letters and numbers):\\n\\nXa0 + Y c1 + Xb0 + Xc0 + Y a0 + Y b0\\n\\n(We use the pluses to separate utterances, so there is no order\\nimplied.) We are interested in building a grammar describing the\\ntext. For a short text, the simplest grammar might in fact be the\\ngrammar consisting of the list of all valid sentences:\\n\\n{Xa0|Y c1|Xb0|Xc0|Y a0|Y b0}\\n\\nThis grammar has only 25 symbols. However, if a new corpus is\\npresented\\n\\nZa0 + Wc0 + Zb0 + Zc0 + Wa0 + Wb0\\n\\nThe listing grammar would have 49 symbols, and a shorter gram-\\nmar, with only 39 symbols, could be found:\\n\\n{X|Y |Z|W}{a|b}{0} (17)\\n{Y }{c}{1} (9)\\n{X|Z|W}{c}{0} (13)\\n\\n2.3 How to encode semantics?\\n\\nWe will now examine a similar example that includes some simple\\nsemantics.\\n\\n6\\n\\n\\n\\nConsider a set of nouns ni, i \\u2208 1..99 and a set of verbs vj,\\nj \\u2208 1..9. Let v0 be kick and n0 be bucket; and all other noun-\\nverb combinations are intended to have normal, \\u201dcompositional\\u201d\\nmeanings. \",\n",
      "      \"name\": \"0001002v1.pdf\",\n",
      "      \"location\": \"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001002v1.pdf\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# \"kind\": \"text\" - requires 2023-10-01-Preview version of Azure AI HTTP REST API\n",
    "\n",
    "results_total = 2\n",
    "results_per_page = results_total # we return ALL results in a single call\n",
    "\n",
    "# search query payload\n",
    "search_payload = {\n",
    "    \n",
    "    \"count\": \"true\",\n",
    "    \"select\": \"id, name, title, location, chunk\",\n",
    "    \"top\": results_per_page, \"skip\": 0, # both can be omitted if results_per_page == results_total\n",
    "    \"vectorQueries\": [\n",
    "        {\n",
    "            \"kind\": \"text\",\n",
    "            \"k\": results_total,      \n",
    "            \"fields\": \"chunkVector\", \n",
    "            \"text\": QUESTION\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/search.post.search\",\n",
    "                     data=json.dumps(search_payload), headers=headers, params=params_prj) # 2023-10-01-Preview version\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc66a19",
   "metadata": {},
   "source": [
    "## ...et *voilà*, the two answers are 100% identical\n",
    "You want to confirm the differences, go [here](https://www.diffchecker.com/text-compare/) ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3d535",
   "metadata": {},
   "source": [
    "# BONUS cell: using Azure OpenAI Extensions to make a single OpenAI call that implicitly contacts Azure AI Search\n",
    "Finally, we make a single call to OpenAI, passing all the necessary information to access to Azure Search (its endpoint, authentication method, index name…) in the payload to extract the top chunks from the vector index we built in Azure AI Search.\n",
    "\n",
    "## At this point, the question arises: Who calls Who?\n",
    "…and the answer is that **OpenAI calls Azure Search**, there’s no doubt about it as we’ve only invoked the Azure OpenAI endpoint. So here’s how to implement this second method, which is certainly more practical than the previous one, even if potentially less flexible and controllable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54baf406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"5a73ac5c-c267-47d9-81e9-477e96b98ca5\",\n",
      "  \"model\": \"gpt-4\",\n",
      "  \"created\": 1704622705,\n",
      "  \"object\": \"extensions.chat.completion\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"A meaning function in the context of semantics and linguistic theory is a mathematical or formal mechanism that assigns meanings to expressions in a language. It is a way to represent how the meaning of a complex expression is determined by the meanings of its parts and their syntactic arrangement. A meaning function typically applies to expressions with fully specified syntactic representations and yields unique semantic values, ensuring that syntactic and semantic ambiguity are eliminated from the mapping it defines[doc1]. \\n\\nIn computational linguistics, meaning functions can be used to build a homomorphic mapping from syntax to semantics, ensuring that the meaning of each expression in a corpus is composed from the meanings of its constituent parts[doc3]. The concept of a meaning function is closely related to the principle of compositionality, which posits that the meaning of a complex expression is a function of the meanings of its parts and the rules used to combine them[doc2]. \\n\\nThe meaning function is an essential tool in formal and computational semantics, allowing for the systematic representation of meaning within a language. It can be represented as tables of symbols or lambda expressions, and its size can be computed to understand the complexity of the semantic representation[doc3][doc5].\",\n",
      "        \"end_turn\": true,\n",
      "        \"context\": {\n",
      "          \"messages\": [\n",
      "            {\n",
      "              \"role\": \"tool\",\n",
      "              \"content\": \"{\\\"citations\\\": [{\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\nrequirement that the meaning of an expression be functionally determined by its structure and the \\\\n\\\\nmeanings of its constituents.4 Furthermore, if this pre-theoretical sense of synonymy is crisply \\\\n\\\\ndefined, we can expect a suitably defined compositional function \\u00b5 to reflect it. \\\\n\\\\n \\\\n\\\\n4. BEYOND COMPOSITIONALITY: SYSTEMATIC RELATIONAL THEORIES OF     \\\\n\\\\nMEANING \\\\n\\\\n \\\\n\\\\n    The importance which has been assigned to compositionality in semantic theory can be \\\\n\\\\nattributed to the fact that, until recently, many (most?) semanticists have identified it directly \\\\n\\\\n                                                                                                                                                                                           \\\\nsense of inter-substitutivity.  \\\\n\\\\n4 Fulop and Keenan (forthcoming) (F&K) show that the standard formulation of compositionality \\\\nas the homomorphism requirement on the mapping from the set of syntactic structures of a \\\\nlanguage to the set of its semantic values is not strong enough to exclude certain \\u201cpathological\\u201d \\\\nmeaning functions that yield bizarre patterns of interpretation. These patterns are generated by \\\\nnon-standard changes in interpretation across the set of possible models. F&K suggest two \\\\nsuccessively stronger definitions of compositionality to filter out the problematic meaning \\\\nfunctions that they identify. They achieve this filtering effect by incorporating additional \\\\nconstraints upon the set possible models into the definition of a compositional meaning function. \\\\nWhile F&K\\u2019s counterexamples to the standard compositionality condition provide important \\\\ninsight into the limitations of this condition, they do not affect Zadrozny\\u2019s theorem. As the \\\\nencoding of a meaning function m provided by \\u00b5 does not depend upon properties of models, \\u00b5 \\\\nsatisfies F&P\\u2019s strengthened definitions of compositionality.     \\\\n\\\\n\\\\n\\\\nwith the possibility of a systematic representation of meaning. Davidson (1984) appears to go so \\\\n\\\\nfar as to take compositionality, together with the existence of a finite set of semantic primitives, \\\\n\\\\nto be a necessary condition for the learnability of a language. He expresses this idea as follows.   \\\\n\\\\n \\\\n\\\\nWhen we regard the meaning of each sentence as a function of a finite number of features of the sentence, we have an \\\\n\\\\ninsight not only into what there is to be learned; we also understand how an infinite aptitude can be encompassed by finite \\\\n\\\\naccomplishments. For suppose that a language lacks this feature; then no matter how many sentences a would-be speaker \\\\n\\\\nlearns to produce and understand, there will remain others whose meanings are not given by rules already mastered. It is \\\\n\\\\nnatural to say such a language is unlearnable. [italics Davidson\\u2019s] (p. 8). \\\\n\\\\n \\\\n\\\\nIt is important to note that in order to sustain a homomorphism from the syntax to the semantics \\\\n\\\\nof the sort required by compositionality, a meaning function must apply to expressions with fully \\\\n\\\\nspecified syntactic representations and yield unique semantic values. Therefore, syntactic and \\\\n\\\\nsemantic ambiguity are eliminated from the mapping which such a function defines. Ambiguous \\\\n\\\\nlexical items are separated into words that stand in a one-to-one correspondence with the distinct \\\\n\\\\nsenses of the original terms. Similarly, the different scope readings of a phrase are obtained from \\\\n\\\\ndistinct syntactic sources. So, for example, in Montague (1974) a meaning function applies to \\\\n\\\\nfully disambiguated syntactic structures in which each lexical item has a unique semantic value, \\\\n\\\\nand the relative scope relations of the constituents in a phrase P are fully determined by P\\u2019s \\\\n\\\\nsyntactic derivation tree.  \\\\n\\\\n \\\\n\\\\n\\\\n\\\\n    In fact, it is possible to construct a theory of meaning which is both non-compositional and \\\\n\\\\nsystematic.5 This involves taking the meaning of a syntactically complex expression E to be the \\\\n\\\\nset of values of a relation on the meanings of E\\u2019s syntactic constituents rather than the value of a \\\\n\\\\nfunction. We can sketch the form of such a theory. Let S be the semantic principles (constraints, \\\\n\\\\nrules, etc.) of the language L. For each lexical item E of L, S assigns E a set of semantic values. \\\\n\\\\nFor each syntactically well formed expression E of L with immediate syntactic constituents \\\\n\\\\ne1,...,ek, S generates a relation R(<sv(e1),...,sv(ek)>,int(E)), where sv(ei) (1[ i [ k) is a set of \\\\n\\\\nsemantic values, and  sv(E) = {int(E): R(<sv(e1),...,sv(ek)>,int(E))}.  \\\\n\\\\n \\\\n\\\\n    A theory of this kind is systematic because for each complex expression E in L, there is a \\\\n\\\\nrelation R that maps the semantic values of the immediate constituents of E into an interpretation \\\\n\\\\nof E int(E), and the semantic value of E is the set  int(E) of R\\u2019s values. Therefore, the meaning of \\\\n\\\\nE is determined by the meanings of its constituents. However, as R is not, in general, a function, \\\\n\\\\nit maps an ordered k-tuple of meanings into a set of possible interpretations rather than into a \\\\n\\\\nsingle disambiguated interpretation. \\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1481. Scores=6.3395724Org Highlight count=23.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\n\\\\nar\\\\nX\\\\n\\\\niv\\\\n:c\\\\n\\\\ns.\\\\nC\\\\n\\\\nL\\\\n/0\\\\n\\\\n00\\\\n10\\\\n\\\\n06\\\\n   \\\\n\\\\n9 \\\\nJa\\\\n\\\\nn \\\\n20\\\\n\\\\n00\\\\n\\\\nCOMPOSITIONALITY, SYNONYMY, AND THE SYSTEMATIC REPRESENTATION \\\\nOF MEANING  \\\\n \\\\n                     Shalom Lappin                   and          Wlodek Zadrozny \\\\n                     King\\u2019s College London                       IBM T.J. Watson Research Center \\\\n                     shalom.lappin@kcl.ac.uk                   wlodz@us.ibm.com \\\\n \\\\n \\\\n\\\\n \\\\n\\\\n1. INTRODUCTION     \\\\n\\\\n     In a recent issue of Linguistics and Philosophy Kasmi and Pelletier (1998) (K&P), and \\\\n\\\\nWesterst\\u00e5hl (1998) criticize Zadrozny's (1994) argument that any semantics can be represented \\\\n\\\\ncompositionally. The argument is based upon Zadrozny's theorem that every meaning function m \\\\n\\\\ncan be encoded by a function \\u00b5 such that (i) for any expression E of a specified language L, m(E) \\\\n\\\\ncan be recovered from \\u00b5(E), and (ii) \\u00b5 is a homomorphism from the syntactic structures of L to \\\\n\\\\ninterpretations of L. In both cases, the primary motivation for the objections brought against \\\\n\\\\nZadrozny\\u2019s argument is the view that his encoding of the original meaning function does not \\\\n\\\\nproperly reflect the synonymy relations posited for the language.1 \\\\n\\\\n \\\\n\\\\n In this paper we will look at both the technical issues that K&P and Westerst\\u00e5hl raise, \\\\n\\\\nand the relation between synonymy and compositionality. We will argue that their technical \\\\n\\\\ncriticisms do not go through, while the objection from synonymy assumes an additional \\\\n\\\\nconstraint on compositionality, which they do not make explicit. We also correct some \\\\n\\\\nmisconceptions about the function \\u00b5, e.g. Janssen (1997). We prove that \\u00b5 properly encodes \\\\n\\\\nsynonymy relations, i.e. if two expressions are synonymous, then their compositional meanings \\\\n\\\\nare identical. In Sections 2 and 3 we take up K&P's and Westerst\\u00e5hl's formal objections. Section \\\\n\\\\n\\\\n\\\\n4 considers the relation between compositionality and the systematic representation of meaning. \\\\n\\\\nWe suggest that the reason that semanticists have been anxious to preserve compositionality as a \\\\n\\\\nsignificant constraint on semantic theory is that it has been mistakenly regarded as a condition \\\\n\\\\nthat must be satisfied by any theory that sustains a systematic connection between the meaning \\\\n\\\\nof an expression and the meanings of its parts. Recent developments in formal and computational \\\\n\\\\nsemantics show that systematic theories of meanings need not be compositional. \\\\n\\\\n \\\\n\\\\n Zadrozny\\u2019s argument is based on the observation that a function is defined as a collection \\\\n\\\\nof pairs <argument, value>, such that each argument has only one value in the collection (in \\\\n\\\\ncontrast to a relation in which an argument can be associated with more than one value). The \\\\n\\\\nclassical version of compositionality is defined as a functional dependence of the meaning of an \\\\n\\\\nexpression on the meaning of its parts. Since many semantic theories represent meanings as \\\\n\\\\nfunctions, the requirement of compositionality can be expressed as a condition on the <argument, \\\\n\\\\nvalue> pairs that define the function which assigns meanings to the expressions of a language. \\\\n\\\\nSpecifically, \\u00b5(s.t) = \\u00b5(\\u00b5(s),\\u00b5(t)) means that \\u00b5 must have among its elements the pairs <s.t, \\\\n\\\\n\\u00b5(\\u00b5(s),\\u00b5(t))>, <s,\\u00b5(s)>, and <t,\\u00b5(t)>. Using this approach, the following theorem is proven. \\\\n\\\\n \\\\n\\\\nTHEOREM 1: Let M be an arbitrary set. Let A be an arbitrary alphabet. Let \\u2018.\\u2019 be a binary \\\\n\\\\noperation (concatenation), and let S be the set closure of A under \\u2018.\\u2019. Let m: S dM be an arbitrary \\\\n\\\\nfunction. Then there is a set of functions M* and a unique map \\u00b5: Sd M* such that for all s,t in S, \\\\n\\\\n\\u00b5(s.t) = \\u00b5(\\u00b5(s),\\u00b5(t)) = \\u00b5(s)(\\u00b5(t)) , and \\u00b5(\\u00b5(s)) = m(s).  \\\\n\\\\n \\\\n\\\\n                                                                                                                                                                                           \\\\n1Janssen (1997) makes a similar claim. \\\\n\\\\n\\\\n\\\\nTheorem 1 entails that even if for the original meaning function m of a language L, m(A) \\\\n\\\\n= m(B) and m(C.A) g m(C.B), there is a function \\u00b5 such that \\u00b5(C.A) = \\u00b5(C)(\\u00b5(A)), and \\u00b5(C.B) = \\\\n\\\\n\\u00b5(C)(\\u00b5(B)).2  \\\\n\\\\n \\\\n\\\\n In the same paper, it is shown (Corollary 2) that the language L doesn\\u2019t have to be closed \\\\n\\\\nunder concatenation, and (Proposition 3) that the original meanings given by m(s) can be \\\\n\\\\nrecovered in other ways, e.g. by having  \\u00b5(s)($) = m(s), for some element $. We will use the last \\\\n\\\\nfact (and method) in the proof of the Synonymy Theorem in Section 3.    \\\\n\\\\n \\\\n\\\\n2. FUNCTIONALITY, TYPE RAISING  and  K&P\\u2019s OBJECTIONS  \\\\n\\\\n \\\\n\\\\n    Kasmi and Pelletier (1998) (K&P) raise two main objections. First, they observe that \\\\n\\\\nZadrozny\\u2019s encoding of L\\u2019s semantics substitutes functions for the original meanings that m \\\\n\\\\nassigns to the expressions of L. So, for example, \\u00b5(A) = f1 rather than m(A), and \\u00b5(B) = f2 rather \\\\n\\\\nthan m(B), where m(A) = m(B). They conclude that \\u00b5 is not a meaning function in the sense that \\\\n\\\\nm is. Therefore, while \\u00b5 is indeed compositional, it does not encode the semantics of L. \\\\n\\\\n \\\\n\\\\n\\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1851. Scores=5.1756372Org Highlight count=19.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0\\\\nIf our corpus were to be the 10 \\u00d7 100 table consisting\\\\nof all verb-noun combinations:\\\\n\\\\nv0n0 + v1n0 + ... + vjni + ...\\\\n\\\\nwe could quickly use the previous example to write a simple finite\\\\nstate grammar that describes the corpus:\\\\n\\\\n{v0|v1|...}{n0|n1...} (21 + 201)\\\\n\\\\nBut in this subsection we are supposed to introduce some seman-\\\\ntics. Thus, let our corpus consist of all those 1,000 sentences to-\\\\ngether with their meanings, which, to keep things as simple as\\\\npossible, will be simplified to two attributes. Also, for the reason\\\\nof simplicity, we assume that only \\u201dkick bucket\\u201d has an idiomatic\\\\nmeaning, and all other entries are assigned the meaning consisting\\\\nof the two attribute expression [[action, vj ], [object, ni]]. Hence,\\\\nour corpus will look as follows:\\\\n\\\\nkick bucket action die object nil\\\\n\\\\nv1 bucket action v1 object bucket\\\\n\\\\n...\\\\n\\\\nvj ni action vj object ni\\\\n\\\\n...\\\\n\\\\nNow, notice that this corpus cannot be encoded by means of a\\\\nshort finite state grammar, because of the dependence of the mean-\\\\nings (i.e. the pair [action, ..., object, ...]) on the first two elements\\\\nof each sentence. We will have to extend our grammar formalism\\\\nto address this dependence (Section 3).\\\\n\\\\n7\\\\n\\\\n\\\\n\\\\n2.4 On meaning functions\\\\n\\\\nEven though we cannot encode the corpus by a short, finite state\\\\ngrammar, we can easily provide for it a compositional semantics.\\\\nTo avoid the complications of type raising, we will build a homo-\\\\nmorphic mapping from syntax to semantics. To do it, it is enough\\\\nto build meaning functions in a manner ensuring that the meaning\\\\nof each vjni is composed from the meaning of vj and the mean-\\\\ning of ni. Since our corpus is simple, these meaning functions are\\\\nsimple, too: For the verbs the meaning function is given by the\\\\ntable:\\\\n\\\\n[v0, [verb, v0]]; [v1, [verb, v1]] ... [v9, [verb, v9]] (90)\\\\n\\\\nFor the nouns:\\\\n\\\\n[n0, [noun, n0]]; [n1, [noun, n1]] ... [n99, [noun, n99]] (900)\\\\n\\\\nWe have represented both meaning functions as tables of sym-\\\\nbols. Since this chapter deals with sizes of objects, we compute\\\\nthem for the meaning functions: the size of the first function is\\\\n90 = 10\\u00d79, and for the second one it is 900 = 100\\u00d79. Therefore,\\\\nthe meaning function for the whole corpus could be represented\\\\nas a table with 1,000 entries:\\\\n\\\\n[[[verb, v9], [noun, n99]], [[action, v9], [object, n99]]]\\\\n. . .\\\\n[[[verb, vj ], [noun, ni]], [[action, vj ], [object, ni]]]\\\\n. . .\\\\n[[[verb, v1], [noun, bucket]], [[action, v1 ], [object, bucket]]]\\\\n[[[verb, kick], [noun, bucket]], [[action, die], [object, nil]]\\\\n\\\\nand the size of this table is 29\\u00d71000. Finally, the total size of the\\\\ntables that describe the compositional interpretation of the corpus\\\\nis 29000 + 900 + 90, i.e. roughly 30, 000. Notice that if we had\\\\nmore verbs and nouns, the tables describing the meaning functions\\\\n\\\\n8\\\\n\\\\n\\\\n\\\\nwould be even larger.2 Also, note that we have not counted the\\\\ncost of encoding the positions of elements of the table, which would\\\\nbe the log of the total number of symbols in the table. This\\\\nsimplifying assumption does not change anything in the strength\\\\nof our arguments (as larger tables have longer encodings).\\\\n\\\\n3 Compositional semantics through the Min-\\\\n\\\\nimum Description Length principle\\\\n\\\\nIn this section we first extend our notation to deal with semantic\\\\ngrammars. Then we apply the minimum description length princi-\\\\nple to construct a compact representation of our example corpus.\\\\nThis experience will motivate our new, non-vacuous definition of\\\\nthe notion of compositional semantics given in Section 4.\\\\n\\\\n3.1 Representations\\\\n\\\\nWe have seen that it is impossible to efficiently encode our se-\\\\nmantic corpus using a finite state grammar. Therefore, we have\\\\nto make our representation of grammars more expressive (at the\\\\nprice of a slightly bigger interpreter). Namely, we will allow a sim-\\\\nple form of unification.\\\\n\\\\n2 The reader familiar with [12] should notice that the meaning functions\\\\nobtained by the solution lemma also consist of tables of element-value pairs. It\\\\nis easy to see that for the corpus we are encoding the solution lemma produces\\\\nthe same meaning functions.\\\\n\\\\nIn the other direction, the method for deriving compositional semantics us-\\\\ning the minimum description length principle (Sections 3 and 4) are directly\\\\napplicable to meaning functions obtained by the solution lemma in [12], pro-\\\\nvided they are finite (which covers the practically interesting cases); and it\\\\nseems applicable to the infinite case, if it has a finite representation. However,\\\\nwe will not pursue this connection any further.\\\\n\\\\n9\\\\n\\\\n\\\\n\\\\nExample. Assume we do not want {a|b}{a|b|d} to generate ab.\\\\nWe can do it by changing the notation:\\\\n\\\\nX = {a|b}\\\\n{X}{X|d}\\\\n\\\\nThe intention is simple: first, we define a class variable (X) for the\\\\nclass consisting of elements a and b; then, we generate all strings\\\\nusing the rule with variable X: XX and Xd; and finally we substi-\\\\ntute for X all its possible values, which produces aa, ad, bb and bd.\\\\n\\\\nMore generally, let us assume that we have an alphabet a1, a2, ...,\\\\n\\\\nen\\\\n0001002v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1441. Scores=7.5886955Org Highlight count=19.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA2djEucGRm0\\\\nthrough functional composition) provides one of the strongest instances of a compositional \\\\n\\\\nencoding of meaning. It is more powerful than the mere requirement of a homomorphism \\\\n\\\\nbetween syntactic structure and semantic values. Zadrozny\\u2019s theorems establish that even if we \\\\n\\\\ninsist on casting our theory of meaning in functional form, compositionality without additional \\\\n\\\\nconstraints is, in itself, a vacuous requirement. Hence, the conclusion holds for the weaker \\\\n\\\\ncondition of homomorphism. \\\\n\\\\n \\\\n\\\\n Let us now take up the issue of synonymy. Westerst\\u00e5hl claims that Zadrozny's encoding \\\\n\\\\nof m through \\u00b5 sustains compositionality at the cost of violating the meaning relations posited for \\\\n\\\\nL.  Therefore, the semantics which \\u00b5 provides for L is arbitrary. K&P\\u2019s first  objection is a \\\\n\\\\nversion of this point. Janssen (1997) also raises it: \\\\n\\\\n \\\\n\\\\nOn the semantic side some doubts can be raised. The given original meanings are encoded using non-well founded sets. It \\\\n\\\\nis strange that synonymous sentences get different meanings. (p. 456). \\\\n\\\\n \\\\n\\\\n\\\\n\\\\nJanssen\\u2019s objections are misplaced, and they can be addressed through a technical argument. \\\\n\\\\nNamely, the function \\u00b5 can have the synonymy property, i.e. it can map synonymous expressions \\\\n\\\\ninto the same meanings. In fact, the function used in the proof of Proposition 3 in \\\\n\\\\nZadrozny(1994) does have the synonymy property.  \\\\n\\\\n \\\\n\\\\nTheorem (The synonymy theorem): Let S be a language with a binary operation \\u2018.\\u2019 , and let M \\\\n\\\\nbe an arbitrary set. Let m: S d M be an arbitrary function (assigning meanings to expressions of \\\\n\\\\nS). Let S* be the set  { s.$  :  s c  S }, where $ is a distinguished element outside S (standing for \\\\n\\\\n\\u201cend-of-expression\\u201d). \\\\n\\\\nThen there is a set of functions M* and a unique map \\u00b5: S* d M* such that for all s,t c S,  \\\\n\\\\n1. Compositionality:  \\u00b5(s.t) = \\u00b5(\\u00b5(s),\\u00b5(t)),  \\\\n\\\\n2. Recoverability of meanings:  \\u00b5(s.$) = m(s) \\\\n\\\\n3. Synonymy:  \\\\n\\\\nIf a and b are synonyms, i.e. they appear in the same strings of S, and can be \\\\n\\\\nintersubstituted without the change of meanings; i.e. m(a) = m(b) and m(xay) = m(xby) for \\\\n\\\\nall xay in S. Then \\\\n\\\\n\\u00b5(a) = \\u00b5(b). \\\\n\\\\n \\\\n\\\\n \\\\n\\\\nFor the proof, see the Appendix.  \\\\n\\\\n \\\\n\\\\nWesterst\\u00e5hl  makes his claims on the basis of the fact that where m(A) = m(B),  \\\\n\\\\n\\\\n\\\\n\\u00b5(A) g\\u00b5(B), when m(A.C) g m(B.C)  for some C. That is, compositionality breaks for C. In fact, \\\\n\\\\nthat  \\u00b5(A) g\\u00b5(B) holds, even though m(A)= m(B), is not at all a symptom of ad hoc design for \\u00b5. \\\\n\\\\nIt is an inescapable consequence of (i) the compositionality of \\u00b5, and (ii) the fact that \\u00b5 must \\\\n\\\\nencode the constraint expressed by m(C.A)gm(C.B). Clearly, if A and B are not \\\\n\\\\ninter-substitutable in all expressions in which they occur as consituents, it is not possible for a \\\\n\\\\ncompositional function to assign them the same values.3 As we have seen above, if A and B are \\\\n\\\\ninter-substitutable  we do in fact have \\u00b5(A)=\\u00b5(B). \\\\n\\\\n \\\\n\\\\nIt is important to distinguish two notions of synonymy. On the first, synonymy corresponds to \\\\n\\\\nsameness of meaning in some pre-theoretical sense. On the second, two expressions are \\\\n\\\\nsynonymous iff they are inter-substitutable in all expressions in which they occur as constituents. \\\\n\\\\nWe can take m as representing the given meaning relations of L. It is reasonable to require that \\\\n\\\\nany encoding of m that purports to offer an adequate representation of L's semantics  preserve, at \\\\n\\\\nsome level of representation, the meaning relations that m defines for L. In fact, \\u00b5 does this by \\\\n\\\\nvirtue of the fact that it assigns functions to the expressions of L that retrieve m(E) for each E of \\\\n\\\\nL. Specifically, \\u00b5 preserves the sameness of meaning that m establishes for A and B to the extent \\\\n\\\\nthat f1 assigns the same value to A that f2 assigns to B. However, if m does not establish \\\\n\\\\nsynonymy between A and B in the sense of inter-substitutivity, this lack of synonymy between A \\\\n\\\\nand B is explicitly represented on \\u00b5, which assigns them distinct values.   \\\\n\\\\n \\\\n\\\\nTo insist that a compositional semantic function directly encode sameness of meaning taken in \\\\n\\\\nthe pre-theoretical sense (i.e. sameness of value as established either by another meaning \\\\n\\\\n                                                           \\\\n3See Mates (1952) for a discussion of the relation between compositionality and synonymy in the \\\\n\\\\n\\\\n\\\\nfunction or by linguistic intuition) as identity of value is to add a requirement to the condition of \\\\n\\\\ncompositionality that goes beyond both the homomorphism constraint and the demand that the \\\\n\\\\nfunction express the given meaning relations of L. Clearly \\u00b5 does not satisfy this additional \\\\n\\\\nconstraint, nor could any compositional function, given the assumptions concerning the meaning \\\\n\\\\nrelations that m defines for L. But this constraint is not explicitly incorporated into the notion of \\\\n\\\\ncompositionality that semantic theorists have employed, and that K&P and Westerst\\u00e5hl invoke in \\\\n\\\\ntheir criticisms. If it is adopted, then compositionality becomes a stronger condition than the \\\\n\\\\n\\\\nen\\\\n0001006v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"Microsoft Word - LP99-sub.htm\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1552. Scores=5.05388Org Highlight count=19.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}, {\\\"content\\\": \\\"aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDAydjEucGRm0\\\\nWhat are the advantages of defining compositionality using\\\\nthe Minimum Description Length principle? 1. It brings us back\\\\nto the original definition of compositionality, but makes it non-\\\\nvacuous. 2. It encodes the postulate that the meaning functions\\\\nshould be simple. 3. It allows us to distinguish between composi-\\\\ntional and non-compositional semantics by means of systematicity,\\\\ni.e. the minimality of encodings, as e.g. Hirst [5] wanted. 4. It\\\\ndoes not make a reference to non-intrinsic properties of mean-\\\\ning functions (like being a polynomial). 5. It works for different\\\\nmodels of language understanding: pipeline (syntax, semantics,\\\\npragmatics), construction grammars (cf. [3]), and even semantic\\\\ngrammars. 6. It allows us to compare different meaning functions\\\\nwith respect to how compositional they are \\u2014 we can measure\\\\nthe size of their domains and the length of the encodings. Finally,\\\\nthis definition might even satisfy those philosophers of language\\\\nwho regard compositionality not as a formal property but as an\\\\nunattainable ideal worth striving for. This hope is based on the\\\\nfact that, given an appropriately rich model of language, its min-\\\\nimum description length is, in general, non-computable, and can\\\\n\\\\n4We are assuming that we have to assign the noun and verb categories to\\\\nthe lexical symbols of the corpus.\\\\n\\\\n18\\\\n\\\\n\\\\n\\\\nonly be approximated but never exactly computed.\\\\n\\\\n5 Discussion and Conclusions\\\\n\\\\nLambdas, approximations, and the minimum description\\\\n\\\\nlength\\\\n\\\\nAssuming that we have a \\u03bb-expressions interpreter (e.g. a lisp\\\\nprogram), we could describe the meaning functions of Section 3\\\\nas:\\\\n\\\\n\\u03bbX.[noun,X]\\\\n\\u03bbY.[verb, Y ]\\\\n\\u03bb[verb, Y ][noun,X].[[action, Y ], [object,X]]\\\\n\\u03bb[verb, kick][noun, bucket].[[action, die], [object, nil]]\\\\n\\\\nThe approximate total size of this description is size(\\u03bb\\u2212interpreter)\\\\n+ 66 (the above definitions) + 110 (to describe the domains of the\\\\nfirst two functions).\\\\n\\\\nClearly, the last lambda expression corresponds to an idiomatic\\\\nmeaning. But, note that this definition assigns also the non-\\\\nidiomatic meaning to \\u201dkick bucket\\u201d. Thus, although much simpler,\\\\nit does not exactly correspond to the original meaning function.\\\\nIt does however correspond to grammar GV (0)N(0) of the previous\\\\nsection. Also, representations that ignore exceptions are more of-\\\\nten found in the literature. This point may be worth pursuing:\\\\nSavitch in [9] argues that approximate representation in a more\\\\nexpressive language can be more compact. For approximate rep-\\\\nresentations that overgeneralize, the idiomaticity of an expression\\\\ncan be defined as the existence of a more specific definition of its\\\\nmeaning.\\\\n\\\\nBridging linguistic and probabilistic approaches to natu-\\\\n\\\\nral language\\\\n\\\\nThe relationship between linguistics principles and the MDL\\\\nmethod is not completely surprising. We used the MDL principle\\\\n\\\\n19\\\\n\\\\n\\\\n\\\\nin [13] to argue for a construction-based approach to language un-\\\\nderstanding (cf. [3]). After setting up a formal model based on\\\\nlinguistic and computational evidence, we applied the MDL prin-\\\\nciple to prove that construction-based representations are at least\\\\nan order of magnitude more compact that the corresponding lexi-\\\\ncalized representations of the same linguistic data. The argument\\\\npresented there suggests that in building compositional semantics\\\\nwe might be better off when the language is build by means of\\\\nreach combinatorics (constructions), than by the concatenation of\\\\nlexical items. However, this hypothesis remains to be proved.\\\\n\\\\nIt is known that the most important rules of statistical rea-\\\\nsoning, the maximum likelihood method, the maximum entropy\\\\nmethod, the Bayes rule and the minimum description length, are\\\\nall closely related (cf. pp. 275-321 of [7]). From the material\\\\nof Sections 3 and 4 we can see that compositionality is closely\\\\nrelated to the MDL principle; thus, it is possible to imagine bring-\\\\ning together linguistic and statistical methods for natural language\\\\nunderstanding. For example, starting with semantic classes of [2]\\\\ncontinue derivation of semantic model for a large corpus using the\\\\nmethod of Section 3 with the computational implementation along\\\\nthe lines of [1].\\\\n\\\\nConclusion\\\\n\\\\nWe have redefined the linguistic concept of compositionality as\\\\nthe simplest maximal description of data that satisfies the postu-\\\\nlate that the meaning of the whole is a function of the meaning\\\\nof its parts. By justifying compositionality by the minimum de-\\\\nscription length principle, we have placed the intuitive idea that\\\\nthe meaning of a sentence is a combination of the meanings of its\\\\nconstituents on a firm mathematical foundation.\\\\n\\\\nThis new, non-vacuous definition of compositionality is intu-\\\\nitive and allows us to distinguish between compositional and non-\\\\n\\\\n20\\\\n\\\\n\\\\n\\\\ncompositional semantics, and between idiomatic and non-idiomatic\\\\nexpressions. It is not ad hoc, since it does not make any references\\\\nto non-intrinsic properties of meaning functions (like being a poly-\\\\nnomial). \\\\nen\\\\n0001002v1.pdf\\\", \\\"id\\\": null, \\\"title\\\": \\\"arXiv:cs/0001002v1  [cs.CL]  4 Jan 2000\\\", \\\"filepath\\\": null, \\\"url\\\": null, \\\"metadata\\\": {\\\"chunking\\\": \\\"orignal document size=1275. Scores=6.6028976Org Highlight count=18.\\\"}, \\\"chunk_id\\\": \\\"0\\\"}], \\\"intent\\\": \\\"[\\\\\\\"meaning function definition\\\\\\\", \\\\\\\"what is a meaning function\\\\\\\", \\\\\\\"explanation of meaning function\\\\\\\"]\\\"}\",\n",
      "              \"end_turn\": false\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 10184,\n",
      "    \"completion_tokens\": 261,\n",
      "    \"total_tokens\": 10445\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results_total = 2\n",
    "\n",
    "openaicall_headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_OPENAI_API_KEY']}\n",
    "openaicall_params  = {'api-version': '2023-12-01-preview'}\n",
    "\n",
    "search_payload = {\n",
    "    \"count\": \"true\",\n",
    "    \"select\": \"id, name, title, location, chunk\",\n",
    "    \"top\": results_total,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": QUESTION\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1.0,    \n",
    "    \"max_tokens\": 1000,\n",
    "    \"dataSources\": [\n",
    "        {            \n",
    "            \"type\": \"AzureCognitiveSearch\",\n",
    "            \"parameters\": {                \n",
    "                \"endpoint\": os.environ['AZURE_SEARCH_ENDPOINT'],\n",
    "                \"key\": os.environ['AZURE_SEARCH_KEY'],\n",
    "                \"indexName\": index_name\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# VERSION 1106 IS NEEDED FOR COMPLETIONS. For more information: https://learn.microsoft.com/en-us/azure/ai-services/openai/overview\n",
    "r = requests.post(\n",
    "    f\"{os.environ['AZURE_OPENAI_ENDPOINT']}openai/deployments/{os.environ['GPT4-1106-128k']}/extensions/chat/completions\",\n",
    "    data=json.dumps(search_payload), headers=openaicall_headers, params=openaicall_params)\n",
    "\n",
    "print(json.dumps(r.json(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivbd_20231215",
   "language": "python",
   "name": "openaivbd_20231215"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
