{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
   "metadata": {},
   "source": [
    "# Understanding Memory in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
   "metadata": {},
   "source": [
    "In the previous Notebook, we successfully explored how OpenAI models can enhance the results from Azure Cognitive Search. \n",
    "\n",
    "However, we have yet to discover how to engage in a conversation with the LLM. With [Bing Chat](http://chat.bing.com/), for example, this is possible, as it can understand and reference the previous responses.\n",
    "\n",
    "There is a common misconception that GPT models have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
    "\n",
    "In this Notebook, our goal is to illustrate how we can effectively \"endow the LLM with memory\" by employing prompts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from openai.error import OpenAIError\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import CosmosDBChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import (\n",
    "    get_search_results,\n",
    "    update_vector_indexes,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    num_tokens_from_string,\n",
    "    get_answer,\n",
    ")\n",
    "\n",
    "from common.prompts import COMBINE_CHAT_PROMPT_TEMPLATE\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials_my.env\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"]    = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"]     = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"]    = os.environ[\"OPENAI_API_TYPE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
   "metadata": {},
   "source": [
    "### Let's start with the basics\n",
    "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION           = \"Tell me some use cases for reinforcement learning\"\n",
    "FOLLOW_UP_QUESTION = \"Give me the main points of our conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = os.environ[\"COMPLETION3516_DEPLOYMENT\"] # we DO have chatgpt-4 too, but not needed in this case\n",
    "COMPLETION_TOKENS = 500\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Game playing: Reinforcement learning has been successfully used to train agents to play complex games like chess, Go, and poker. These agents can learn strategies, adapt to opponents, and improve their gameplay over time.\n",
       "\n",
       "2. Robotics: Reinforcement learning can be applied to train robots to perform various tasks, such as object manipulation, grasping, and navigation. The agents learn to interact with their environment, receive rewards or penalties based on their actions, and optimize their behavior accordingly.\n",
       "\n",
       "3. Autonomous vehicles: Reinforcement learning can be used to train self-driving cars to make decisions on the road. The agents learn to navigate traffic, follow traffic rules, and make safe and efficient driving choices.\n",
       "\n",
       "4. Resource management: Reinforcement learning can be applied to optimize the allocation of resources, such as energy, bandwidth, or server capacity. The agents learn to make decisions that maximize efficiency, minimize costs, or optimize performance based on the available resources.\n",
       "\n",
       "5. Recommendation systems: Reinforcement learning can be used to build personalized recommendation systems. The agents learn from user feedback to recommend relevant products, movies, or content, adapting to individual preferences and improving the accuracy of recommendations over time.\n",
       "\n",
       "6. Healthcare: Reinforcement learning can assist in medical treatment planning, drug dosage optimization, and personalized therapy. The agents can learn optimal treatment strategies based on patient data, clinical guidelines, and feedback from medical professionals.\n",
       "\n",
       "7. Finance: Reinforcement learning can be applied to financial trading and portfolio management. The agents learn to make investment decisions, manage risk, and optimize trading strategies based on market data and financial indicators.\n",
       "\n",
       "8. Industrial control systems: Reinforcement learning can be used to optimize and control complex industrial processes, such as power generation, chemical manufacturing, or supply chain management. The agents learn to make decisions that maximize productivity, minimize costs, and ensure smooth operation.\n",
       "\n",
       "9. Natural language processing: Reinforcement learning can be applied to build conversational agents or chatbots. The agents learn to generate appropriate responses, engage in meaningful conversations, and improve their language understanding and generation capabilities.\n",
       "\n",
       "10. Personalized education: Reinforcement learning can be used to develop adaptive learning systems. The agents learn to personalize educational content, adapt teaching strategies, and provide tailored feedback to individual learners, enhancing the effectiveness and efficiency of the learning process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "response = chain.run(QUESTION)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I apologize, but as an AI language model, I don't have the capability to remember previous conversations. Once a conversation ends, the information is not stored or accessible. However, I'm here to help with any new questions or topics you'd like to discuss.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "chain.run(FOLLOW_UP_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As you can see, it doesn't remember what it just responded, sometimes it responds based only on the system prompt, or just randomly. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "                {history}\n",
    "                Human: {question}\n",
    "                AI:\n",
    "            \"\"\"\n",
    "    )\n",
    "chain = LLMChain(llm=llm, prompt=hist_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Reinforcement learning can be used in various domains such as game playing, robotics, autonomous vehicles, resource management, recommendation systems, healthcare, finance, industrial control systems, natural language processing, and personalized education.\n",
       "- In game playing, reinforcement learning can train agents to play complex games and improve their gameplay over time.\n",
       "- In robotics, reinforcement learning can train robots to perform tasks like object manipulation, grasping, and navigation.\n",
       "- In autonomous vehicles, reinforcement learning can help self-driving cars make safe and efficient driving decisions.\n",
       "- In resource management, reinforcement learning can optimize the allocation of resources like energy, bandwidth, or server capacity.\n",
       "- In recommendation systems, reinforcement learning can personalize recommendations based on user feedback and improve accuracy over time.\n",
       "- In healthcare, reinforcement learning can assist in treatment planning, drug dosage optimization, and personalized therapy.\n",
       "- In finance, reinforcement learning can be used in trading and portfolio management to make investment decisions and manage risk.\n",
       "- In industrial control systems, reinforcement learning can optimize complex processes like power generation or supply chain management.\n",
       "- In natural language processing, reinforcement learning can be used to build conversational agents or chatbots that generate appropriate responses and improve language understanding.\n",
       "- In personalized education, reinforcement learning can develop adaptive learning systems that personalize educational content and provide tailored feedback to individual learners."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain.run({\"history\":Conversation_history, \"question\":FOLLOW_UP_QUESTION}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
   "metadata": {},
   "source": [
    "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
   "metadata": {},
   "source": [
    "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba257e86-fd90-4a51-a72d-27000913e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Memory adds tokens to the prompt, we would need a better model that allows more space on the prompt\n",
    "COMPLETION_TOKENS = 1000\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
    "embedder = OpenAIEmbeddings(deployment=os.environ[\"EMBEDDING_DEPLOYMENT\"], chunk_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cogsrch-index-files-vector',\n",
       " 'cogsrch-index-csv-vector',\n",
       " 'cogsrch-index-books-vector']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index1_name    = \"cogsrch-index-files\"\n",
    "index2_name    = \"cogsrch-index-csv\"\n",
    "index3_name    = \"cogsrch-index-books-vector\"\n",
    "text_indexes   = [index1_name, index2_name]\n",
    "vector_indexes = [index+\"-vector\" for index in text_indexes] + [index3_name]\n",
    "vector_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b01852c2-6192-496c-adff-4270f9380469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 5\n",
      "CPU times: user 4.65 s, sys: 82.6 ms, total: 4.73 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Search in text-based indexes first and update vector indexes\n",
    "k = 10 # Top k results per each text-based index\n",
    "ordered_results = get_search_results(QUESTION, text_indexes, k=k, reranker_threshold=1, vector_search=False)\n",
    "update_vector_indexes(ordered_search_results=ordered_results, embedder=embedder)\n",
    "\n",
    "# Search in all vector-based indexes available\n",
    "similarity_k = 5 # top results from multi-vector-index similarity search\n",
    "ordered_results = get_search_results(QUESTION, vector_indexes, k=k, vector_search=True,\n",
    "                                        similarity_k=similarity_k,\n",
    "                                        query_vector = embedder.embed_query(QUESTION))\n",
    "\n",
    "print(\"Number of results:\",len(ordered_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca500dd8-148c-4d8a-b58b-2df4c957459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('aHR0cHM6Ly9ibG9ic3RvcmFnZW9xdWRlcnRlamZjcXcuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA4djIucGRm0_12',\n",
       "              {'title': 'arXiv:cs/0001008v2  [cs.MA]  17 Jan 2000_chunk_12',\n",
       "               'name': '0001008v2.pdf',\n",
       "               'location': 'https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf',\n",
       "               'caption': 'Matarić [1995] has studied reinforcement learning in multi-robot domains. She notes, for example, how learning can give rise to social behaviors (Matarić [1997]). The work shows how robots can be individually programmed to pro- duce certain group behaviors.',\n",
       "               'index': 'cogsrch-index-files-vector',\n",
       "               'chunk': 'predictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [1995] has studied reinforcement learning in multi-robot domains.\\nShe notes, for example, how learning can give rise to social behaviors (Matarić\\n[1997]). The work shows how robots can be individually programmed to pro-\\nduce certain group behaviors. It represents a good example of the usefulness\\nand flexibility of learning agents in multi-agent domains. However, the author\\ndoes not offer a mathematical justification for the chosen individual learning\\nalgorithms, nor does she explain why the agents were able to converge to the\\nglobal behaviors. Our research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[1997]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents learn-\\ning about agents (Sen [1996], Weiß [1997]). For example, Sen and Sekaran [1998]\\nshow how learning agents in simple MAS converge to system-wide optimal be-\\nhavior. Their agents use Q-learning or modified classifier systems in order to\\nlearn. The authors implement these agents and compare the performance of the\\ndifferent learning algorithms for developing agent coordination. Hu and Well-\\nman [1998b, 1996] have studied reinforcement learning in market-base MASs,\\nshowing how certain initial learning biases can be self-fulfilling, and how learn-\\ning can be useful but is affected by an agent’s models of other agents. Claus\\nand Boutilier [1997] have also carried out experimental studies of the behavior of\\nreinforcement learning agents. We have been able to use the CLRI framework\\nto predict some of their experimental results Vidal [1998]. Other researchers\\nsuch as Stone and Veloso [1999], Littman [1994], and Hu and Wellman [1998a]\\nhave extended the basic Q-learning Watkins and Dayan [1992] algorithm for use\\nwith MASs in an effort to either improve or prove convergence to the optimal\\nbehavior.\\n\\nWe have also successfully experimented with reinforcement learning simu-\\nlations (Vidal and Durfee [1998]), but we believe that the formal treatment\\nelucidated in these pages will shed more light into the real nature of the prob-\\nlem and the relative importance of the various parameters that describe the\\n\\n25\\n\\n\\n\\ncapabilities of an agent’s learning algorithm.\\n\\n11 Summary\\n\\nWe have presented a framework for studying and predicting the behavior of\\nMASs composed of learning agents. We believe that this framework captures the\\nmost important parameters that describe an agents’ learning and the system’s\\nrules of encounter. Various comparisons between the framework’s predictions\\nand experimental results were given. These comparisons showed that the theo-\\nretical predictions closely match our experimental results and the experimental\\nresults published by others. Our success in reproducing these results allows us\\nto confidently state the effectiveness and accuracy of our theory in predicting\\nthe expected error of machine learning agents in MASs.\\n\\nSince our theory describes an agent’s behavior at a high-level (i.e., the agent’s\\nerror), it is not capable of making system-specific predictions (e.g., predicting\\nthe particular actions that are favored). These types of system-specific predic-\\ntions can only be arrived at by the traditional method of implementing popula-\\ntions of such agents and testing their behaviors. However, we expect that there\\nwill be times when the predictions from our theory will be enough to answer a\\ndesigner’s questions. A MAS designer that only needs to determine how “good”\\nthe agent’s behavior will be could probably use the CLRI framework. A de-\\nsigner that needs to know which particular emergent behaviors will be favored\\nby his agents will need to implement the agents.\\n\\nFinally, while we have given some examples as to how learning rates can be\\ndetermined for particular machine learning implementations, we do not have\\nany general method for determining these rates. However, we showed how to\\nuse the sample complexity of a learning problem to determine a lower bound on\\nthe learning rate of a consistent learning agent. This bound is useful for quickly\\n',\n",
       "               'score': 0.03306011110544205}),\n",
       "             ('snqdma0s_0',\n",
       "              {'title': 'Correcting errors in synthetic DNA through consensus shuffling_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/',\n",
       "               'caption': 'Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'Although efficient methods exist to assemble synthetic oligonucleotides into genes and genomes, these suffer from the presence of 1–3 random errors/kb of DNA. Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population. The DNA is fragmented, and mismatched fragments are removed upon binding to an immobilized mismatch binding protein (MutS). PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population. We show that two iterations of consensus shuffling improved a population of synthetic green fluorescent protein (GFPuv) clones from ∼60 to >90% fluorescent, and decreased errors 3.5- to 4.3-fold to final values of ∼1 error per 3500 bp. In addition, two iterations of consensus shuffling corrected a population of GFPuv clones where all members were non-functional, to a population where 82% of clones were fluorescent. Consensus shuffling should facilitate the rapid and accurate synthesis of long DNA sequences.',\n",
       "               'score': 0.03229166567325592}),\n",
       "             ('q1y8uscj_0',\n",
       "              {'title': 'Recent Hits Acquired by BLAST (ReHAB): A tool to identify new hits in sequence similarity searches_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/',\n",
       "               'caption': 'ReHAB is designed to handle large numbers of query sequences, such as whole genomes or sets of genomes. Advanced computer skills are not needed to use ReHAB; the graphics interface is simple to use and was designed with the bench biologist in mind.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'BACKGROUND: Sequence similarity searching is a powerful tool to help develop hypotheses in the quest to assign functional, structural and evolutionary information to DNA and protein sequences. As sequence databases continue to grow exponentially, it becomes increasingly important to repeat searches at frequent intervals, and similarity searches retrieve larger and larger sets of results. New and potentially significant results may be buried in a long list of previously obtained sequence hits from past searches. RESULTS: ReHAB (Recent Hits Acquired from BLAST) is a tool for finding new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with two versions of a protein sequence database and highlights hits that are present only in the updated database. Results are presented in an easily comprehended table, or in a BLAST-like report, using colors to highlight the new hits. ReHAB is designed to handle large numbers of query sequences, such as whole genomes or sets of genomes. Advanced computer skills are not needed to use ReHAB; the graphics interface is simple to use and was designed with the bench biologist in mind. CONCLUSIONS: This software greatly simplifies the problem of evaluating the output of large numbers of protein database searches.',\n",
       "               'score': 0.03181818127632141}),\n",
       "             ('ri5v6u4x_0',\n",
       "              {'title': 'Towards evidence-based, GIS-driven national spatial health information infrastructure and surveillance services in the United Kingdom_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/',\n",
       "               'caption': 'this paper has the following objectives: (1) to illustrate with practical, real-world scenarios and examples from the literature the different gis methods and uses to improve community health and healthcare practices, e.g., for improving hospital bed availability, in community health and bioterrorism surveillance services, and in the latest sars …',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'The term \"Geographic Information Systems\" (GIS) has been added to MeSH in 2003, a step reflecting the importance and growing use of GIS in health and healthcare research and practices. GIS have much more to offer than the obvious digital cartography (map) functions. From a community health perspective, GIS could potentially act as powerful evidence-based practice tools for early problem detection and solving. When properly used, GIS can: inform and educate (professionals and the public); empower decision-making at all levels; help in planning and tweaking clinically and cost-effective actions, in predicting outcomes before making any financial commitments and ascribing priorities in a climate of finite resources; change practices; and continually monitor and analyse changes, as well as sentinel events. Yet despite all these potentials for GIS, they remain under-utilised in the UK National Health Service (NHS). This paper has the following objectives: (1) to illustrate with practical, real-world scenarios and examples from the literature the different GIS methods and uses to improve community health and healthcare practices, e.g., for improving hospital bed availability, in community health and bioterrorism surveillance services, and in the latest SARS outbreak; (2) to discuss challenges and problems currently hindering the wide-scale adoption of GIS across the NHS; and (3) to identify the most important requirements and ingredients for addressing these challenges, and realising GIS potential within the NHS, guided by related initiatives worldwide. The ultimate goal is to illuminate the road towards implementing a comprehensive national, multi-agency spatio-temporal health information infrastructure functioning proactively in real time. The concepts and principles presented in this paper can be also applied in other countries, and on regional (e.g., European Union) and global levels.',\n",
       "               'score': 0.03177805617451668}),\n",
       "             ('aHR0cHM6Ly9ibG9ic3RvcmFnZW9xdWRlcnRlamZjcXcuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA4djMucGRm0_8',\n",
       "              {'title': 'Predicting the Expected Behavior of Agents that Learn About Agents: The CLRI Framework_chunk_8',\n",
       "               'name': '0001008v3.pdf',\n",
       "               'location': 'https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v3.pdf',\n",
       "               'caption': 'The sellers use a reinforcement learning algorithm with their reinforcements being the profit the agent achieved in each round, or 0 if it did not sell the good at the time.',\n",
       "               'index': 'cogsrch-index-files-vector',\n",
       "               'chunk': 'In each time step the sellers post a price and the buyer\\ndecides which of the sellers to buy from, namely, the one with the lowest bid.\\nThe sellers can bid any one of 20 prices in an effort to maximize their profits.\\nThe sellers use a reinforcement learning algorithm with their reinforcements\\nbeing the profit the agent achieved in each round, or 0 if it did not sell the good\\nat the time. In this system we had one good being sold (|W | = 1).As predicted\\nby economic theory, the price in this system settles to the sellers’ marginal cost,\\nbut it takes time to get there due to the learning inefficiencies.\\n\\nWe experimented with different αj rates2 for the reinforcement learning of\\nagent j, while keeping αi = .1 fixed, and plotted the running average of the\\nerror of agent i. A comparison is shown in Figure 9. Figure 9(a) gives the\\nexperimental results for three different values of αj . It shows i’s average error,\\nover 100 runs, as a function of time. Since both sellers start with no knowledge,\\ntheir initial actions are completely random which makes their error equal to .5.\\n\\n2\\nα is the relative weight the algorithm gives to the most recent payoff. α = 1 means that\\n\\nit will forget all previous experience and use only the latest payoff to determine what action\\nto take.\\n\\n17\\n\\n\\n\\nThen, depending on αj , i’s error will either start to go down from there or will\\nfirst go up some and then down. Eventually, i’s error gets very close to 0, as\\nthe system reaches a market equilibrium.\\n\\nWe can predict this behavior using Eq. (27). Based on the game description,\\nwe set |Ai| = |Aj | = 20, since there were 20 possible actions. We let ri =\\nrj = 1 because reinforcement learning with fixed payoffs enforces the condition\\nthat once an agent is taking the correct action it will never change its decision\\nfunction to take a different action. The agent might, however, still take a wrong\\naction but only when its exploration rate dictates it.\\n\\nWe then let Iij = Iji = .17 based on the rough calculation that each agent\\nhas an equal probability of bidding any one of the 20 prices. If ∆t\\n\\ni = 20 then\\nIji for this situation is the probability that j was also bidding 20 or above,\\ni.e., 1/20, times the probability that j’s new price is lower than 20, i.e. 19/20.\\nSimilarly, if ∆t\\n\\ni = 19 then Iji is equal to 2/20 times 18/20. The average of all\\nof these probabilities is .17. A more precise calculation of the impact would\\nrequire us to find it via experimentation by actually running the system.\\n\\nFinally, we chose li = lj = ci = cj = .005 for the first curve (i.e., the one that\\ncompares with αj = .1). We knew that for such a low αj the learning and change\\nrate should be the same. The actual value was chosen via experimentation. The\\nresulting curve is shown in Figure 9(b). At this moment, we do not possess a\\nformal way of deriving learning and change rates from α-rates.\\n\\nFor the second curve (αj = .3) we knew that, since only αj had changed\\nfrom the first experiment, we should only change lj and cj . In fact, these\\ntwo values should only be increased. We found their exact values, again by\\nexperimentation, to be lj = .04, cj = .4. For the third curve we found the\\nvalues to be lj = .055, cj = .8.\\n\\nOne difference we notice between the experimental and the theoretical results\\nis that the experimental results show a longer delay before the error starts to\\ndecrease. We attribute this delay to the agent’s initially high exploration rate.\\nThat is, the agents initially start by taking all random actions but progressively\\nreduce this rate of exploration. As the exploration rate decreases the discrepancy\\nbetween our theoretical predictions and experimental results is reduced.\\n\\nIn summary, while it is true that we found lj and cj by experimentation,\\nall the other values were calculated from the description of the problem. Even\\nthe relative values of lj and cj follow the intuitive relation with αj that, as\\nαj increases so does lj and (even more) cj . Section 9 shows how to calculate\\nlower bounds on the learning rate. We believe that this experiment provides\\nsolid evidence that our theory can be used to approximately determine the\\nquantitative behaviors of MASs with learning agents.\\n\\n18\\n\\n\\n\\n0.5\\n\\n0.55\\n\\n0.6\\n\\n0.65\\n\\n0.7\\n\\n0.75\\n\\n0.8\\n\\n0.85\\n\\n0.9\\n\\n0.95\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\nP\\nro\\n\\nba\\nbi\\n\\nlit\\ny \\n\\nof\\n c\\n\\nho\\nos\\n\\nin\\ng \\n\\nan\\n o\\n\\npt\\nim\\n\\nal\\n jo\\n\\nin\\nt a\\n\\nct\\nio\\n\\nn\\n\\nNumber of Interactions\\n\\nChoosing optimal joint actions\\n\\nIndependent learners\\nJoint action learners\\n\\n(a) Experiment\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\ntime\\n\\n1\\n-\\ne\\nr\\nr\\no\\nr\\n\\nli = .1\\n\\n(b) Theory\\n\\nFigure 10: Comparing theory (b) with results from [6] (a).\\n\\n8 Application of our Theory to Experiments in\\n\\nthe Literature\\n\\nIn this section we show how we can apply our theory to experimental results\\nfound in the AI and MAS literature. While we will often not be able to com-\\npletely reproduce the authors’ results exactly, we believe that being able to\\n',\n",
       "               'score': 0.0317540317773819})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment the below line if you want to inspect the ordered results\n",
    "ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2a3595-c3b7-4376-b9c5-0db7a42b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "top_docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location+os.environ['BLOB_SAS_TOKEN']}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(top_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f1cf00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of elements in top_docs: 5. Here they are:\n",
      "[Document(page_content='predictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [1995] has studied reinforcement learning in multi-robot domains.\\nShe notes, for example, how learning can give rise to social behaviors (Matarić\\n[1997]). The work shows how robots can be individually programmed to pro-\\nduce certain group behaviors. It represents a good example of the usefulness\\nand flexibility of learning agents in multi-agent domains. However, the author\\ndoes not offer a mathematical justification for the chosen individual learning\\nalgorithms, nor does she explain why the agents were able to converge to the\\nglobal behaviors. Our research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[1997]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents learn-\\ning about agents (Sen [1996], Weiß [1997]). For example, Sen and Sekaran [1998]\\nshow how learning agents in simple MAS converge to system-wide optimal be-\\nhavior. Their agents use Q-learning or modified classifier systems in order to\\nlearn. The authors implement these agents and compare the performance of the\\ndifferent learning algorithms for developing agent coordination. Hu and Well-\\nman [1998b, 1996] have studied reinforcement learning in market-base MASs,\\nshowing how certain initial learning biases can be self-fulfilling, and how learn-\\ning can be useful but is affected by an agent’s models of other agents. Claus\\nand Boutilier [1997] have also carried out experimental studies of the behavior of\\nreinforcement learning agents. We have been able to use the CLRI framework\\nto predict some of their experimental results Vidal [1998]. Other researchers\\nsuch as Stone and Veloso [1999], Littman [1994], and Hu and Wellman [1998a]\\nhave extended the basic Q-learning Watkins and Dayan [1992] algorithm for use\\nwith MASs in an effort to either improve or prove convergence to the optimal\\nbehavior.\\n\\nWe have also successfully experimented with reinforcement learning simu-\\nlations (Vidal and Durfee [1998]), but we believe that the formal treatment\\nelucidated in these pages will shed more light into the real nature of the prob-\\nlem and the relative importance of the various parameters that describe the\\n\\n25\\n\\n\\n\\ncapabilities of an agent’s learning algorithm.\\n\\n11 Summary\\n\\nWe have presented a framework for studying and predicting the behavior of\\nMASs composed of learning agents. We believe that this framework captures the\\nmost important parameters that describe an agents’ learning and the system’s\\nrules of encounter. Various comparisons between the framework’s predictions\\nand experimental results were given. These comparisons showed that the theo-\\nretical predictions closely match our experimental results and the experimental\\nresults published by others. Our success in reproducing these results allows us\\nto confidently state the effectiveness and accuracy of our theory in predicting\\nthe expected error of machine learning agents in MASs.\\n\\nSince our theory describes an agent’s behavior at a high-level (i.e., the agent’s\\nerror), it is not capable of making system-specific predictions (e.g., predicting\\nthe particular actions that are favored). These types of system-specific predic-\\ntions can only be arrived at by the traditional method of implementing popula-\\ntions of such agents and testing their behaviors. However, we expect that there\\nwill be times when the predictions from our theory will be enough to answer a\\ndesigner’s questions. A MAS designer that only needs to determine how “good”\\nthe agent’s behavior will be could probably use the CLRI framework. A de-\\nsigner that needs to know which particular emergent behaviors will be favored\\nby his agents will need to implement the agents.\\n\\nFinally, while we have given some examples as to how learning rates can be\\ndetermined for particular machine learning implementations, we do not have\\nany general method for determining these rates. However, we showed how to\\nuse the sample complexity of a learning problem to determine a lower bound on\\nthe learning rate of a consistent learning agent. This bound is useful for quickly\\n', metadata={'source': 'https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D'}), Document(page_content='Although efficient methods exist to assemble synthetic oligonucleotides into genes and genomes, these suffer from the presence of 1–3 random errors/kb of DNA. Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population. The DNA is fragmented, and mismatched fragments are removed upon binding to an immobilized mismatch binding protein (MutS). PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population. We show that two iterations of consensus shuffling improved a population of synthetic green fluorescent protein (GFPuv) clones from ∼60 to >90% fluorescent, and decreased errors 3.5- to 4.3-fold to final values of ∼1 error per 3500 bp. In addition, two iterations of consensus shuffling corrected a population of GFPuv clones where all members were non-functional, to a population where 82% of clones were fluorescent. Consensus shuffling should facilitate the rapid and accurate synthesis of long DNA sequences.', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D'}), Document(page_content='BACKGROUND: Sequence similarity searching is a powerful tool to help develop hypotheses in the quest to assign functional, structural and evolutionary information to DNA and protein sequences. As sequence databases continue to grow exponentially, it becomes increasingly important to repeat searches at frequent intervals, and similarity searches retrieve larger and larger sets of results. New and potentially significant results may be buried in a long list of previously obtained sequence hits from past searches. RESULTS: ReHAB (Recent Hits Acquired from BLAST) is a tool for finding new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with two versions of a protein sequence database and highlights hits that are present only in the updated database. Results are presented in an easily comprehended table, or in a BLAST-like report, using colors to highlight the new hits. ReHAB is designed to handle large numbers of query sequences, such as whole genomes or sets of genomes. Advanced computer skills are not needed to use ReHAB; the graphics interface is simple to use and was designed with the bench biologist in mind. CONCLUSIONS: This software greatly simplifies the problem of evaluating the output of large numbers of protein database searches.', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D'}), Document(page_content='The term \"Geographic Information Systems\" (GIS) has been added to MeSH in 2003, a step reflecting the importance and growing use of GIS in health and healthcare research and practices. GIS have much more to offer than the obvious digital cartography (map) functions. From a community health perspective, GIS could potentially act as powerful evidence-based practice tools for early problem detection and solving. When properly used, GIS can: inform and educate (professionals and the public); empower decision-making at all levels; help in planning and tweaking clinically and cost-effective actions, in predicting outcomes before making any financial commitments and ascribing priorities in a climate of finite resources; change practices; and continually monitor and analyse changes, as well as sentinel events. Yet despite all these potentials for GIS, they remain under-utilised in the UK National Health Service (NHS). This paper has the following objectives: (1) to illustrate with practical, real-world scenarios and examples from the literature the different GIS methods and uses to improve community health and healthcare practices, e.g., for improving hospital bed availability, in community health and bioterrorism surveillance services, and in the latest SARS outbreak; (2) to discuss challenges and problems currently hindering the wide-scale adoption of GIS across the NHS; and (3) to identify the most important requirements and ingredients for addressing these challenges, and realising GIS potential within the NHS, guided by related initiatives worldwide. The ultimate goal is to illuminate the road towards implementing a comprehensive national, multi-agency spatio-temporal health information infrastructure functioning proactively in real time. The concepts and principles presented in this paper can be also applied in other countries, and on regional (e.g., European Union) and global levels.', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D'}), Document(page_content='In each time step the sellers post a price and the buyer\\ndecides which of the sellers to buy from, namely, the one with the lowest bid.\\nThe sellers can bid any one of 20 prices in an effort to maximize their profits.\\nThe sellers use a reinforcement learning algorithm with their reinforcements\\nbeing the profit the agent achieved in each round, or 0 if it did not sell the good\\nat the time. In this system we had one good being sold (|W | = 1).As predicted\\nby economic theory, the price in this system settles to the sellers’ marginal cost,\\nbut it takes time to get there due to the learning inefficiencies.\\n\\nWe experimented with different αj rates2 for the reinforcement learning of\\nagent j, while keeping αi = .1 fixed, and plotted the running average of the\\nerror of agent i. A comparison is shown in Figure 9. Figure 9(a) gives the\\nexperimental results for three different values of αj . It shows i’s average error,\\nover 100 runs, as a function of time. Since both sellers start with no knowledge,\\ntheir initial actions are completely random which makes their error equal to .5.\\n\\n2\\nα is the relative weight the algorithm gives to the most recent payoff. α = 1 means that\\n\\nit will forget all previous experience and use only the latest payoff to determine what action\\nto take.\\n\\n17\\n\\n\\n\\nThen, depending on αj , i’s error will either start to go down from there or will\\nfirst go up some and then down. Eventually, i’s error gets very close to 0, as\\nthe system reaches a market equilibrium.\\n\\nWe can predict this behavior using Eq. (27). Based on the game description,\\nwe set |Ai| = |Aj | = 20, since there were 20 possible actions. We let ri =\\nrj = 1 because reinforcement learning with fixed payoffs enforces the condition\\nthat once an agent is taking the correct action it will never change its decision\\nfunction to take a different action. The agent might, however, still take a wrong\\naction but only when its exploration rate dictates it.\\n\\nWe then let Iij = Iji = .17 based on the rough calculation that each agent\\nhas an equal probability of bidding any one of the 20 prices. If ∆t\\n\\ni = 20 then\\nIji for this situation is the probability that j was also bidding 20 or above,\\ni.e., 1/20, times the probability that j’s new price is lower than 20, i.e. 19/20.\\nSimilarly, if ∆t\\n\\ni = 19 then Iji is equal to 2/20 times 18/20. The average of all\\nof these probabilities is .17. A more precise calculation of the impact would\\nrequire us to find it via experimentation by actually running the system.\\n\\nFinally, we chose li = lj = ci = cj = .005 for the first curve (i.e., the one that\\ncompares with αj = .1). We knew that for such a low αj the learning and change\\nrate should be the same. The actual value was chosen via experimentation. The\\nresulting curve is shown in Figure 9(b). At this moment, we do not possess a\\nformal way of deriving learning and change rates from α-rates.\\n\\nFor the second curve (αj = .3) we knew that, since only αj had changed\\nfrom the first experiment, we should only change lj and cj . In fact, these\\ntwo values should only be increased. We found their exact values, again by\\nexperimentation, to be lj = .04, cj = .4. For the third curve we found the\\nvalues to be lj = .055, cj = .8.\\n\\nOne difference we notice between the experimental and the theoretical results\\nis that the experimental results show a longer delay before the error starts to\\ndecrease. We attribute this delay to the agent’s initially high exploration rate.\\nThat is, the agents initially start by taking all random actions but progressively\\nreduce this rate of exploration. As the exploration rate decreases the discrepancy\\nbetween our theoretical predictions and experimental results is reduced.\\n\\nIn summary, while it is true that we found lj and cj by experimentation,\\nall the other values were calculated from the description of the problem. Even\\nthe relative values of lj and cj follow the intuitive relation with αj that, as\\nαj increases so does lj and (even more) cj . Section 9 shows how to calculate\\nlower bounds on the learning rate. We believe that this experiment provides\\nsolid evidence that our theory can be used to approximately determine the\\nquantitative behaviors of MASs with learning agents.\\n\\n18\\n\\n\\n\\n0.5\\n\\n0.55\\n\\n0.6\\n\\n0.65\\n\\n0.7\\n\\n0.75\\n\\n0.8\\n\\n0.85\\n\\n0.9\\n\\n0.95\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\nP\\nro\\n\\nba\\nbi\\n\\nlit\\ny \\n\\nof\\n c\\n\\nho\\nos\\n\\nin\\ng \\n\\nan\\n o\\n\\npt\\nim\\n\\nal\\n jo\\n\\nin\\nt a\\n\\nct\\nio\\n\\nn\\n\\nNumber of Interactions\\n\\nChoosing optimal joint actions\\n\\nIndependent learners\\nJoint action learners\\n\\n(a) Experiment\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\ntime\\n\\n1\\n-\\ne\\nr\\nr\\no\\nr\\n\\nli = .1\\n\\n(b) Theory\\n\\nFigure 10: Comparing theory (b) with results from [6] (a).\\n\\n8 Application of our Theory to Experiments in\\n\\nthe Literature\\n\\nIn this section we show how we can apply our theory to experimental results\\nfound in the AI and MAS literature. While we will often not be able to com-\\npletely reproduce the authors’ results exactly, we believe that being able to\\n', metadata={'source': 'https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D'})]\n"
     ]
    }
   ],
   "source": [
    "# Comment / Uncomment the below line if you want to inspect the ordered top_docs\n",
    "print(f\"Nr. of elements in top_docs: {len(top_docs)}. Here they are:\\n{top_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c26d7540-feb8-4581-849e-003f4bf2a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt token count: 2464\n",
      "Max Completion Token count: 1000\n",
      "Combined docs (context) token count: 3132\n",
      "--------\n",
      "Requested token count: 6596\n",
      "Token limit for gpt-35-turbo-16k : 16384\n",
      "Chain Type selected: stuff\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(top_docs)>0):\n",
    "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    prompt_tokens = num_tokens_from_string(COMBINE_CHAT_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
    "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
    "    \n",
    "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
    "    \n",
    "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
    "    \n",
    "    print(\"System prompt token count:\",prompt_tokens)\n",
    "    print(\"Max Completion Token count:\", COMPLETION_TOKENS)\n",
    "    print(\"Combined docs (context) token count:\",context_tokens)\n",
    "    print(\"--------\")\n",
    "    print(\"Requested token count:\",requested_tokens)\n",
    "    print(\"Token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Chain Type selected:\", chain_type)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2513dd",
   "metadata": {},
   "source": [
    "## Using load_qa_with_sources_chain\n",
    "The *get_answer* function in the next cell is equivalent to the next code, in case **chain_type = map_reduce**:\n",
    "\n",
    "```\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from common.prompts import (COMBINE_PROMPT, COMBINE_QUESTION_PROMPT)\n",
    "\n",
    "llm35 = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
    "chain = load_qa_with_sources_chain(llm35, chain_type=\"map_reduce\",\n",
    "                                   combine_prompt = COMBINE_PROMPT,\n",
    "                                   question_prompt = COMBINE_QUESTION_PROMPT,\n",
    "                                   return_intermediate_steps = True)\n",
    "\n",
    "answer = chain( {\"input_documents\": top_docs, \"question\": QUESTION, \"language\": \"it\"}, return_only_outputs=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce6efa9-2b8f-4810-904d-5986b4ae0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Some use cases for reinforcement learning include:\n",
       "\n",
       "1. Multi-robot domains: Reinforcement learning can be used to study social behaviors and program robots to produce certain group behaviors in multi-robot domains. This can help improve coordination and cooperation among robots. (Source: [1])\n",
       "\n",
       "2. Model-based learning: Agents can build models of other agents through observations and learn effective models based on finite state machines. This can be useful for understanding the behaviors and actions of other agents in a system. (Source: [1])\n",
       "\n",
       "3. Agent coordination in multi-agent systems: Reinforcement learning can be used to develop learning agents that converge to system-wide optimal behavior. This can help improve coordination and cooperation among agents in complex systems. (Source: [1])\n",
       "\n",
       "4. Synthesis of synthetic DNA: Reinforcement learning can be used to reduce random errors in synthetic DNA synthesis. By using consensus shuffling, errors in synthetic DNA can be identified and removed, leading to the rapid and accurate synthesis of long DNA sequences. (Source: [2])\n",
       "\n",
       "5. Protein database searching: Reinforcement learning can be applied to improve the efficiency and accuracy of protein database searches. For example, the ReHAB tool uses reinforcement learning to find new protein hits in repeated PSI-BLAST searches, allowing for the identification of potentially significant results buried in a long list of previous hits. (Source: [3])\n",
       "\n",
       "6. Geographic Information Systems (GIS) in healthcare: Reinforcement learning can be used to improve community health and healthcare practices through GIS. GIS can inform and educate, empower decision-making, help in planning and prioritizing actions, and monitor and analyze changes in health and healthcare. (Source: [4])\n",
       "\n",
       "Please note that the references provided are numerical references to the sources mentioned in the extracted content."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 ms, sys: 3.1 ms, total: 48.1 ms\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get the answer\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27501f1b-7db0-4ee3-9cb1-e609254ffa3d",
   "metadata": {},
   "source": [
    "And if we ask the follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf5b323-3b9c-479b-8502-acfc4f7915dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I couldn't find any extracted parts that provide the main points of our conversation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fa6e6-226c-400f-a504-30255385f43b",
   "metadata": {},
   "source": [
    "You might get a different response from above, but it doesn't matter what response you get, it will be based on the context given, not on previous answers.\n",
    "\n",
    "Until now we just have the same as the prior Notebook 03: results from Azure Search enhanced by OpenAI model, with no memory\n",
    "\n",
    "**Now let's add memory to it:**\n",
    "\n",
    "Reference: https://python.langchain.com/docs/modules/memory/how_to/adding_memory_chain_multiple_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98b876e-d264-48ae-b5ed-9801d6a9152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has several use cases across different domains. Here are some examples that have been mentioned in the extracted parts:\n",
       "\n",
       "1. **Multi-robot domains**: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "\n",
       "2. **Model-based learning**: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "\n",
       "3. **Learning about agents in multi-agent systems**: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "\n",
       "4. **Synthetic DNA synthesis**: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\n",
       "\n",
       "5. **Finding new protein hits**: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\n",
       "\n",
       "6. **Geographic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\")\n",
    "\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf28927b-d9ee-4412-bb07-13e055e832a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the main points of our conversation:\n",
       "\n",
       "1. Reinforcement learning has several use cases across different domains.\n",
       "   - Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "   - Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "   - Learning about agents in multi-agent systems: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "\n",
       "2. Synthetic DNA synthesis: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\n",
       "\n",
       "3. Finding new protein hits: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\n",
       "\n",
       "4. Geographic Information Systems (GIS): GIS has applications in health and"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a61782a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning\\nAI: Reinforcement learning has several use cases across different domains. Here are some examples that have been mentioned in the extracted parts:\\n\\n1. **Multi-robot domains**: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n2. **Model-based learning**: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n3. **Learning about agents in multi-agent systems**: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n4. **Synthetic DNA synthesis**: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\\n\\n5. **Finding new protein hits**: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\\n\\n6. **Geographic\\nHuman: Give me the main points of our conversation\\nAI: Here are the main points of our conversation:\\n\\n1. Reinforcement learning has several use cases across different domains.\\n   - Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n   - Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n   - Learning about agents in multi-agent systems: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n2. Synthetic DNA synthesis: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\\n\\n3. Finding new protein hits: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\\n\\n4. Geographic Information Systems (GIS): GIS has applications in health and'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3830b0b8-0ca2-4d0a-9747-f6273368002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the main points of our conversation:\n",
       "\n",
       "1. Reinforcement learning has several use cases across different domains.\n",
       "   - Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "   - Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "   - Learning about agents in multi-agent systems: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\n",
       "\n",
       "2. Synthetic DNA synthesis: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\n",
       "\n",
       "3. Finding new protein hits: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\n",
       "\n",
       "4. Geographic Information Systems (GIS): GIS has applications in health and"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e732b-3c8c-4df3-8fcb-c3d01e7bec74",
   "metadata": {},
   "source": [
    "You might get a different answer on the above cell, and it is ok, this bot is not yet well configured to answer any question that is not related to its knowledge base, including salutations.\n",
    "\n",
    "Let's check our memory to see that it's keeping the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1279692c-7eb0-4300-8a66-c7025f02c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning\\nAI: Reinforcement learning has several use cases across different domains. Here are some examples that have been mentioned in the extracted parts:\\n\\n1. **Multi-robot domains**: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n2. **Model-based learning**: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n3. **Learning about agents in multi-agent systems**: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n4. **Synthetic DNA synthesis**: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\\n\\n5. **Finding new protein hits**: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\\n\\n6. **Geographic\\nHuman: Give me the main points of our conversation\\nAI: Here are the main points of our conversation:\\n\\n1. Reinforcement learning has several use cases across different domains.\\n   - Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n   - Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n   - Learning about agents in multi-agent systems: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n2. Synthetic DNA synthesis: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\\n\\n3. Finding new protein hits: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\\n\\n4. Geographic Information Systems (GIS): GIS has applications in health and\\nHuman: Thank you\\nAI: Here are the main points of our conversation:\\n\\n1. Reinforcement learning has several use cases across different domains.\\n   - Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n   - Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n   - Learning about agents in multi-agent systems: Experimental work has been done on learning agents in simple multi-agent systems. For example, learning agents have been shown to converge to system-wide optimal behavior, and different learning algorithms have been compared for developing agent coordination<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[1]</a></sup>.\\n\\n2. Synthetic DNA synthesis: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. A method called consensus shuffling has been introduced to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[2]</a></sup>.\\n\\n3. Finding new protein hits: A tool called ReHAB (Recent Hits Acquired from BLAST) has been developed to find new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with different versions of a protein sequence database and highlights hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\">[3]</a></sup>.\\n\\n4. Geographic Information Systems (GIS): GIS has applications in health and'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87405173",
   "metadata": {},
   "source": [
    "## Using CosmosDB as persistent memory\n",
    "\n",
    "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wisg to provide recommendations. \n",
    "\n",
    "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
    "We will use a class in LangChain use CosmosDBChatMessageHistory, see [HERE](https://python.langchain.com/en/latest/_modules/langchain/memory/chat_message_histories/cosmos_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7131daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CosmosDB instance from langchain cosmos class.\n",
    "cosmos = CosmosDBChatMessageHistory(\n",
    "    cosmos_endpoint   = os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "    cosmos_database   = os.environ['AZURE_COSMOSDB_NAME'],\n",
    "    cosmos_container  = os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "    connection_string = os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "    session_id        = \"Agent-Test-Session\" + str(random.randint(1, 1000)),\n",
    "    user_id           = \"Agent-Test-User\" + str(random.randint(1, 1000))\n",
    "    )\n",
    "\n",
    "# prepare the cosmosdb instance\n",
    "cosmos.prepare_cosmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Memory Object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\",chat_memory=cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ceb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has several use cases across different domains. Here are some examples:\n",
       "\n",
       "1. **Multi-robot domains**: Reinforcement learning can be used to study the behavior of multiple robots in a coordinated manner. For example, learning algorithms can be used to program robots to exhibit social behaviors and produce certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "\n",
       "2. **Model-based learning**: Agents can build models of other agents through observations. This approach involves using models based on finite state machines and learning them effectively by observing the actions of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "\n",
       "3. **Agent coordination**: Learning agents can be used to coordinate their actions and achieve system-wide optimal behavior. This can be done by using learning algorithms such as Q-learning or modified classifier systems<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "\n",
       "4. **Market-based multi-agent systems**: Reinforcement learning can be applied to market-based multi-agent systems, where agents learn to optimize their behavior in market-like environments. This can help in understanding the effects of learning biases and models of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "\n",
       "These are just a few examples, and reinforcement learning has applications in various other fields. Let me know if you need more information.\n",
       "\n",
       "References:\n",
       "[1]<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">source</a></sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing using our Question\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a5ff826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points:\n",
       "\n",
       "1. Reinforcement learning has several use cases across different domains, including:\n",
       "   - Multi-robot domains: Reinforcement learning can be used to study the behavior of multiple robots in a coordinated manner, such as programming them to exhibit social behaviors and produce certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "   - Model-based learning: Agents can build models of other agents through observations, using models based on finite state machines and learning them effectively by observing the actions of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "   - Agent coordination: Learning agents can be used to coordinate their actions and achieve system-wide optimal behavior, using learning algorithms such as Q-learning or modified classifier systems<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "   - Market-based multi-agent systems: Reinforcement learning can be applied to market-based multi-agent systems, where agents learn to optimize their behavior in market-like environments, helping understand the effects of learning biases and models of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "\n",
       "2. Consensus shuffling is a method used to significantly reduce random errors in synthetic DNA. It involves re-hybridization of the DNA population, fragmentation, and removal of mismatched fragments. PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">source</a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5ac98",
   "metadata": {},
   "source": [
    "Let's check our Azure CosmosDB to see the whole conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1d7688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me some use cases for reinforcement learning'),\n",
       " AIMessage(content='Reinforcement learning has several use cases across different domains. Here are some examples:\\n\\n1. **Multi-robot domains**: Reinforcement learning can be used to study the behavior of multiple robots in a coordinated manner. For example, learning algorithms can be used to program robots to exhibit social behaviors and produce certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n\\n2. **Model-based learning**: Agents can build models of other agents through observations. This approach involves using models based on finite state machines and learning them effectively by observing the actions of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n\\n3. **Agent coordination**: Learning agents can be used to coordinate their actions and achieve system-wide optimal behavior. This can be done by using learning algorithms such as Q-learning or modified classifier systems<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n\\n4. **Market-based multi-agent systems**: Reinforcement learning can be applied to market-based multi-agent systems, where agents learn to optimize their behavior in market-like environments. This can help in understanding the effects of learning biases and models of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n\\nThese are just a few examples, and reinforcement learning has applications in various other fields. Let me know if you need more information.\\n\\nReferences:\\n[1]<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">source</a></sup>'),\n",
       " HumanMessage(content='Give me the main points of our conversation'),\n",
       " AIMessage(content='Based on our conversation, here are the main points:\\n\\n1. Reinforcement learning has several use cases across different domains, including:\\n   - Multi-robot domains: Reinforcement learning can be used to study the behavior of multiple robots in a coordinated manner, such as programming them to exhibit social behaviors and produce certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n   - Model-based learning: Agents can build models of other agents through observations, using models based on finite state machines and learning them effectively by observing the actions of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n   - Agent coordination: Learning agents can be used to coordinate their actions and achieve system-wide optimal behavior, using learning algorithms such as Q-learning or modified classifier systems<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n   - Market-based multi-agent systems: Reinforcement learning can be applied to market-based multi-agent systems, where agents learn to optimize their behavior in market-like environments, helping understand the effects of learning biases and models of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n\\n2. Consensus shuffling is a method used to significantly reduce random errors in synthetic DNA. It involves re-hybridization of the DNA population, fragmentation, and removal of mismatched fragments. PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">source</a'),\n",
       " HumanMessage(content='Thank you'),\n",
       " AIMessage(content='Based on our conversation, here are the main points:\\n\\n1. Reinforcement learning has several use cases across different domains, including:\\n   - Multi-robot domains: Reinforcement learning can be used to study the behavior of multiple robots in a coordinated manner, such as programming them to exhibit social behaviors and produce certain group behaviors<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n   - Model-based learning: Agents can build models of other agents through observations, using models based on finite state machines and learning them effectively by observing the actions of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n   - Agent coordination: Learning agents can be used to coordinate their actions and achieve system-wide optimal behavior, using learning algorithms such as Q-learning or modified classifier systems<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n   - Market-based multi-agent systems: Reinforcement learning can be applied to market-based multi-agent systems, where agents learn to optimize their behavior in market-like environments, helping understand the effects of learning biases and models of other agents<sup><a href=\"https://blobstorageoqudertejfcqw.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">[1]</a></sup>.\\n\\n2. Consensus shuffling is a method used to significantly reduce random errors in synthetic DNA. It involves re-hybridization of the DNA population, fragmentation, and removal of mismatched fragments. PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-11-25T09%3A09%3A24Z&se=2030-11-26T09%3A09%3A00Z&sp=rl&sig=1zCNOg4UIcVHew2GngrqYs%2FyF1Nq%2BnvD5nPf6Ka3k%2B0%3D\" target=\"_blank\">source</a')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load message from cosmosdb\n",
    "cosmos.load_messages()\n",
    "cosmos.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
   "metadata": {},
   "source": [
    "![CosmosDB Memory](./images/cosmos-chathistory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
    "\n",
    "We added persitent memory using CosmosDB.\n",
    "\n",
    "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, regardless of the input and it struggles to respond to prompts like: Hello, Thank you, Bye, What's your name, What's the weather and any other task that is not search in the knowledge base.\n",
    "\n",
    "\n",
    "## <u>Important Note</u>:<br>\n",
    "As we proceed, while all the code will remain compatible with GPT-3.5 models, we highly recommend transitioning to GPT-4. Here's why:\n",
    "\n",
    "**GPT-3.5-Turbo** can be likened to a 7-year-old child. You can provide it with concise instructions, but it frequently struggles to follow them accurately. Additionally, its limited memory can make sustained conversations challenging.\n",
    "\n",
    "**GPT-3.5-Turbo-16k** resembles the same 7-year-old, but with an increased attention span for longer instructions. However, it still faces difficulties accurately executing them about half the time.\n",
    "\n",
    "**GPT-4** exhibits the capabilities of a 10-12-year-old child. It possesses enhanced reasoning skills and more consistently adheres to instructions. While its memory retention for instructions is moderate, it excels at following them.\n",
    "\n",
    "**GPT-4-32k** is akin to the 10-12-year-old child with an extended memory. It comprehends lengthy sets of instructions and engages in meaningful conversations. Thanks to its robust memory, it offers detailed responses.\n",
    "\n",
    "Understanding this analogy above will become clearer as you complete the final notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
    "\n",
    "But, does this solve all the possible scenarios that a virtual assistant will require?  **What about if the answer to the Smart Search Engine is not related to text, but instead requires to look into tabular data?** The next notebook explains and solves the tabular problem and the concept of Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivbd",
   "language": "python",
   "name": "openaivbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
