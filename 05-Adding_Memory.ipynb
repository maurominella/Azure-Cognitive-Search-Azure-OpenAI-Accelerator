{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
   "metadata": {},
   "source": [
    "# Understanding Memory in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
   "metadata": {},
   "source": [
    "In the previous Notebook, we successfully explored how OpenAI models can enhance the results from Azure Cognitive Search. \n",
    "\n",
    "However, we have yet to discover how to engage in a conversation with the LLM. With [Bing Chat](http://chat.bing.com/), for example, this is possible, as it can understand and reference the previous responses.\n",
    "\n",
    "There is a common misconception that GPT models have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
    "\n",
    "In this Notebook, our goal is to illustrate how we can effectively \"endow the LLM with memory\" by employing prompts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import CosmosDBChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import (\n",
    "    get_search_results,\n",
    "    update_vector_indexes,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    num_tokens_from_string,\n",
    "    get_answer,\n",
    ")\n",
    "\n",
    "from common.prompts import COMBINE_CHAT_PROMPT_TEMPLATE\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials_my.env\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
   "metadata": {},
   "source": [
    "### Let's start with the basics\n",
    "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Tell me some use cases for reinforcement learning\"\n",
    "FOLLOW_UP_QUESTION = \"Give me the main points of our conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = os.environ[\"COMPLETION3516_DEPLOYMENT\"]\n",
    "COMPLETION_TOKENS = 500\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Game playing: Reinforcement learning has been successfully applied to various games like chess, Go, and poker, where the agent learns to make optimal decisions by playing against itself or human opponents.\n",
       "\n",
       "2. Robotics: Reinforcement learning can be used to train robots to perform complex tasks such as grasping objects, locomotion, and navigation in dynamic environments.\n",
       "\n",
       "3. Autonomous vehicles: Reinforcement learning can be used to train self-driving cars to make decisions in real-time, such as lane changing, merging, and avoiding obstacles.\n",
       "\n",
       "4. Resource management: Reinforcement learning can be applied to optimize resource allocation and scheduling in various domains, such as energy management, traffic control, and supply chain management.\n",
       "\n",
       "5. Healthcare: Reinforcement learning can be used for personalized treatment recommendation, drug dosage optimization, and clinical decision support systems.\n",
       "\n",
       "6. Finance: Reinforcement learning can be applied to optimize trading strategies, portfolio management, and risk assessment in financial markets.\n",
       "\n",
       "7. Advertising and marketing: Reinforcement learning can be used to optimize online advertising campaigns, personalized recommendations, and customer targeting.\n",
       "\n",
       "8. Industrial control systems: Reinforcement learning can be applied to optimize control policies in manufacturing processes, power systems, and chemical plants.\n",
       "\n",
       "9. Natural language processing: Reinforcement learning can be used to train conversational agents and chatbots to generate appropriate responses and engage in meaningful conversations.\n",
       "\n",
       "10. Personalized education: Reinforcement learning can be applied to adaptive learning systems, where the agent learns to personalize the educational content and pacing based on the learner's progress and needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "response = chain.run(QUESTION)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. The conversation was about a specific topic, but it is not specified what that topic was.\\n2. The conversation likely involved multiple points or ideas, but those points are not mentioned.\\n3. No specific details or context of the conversation are provided.\\n4. The main points of the conversation are not provided or outlined.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "chain.run(FOLLOW_UP_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As you can see, it doesn't remember what it just responded, sometimes it responds based only on the system prompt, or just randomly. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "                {history}\n",
    "                Human: {question}\n",
    "                AI:\n",
    "            \"\"\"\n",
    "    )\n",
    "chain = LLMChain(llm=llm, prompt=hist_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Reinforcement learning can be used in various domains such as game playing, robotics, autonomous vehicles, resource management, healthcare, finance, advertising and marketing, industrial control systems, natural language processing, and personalized education.\n",
       "- In game playing, reinforcement learning allows agents to learn optimal decisions by playing against themselves or human opponents.\n",
       "- In robotics, reinforcement learning can train robots to perform complex tasks in dynamic environments.\n",
       "- In autonomous vehicles, reinforcement learning helps make real-time decisions for lane changing, merging, and obstacle avoidance.\n",
       "- In resource management, reinforcement learning optimizes resource allocation and scheduling in domains like energy management, traffic control, and supply chain management.\n",
       "- In healthcare, reinforcement learning aids in personalized treatment recommendation, drug dosage optimization, and clinical decision support systems.\n",
       "- In finance, reinforcement learning optimizes trading strategies, portfolio management, and risk assessment in financial markets.\n",
       "- In advertising and marketing, reinforcement learning optimizes online advertising campaigns, personalized recommendations, and customer targeting.\n",
       "- In industrial control systems, reinforcement learning optimizes control policies in manufacturing processes, power systems, and chemical plants.\n",
       "- In natural language processing, reinforcement learning trains conversational agents and chatbots to generate appropriate responses and engage in meaningful conversations.\n",
       "- In personalized education, reinforcement learning adapts educational content and pacing based on the learner's progress and needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain.run({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
   "metadata": {},
   "source": [
    "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
   "metadata": {},
   "source": [
    "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba257e86-fd90-4a51-a72d-27000913e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Memory adds tokens to the prompt, we would need a better model that allows more space on the prompt\n",
    "COMPLETION_TOKENS = 2000\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
    "embedder = AzureOpenAIEmbeddings(deployment=os.environ[\"EMBEDDING_DEPLOYMENT\"], chunk_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cogsrch-index-files-vector',\n",
       " 'cogsrch-index-csv-vector',\n",
       " 'cogsrch-index-books-vector']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index1_name = \"cogsrch-index-files\"\n",
    "index2_name = \"cogsrch-index-csv\"\n",
    "index3_name = \"cogsrch-index-books-vector\"\n",
    "text_indexes = [index1_name, index2_name]\n",
    "vector_indexes = [index+\"-vector\" for index in text_indexes] + [index3_name]\n",
    "vector_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b01852c2-6192-496c-adff-4270f9380469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 5\n",
      "CPU times: user 1.15 s, sys: 32.7 ms, total: 1.18 s\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Search in text-based indexes first and update vector indexes\n",
    "k=10 # Top k results per each text-based index\n",
    "ordered_results = get_search_results(QUESTION, text_indexes, k=k, reranker_threshold=1, vector_search=False)\n",
    "update_vector_indexes(ordered_search_results=ordered_results, embedder=embedder)\n",
    "\n",
    "# Search in all vector-based indexes available\n",
    "similarity_k = 5 # top results from multi-vector-index similarity search\n",
    "ordered_results = get_search_results(QUESTION, vector_indexes, k=k, vector_search=True,\n",
    "                                        similarity_k=similarity_k,\n",
    "                                        query_vector = embedder.embed_query(QUESTION))\n",
    "print(\"Number of results:\", len(ordered_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca500dd8-148c-4d8a-b58b-2df4c957459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA4djIucGRm0_12',\n",
       "              {'title': 'arXiv:cs/0001008v2  [cs.MA]  17 Jan 2000_chunk_12',\n",
       "               'name': '0001008v2.pdf',\n",
       "               'location': 'https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf',\n",
       "               'caption': 'Matarić [1995] has studied reinforcement learning in multi-robot domains. She notes, for example, how learning can give rise to social behaviors (Matarić [1997]). The work shows how robots can be individually programmed to pro- duce certain group behaviors.',\n",
       "               'index': 'cogsrch-index-files-vector',\n",
       "               'chunk': 'predictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [1995] has studied reinforcement learning in multi-robot domains.\\nShe notes, for example, how learning can give rise to social behaviors (Matarić\\n[1997]). The work shows how robots can be individually programmed to pro-\\nduce certain group behaviors. It represents a good example of the usefulness\\nand flexibility of learning agents in multi-agent domains. However, the author\\ndoes not offer a mathematical justification for the chosen individual learning\\nalgorithms, nor does she explain why the agents were able to converge to the\\nglobal behaviors. Our research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[1997]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents learn-\\ning about agents (Sen [1996], Weiß [1997]). For example, Sen and Sekaran [1998]\\nshow how learning agents in simple MAS converge to system-wide optimal be-\\nhavior. Their agents use Q-learning or modified classifier systems in order to\\nlearn. The authors implement these agents and compare the performance of the\\ndifferent learning algorithms for developing agent coordination. Hu and Well-\\nman [1998b, 1996] have studied reinforcement learning in market-base MASs,\\nshowing how certain initial learning biases can be self-fulfilling, and how learn-\\ning can be useful but is affected by an agent’s models of other agents. Claus\\nand Boutilier [1997] have also carried out experimental studies of the behavior of\\nreinforcement learning agents. We have been able to use the CLRI framework\\nto predict some of their experimental results Vidal [1998]. Other researchers\\nsuch as Stone and Veloso [1999], Littman [1994], and Hu and Wellman [1998a]\\nhave extended the basic Q-learning Watkins and Dayan [1992] algorithm for use\\nwith MASs in an effort to either improve or prove convergence to the optimal\\nbehavior.\\n\\nWe have also successfully experimented with reinforcement learning simu-\\nlations (Vidal and Durfee [1998]), but we believe that the formal treatment\\nelucidated in these pages will shed more light into the real nature of the prob-\\nlem and the relative importance of the various parameters that describe the\\n\\n25\\n\\n\\n\\ncapabilities of an agent’s learning algorithm.\\n\\n11 Summary\\n\\nWe have presented a framework for studying and predicting the behavior of\\nMASs composed of learning agents. We believe that this framework captures the\\nmost important parameters that describe an agents’ learning and the system’s\\nrules of encounter. Various comparisons between the framework’s predictions\\nand experimental results were given. These comparisons showed that the theo-\\nretical predictions closely match our experimental results and the experimental\\nresults published by others. Our success in reproducing these results allows us\\nto confidently state the effectiveness and accuracy of our theory in predicting\\nthe expected error of machine learning agents in MASs.\\n\\nSince our theory describes an agent’s behavior at a high-level (i.e., the agent’s\\nerror), it is not capable of making system-specific predictions (e.g., predicting\\nthe particular actions that are favored). These types of system-specific predic-\\ntions can only be arrived at by the traditional method of implementing popula-\\ntions of such agents and testing their behaviors. However, we expect that there\\nwill be times when the predictions from our theory will be enough to answer a\\ndesigner’s questions. A MAS designer that only needs to determine how “good”\\nthe agent’s behavior will be could probably use the CLRI framework. A de-\\nsigner that needs to know which particular emergent behaviors will be favored\\nby his agents will need to implement the agents.\\n\\nFinally, while we have given some examples as to how learning rates can be\\ndetermined for particular machine learning implementations, we do not have\\nany general method for determining these rates. However, we showed how to\\nuse the sample complexity of a learning problem to determine a lower bound on\\nthe learning rate of a consistent learning agent. This bound is useful for quickly\\n',\n",
       "               'score': 0.03253968432545662}),\n",
       "             ('snqdma0s_0',\n",
       "              {'title': 'Correcting errors in synthetic DNA through consensus shuffling_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/',\n",
       "               'caption': 'Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'Although efficient methods exist to assemble synthetic oligonucleotides into genes and genomes, these suffer from the presence of 1–3 random errors/kb of DNA. Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population. The DNA is fragmented, and mismatched fragments are removed upon binding to an immobilized mismatch binding protein (MutS). PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population. We show that two iterations of consensus shuffling improved a population of synthetic green fluorescent protein (GFPuv) clones from ∼60 to >90% fluorescent, and decreased errors 3.5- to 4.3-fold to final values of ∼1 error per 3500 bp. In addition, two iterations of consensus shuffling corrected a population of GFPuv clones where all members were non-functional, to a population where 82% of clones were fluorescent. Consensus shuffling should facilitate the rapid and accurate synthesis of long DNA sequences.',\n",
       "               'score': 0.03201844170689583}),\n",
       "             ('aHR0cHM6Ly9ibG9ic3RvcmFnZXkyNGJpNTc3anN6ZjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA4djMucGRm0_12',\n",
       "              {'title': 'Predicting the Expected Behavior of Agents that Learn About Agents: The CLRI Framework_chunk_12',\n",
       "               'name': '0001008v3.pdf',\n",
       "               'location': 'https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf',\n",
       "               'caption': 'We are targeting three specific constraints. 1. The values of ci, li, ri, and Iij cannot, in all situations, be mathematically determined from the system’s description. We have found that bounds for the ci, li, and ri values can often be determined when using reinforcement learning or supervised learning.',\n",
       "               'index': 'cogsrch-index-files-vector',\n",
       "               'chunk': 'partially relevant to our work. Our emphasis is on finding ways to predict the\\nbehavior of MASs composed of machine-learning agents. We are only concerned\\nwith the behavior of simpler artificial programmable agents, rather than the\\ncomplex behavior of humans or the unpredictable behavior of animals.\\n\\nThe dynamics of MASs have also been studied by Kephart et. al. [18]. In\\nthis work the authors show how simple predictive agents can lead to globally\\ncyclic or chaotic behaviors. As the authors explain, the chaotic behaviors were a\\nresult of the simple predictive strategies used by the agents. Unlike our agents,\\nmost of their agents are not engaged in learning, instead they use simple fixed\\npredictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [22] has studied reinforcement learning in multi-robot domains. She\\nnotes, for example, how learning can give rise to social behaviors [23]. The\\nwork shows how robots can be individually programmed to produce certain\\ngroup behaviors. It represents a good example of the usefulness and flexibility\\nof learning agents in multi-agent domains. However, the author does not offer\\na mathematical justification for the chosen individual learning algorithms, nor\\ndoes she explain why the agents were able to converge to the global behaviors.\\nOur research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[4]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents\\nlearning about agents [27, 36]. For example, Sen and Sekaran [28] show how\\nlearning agents in simple MAS converge to system-wide optimal behavior. Their\\nagents use Q-learning or modified classifier systems in order to learn. The\\nauthors implement these agents and compare the performance of the different\\nlearning algorithms for developing agent coordination. Hu and Wellman [14, 12]\\nhave studied reinforcement learning in market-base MASs, showing how certain\\ninitial learning biases can be self-fulfilling, and how learning can be useful but\\nis affected by an agent’s models of other agents. Claus and Boutilier [6] have\\nalso carried out experimental studies of the behavior of reinforcement learning\\nagents. We have been able to use the CLRI framework to predict some of their\\nexperimental results [33]. Other researchers [32, 20, 13] have extended the basic\\nQ-learning [35] algorithm for use with MASs in an effort to either improve or\\n\\n25\\n\\n\\n\\nprove convergence to the optimal behavior.\\nWe have also successfully experimented with reinforcement learning simula-\\n\\ntions [34], but we believe that the formal treatment elucidated in these pages\\nwill shed more light into the real nature of the problem and the relative im-\\nportance of the various parameters that describe the capabilities of an agent’s\\nlearning algorithm.\\n\\n11 Limitations and Future Work\\n\\nThe CLRI framework places some constraints on the type of systems it can\\nmodel, which limits its usability. However, it is important to understand that,\\nas we remove the limitations from the CLRI framework, the dynamics of the\\nsystem become much harder to predict. In the extreme, without any limitations\\non the agents’ abilities, the system becomes a complex adaptive system, as\\nstudied by Holland [11] and others in the field of complexity. The dynamic\\nbehavior of these systems continues to be studied by complexity researchers\\nwith only modest progress. It is only by placing limitations on the system that\\nwe were able to predict the expected error of agents in the systems modeled by\\nthe CLRI framework.\\n\\nOur ongoing work involves the relaxation of some of the constraints made by\\nthe CLRI framework so that it may become more easily and widely applicable,\\nwithout making the system dynamics impossible to analyze. We are targeting\\nthree specific constraints.\\n\\n1. The values of ci, li, ri, and Iij cannot, in all situations, be mathematically\\ndetermined from the system’s description. We have found that bounds for\\nthe ci, li, and ri values can often be determined when using reinforcement\\nlearning or supervised learning. However, the bounds are often very loose.\\n',\n",
       "               'score': 0.03131881356239319}),\n",
       "             ('ri5v6u4x_0',\n",
       "              {'title': 'Towards evidence-based, GIS-driven national spatial health information infrastructure and surveillance services in the United Kingdom_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/',\n",
       "               'caption': 'this paper has the following objectives: (1) to illustrate with practical, real-world scenarios and examples from the literature the different gis methods and uses to improve community health and healthcare practices, e.g., for improving hospital bed availability, in community health and bioterrorism surveillance services, and in the latest sars …',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'The term \"Geographic Information Systems\" (GIS) has been added to MeSH in 2003, a step reflecting the importance and growing use of GIS in health and healthcare research and practices. GIS have much more to offer than the obvious digital cartography (map) functions. From a community health perspective, GIS could potentially act as powerful evidence-based practice tools for early problem detection and solving. When properly used, GIS can: inform and educate (professionals and the public); empower decision-making at all levels; help in planning and tweaking clinically and cost-effective actions, in predicting outcomes before making any financial commitments and ascribing priorities in a climate of finite resources; change practices; and continually monitor and analyse changes, as well as sentinel events. Yet despite all these potentials for GIS, they remain under-utilised in the UK National Health Service (NHS). This paper has the following objectives: (1) to illustrate with practical, real-world scenarios and examples from the literature the different GIS methods and uses to improve community health and healthcare practices, e.g., for improving hospital bed availability, in community health and bioterrorism surveillance services, and in the latest SARS outbreak; (2) to discuss challenges and problems currently hindering the wide-scale adoption of GIS across the NHS; and (3) to identify the most important requirements and ingredients for addressing these challenges, and realising GIS potential within the NHS, guided by related initiatives worldwide. The ultimate goal is to illuminate the road towards implementing a comprehensive national, multi-agency spatio-temporal health information infrastructure functioning proactively in real time. The concepts and principles presented in this paper can be also applied in other countries, and on regional (e.g., European Union) and global levels.',\n",
       "               'score': 0.03131881356239319}),\n",
       "             ('q1y8uscj_0',\n",
       "              {'title': 'Recent Hits Acquired by BLAST (ReHAB): A tool to identify new hits in sequence similarity searches_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/',\n",
       "               'caption': 'ReHAB is designed to handle large numbers of query sequences, such as whole genomes or sets of genomes. Advanced computer skills are not needed to use ReHAB; the graphics interface is simple to use and was designed with the bench biologist in mind.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'BACKGROUND: Sequence similarity searching is a powerful tool to help develop hypotheses in the quest to assign functional, structural and evolutionary information to DNA and protein sequences. As sequence databases continue to grow exponentially, it becomes increasingly important to repeat searches at frequent intervals, and similarity searches retrieve larger and larger sets of results. New and potentially significant results may be buried in a long list of previously obtained sequence hits from past searches. RESULTS: ReHAB (Recent Hits Acquired from BLAST) is a tool for finding new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with two versions of a protein sequence database and highlights hits that are present only in the updated database. Results are presented in an easily comprehended table, or in a BLAST-like report, using colors to highlight the new hits. ReHAB is designed to handle large numbers of query sequences, such as whole genomes or sets of genomes. Advanced computer skills are not needed to use ReHAB; the graphics interface is simple to use and was designed with the bench biologist in mind. CONCLUSIONS: This software greatly simplifies the problem of evaluating the output of large numbers of protein database searches.',\n",
       "               'score': 0.031159421429038048})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment the below line if you want to inspect the ordered results\n",
    "ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2a3595-c3b7-4376-b9c5-0db7a42b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "top_docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location+os.environ['BLOB_SAS_TOKEN']}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(top_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "050a7262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of elements in top_docs: 5. Here they are:\n",
      "[Document(page_content='predictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [1995] has studied reinforcement learning in multi-robot domains.\\nShe notes, for example, how learning can give rise to social behaviors (Matarić\\n[1997]). The work shows how robots can be individually programmed to pro-\\nduce certain group behaviors. It represents a good example of the usefulness\\nand flexibility of learning agents in multi-agent domains. However, the author\\ndoes not offer a mathematical justification for the chosen individual learning\\nalgorithms, nor does she explain why the agents were able to converge to the\\nglobal behaviors. Our research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[1997]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents learn-\\ning about agents (Sen [1996], Weiß [1997]). For example, Sen and Sekaran [1998]\\nshow how learning agents in simple MAS converge to system-wide optimal be-\\nhavior. Their agents use Q-learning or modified classifier systems in order to\\nlearn. The authors implement these agents and compare the performance of the\\ndifferent learning algorithms for developing agent coordination. Hu and Well-\\nman [1998b, 1996] have studied reinforcement learning in market-base MASs,\\nshowing how certain initial learning biases can be self-fulfilling, and how learn-\\ning can be useful but is affected by an agent’s models of other agents. Claus\\nand Boutilier [1997] have also carried out experimental studies of the behavior of\\nreinforcement learning agents. We have been able to use the CLRI framework\\nto predict some of their experimental results Vidal [1998]. Other researchers\\nsuch as Stone and Veloso [1999], Littman [1994], and Hu and Wellman [1998a]\\nhave extended the basic Q-learning Watkins and Dayan [1992] algorithm for use\\nwith MASs in an effort to either improve or prove convergence to the optimal\\nbehavior.\\n\\nWe have also successfully experimented with reinforcement learning simu-\\nlations (Vidal and Durfee [1998]), but we believe that the formal treatment\\nelucidated in these pages will shed more light into the real nature of the prob-\\nlem and the relative importance of the various parameters that describe the\\n\\n25\\n\\n\\n\\ncapabilities of an agent’s learning algorithm.\\n\\n11 Summary\\n\\nWe have presented a framework for studying and predicting the behavior of\\nMASs composed of learning agents. We believe that this framework captures the\\nmost important parameters that describe an agents’ learning and the system’s\\nrules of encounter. Various comparisons between the framework’s predictions\\nand experimental results were given. These comparisons showed that the theo-\\nretical predictions closely match our experimental results and the experimental\\nresults published by others. Our success in reproducing these results allows us\\nto confidently state the effectiveness and accuracy of our theory in predicting\\nthe expected error of machine learning agents in MASs.\\n\\nSince our theory describes an agent’s behavior at a high-level (i.e., the agent’s\\nerror), it is not capable of making system-specific predictions (e.g., predicting\\nthe particular actions that are favored). These types of system-specific predic-\\ntions can only be arrived at by the traditional method of implementing popula-\\ntions of such agents and testing their behaviors. However, we expect that there\\nwill be times when the predictions from our theory will be enough to answer a\\ndesigner’s questions. A MAS designer that only needs to determine how “good”\\nthe agent’s behavior will be could probably use the CLRI framework. A de-\\nsigner that needs to know which particular emergent behaviors will be favored\\nby his agents will need to implement the agents.\\n\\nFinally, while we have given some examples as to how learning rates can be\\ndetermined for particular machine learning implementations, we do not have\\nany general method for determining these rates. However, we showed how to\\nuse the sample complexity of a learning problem to determine a lower bound on\\nthe learning rate of a consistent learning agent. This bound is useful for quickly\\n', metadata={'source': 'https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D'}), Document(page_content='Although efficient methods exist to assemble synthetic oligonucleotides into genes and genomes, these suffer from the presence of 1–3 random errors/kb of DNA. Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population. The DNA is fragmented, and mismatched fragments are removed upon binding to an immobilized mismatch binding protein (MutS). PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population. We show that two iterations of consensus shuffling improved a population of synthetic green fluorescent protein (GFPuv) clones from ∼60 to >90% fluorescent, and decreased errors 3.5- to 4.3-fold to final values of ∼1 error per 3500 bp. In addition, two iterations of consensus shuffling corrected a population of GFPuv clones where all members were non-functional, to a population where 82% of clones were fluorescent. Consensus shuffling should facilitate the rapid and accurate synthesis of long DNA sequences.', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D'}), Document(page_content='partially relevant to our work. Our emphasis is on finding ways to predict the\\nbehavior of MASs composed of machine-learning agents. We are only concerned\\nwith the behavior of simpler artificial programmable agents, rather than the\\ncomplex behavior of humans or the unpredictable behavior of animals.\\n\\nThe dynamics of MASs have also been studied by Kephart et. al. [18]. In\\nthis work the authors show how simple predictive agents can lead to globally\\ncyclic or chaotic behaviors. As the authors explain, the chaotic behaviors were a\\nresult of the simple predictive strategies used by the agents. Unlike our agents,\\nmost of their agents are not engaged in learning, instead they use simple fixed\\npredictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [22] has studied reinforcement learning in multi-robot domains. She\\nnotes, for example, how learning can give rise to social behaviors [23]. The\\nwork shows how robots can be individually programmed to produce certain\\ngroup behaviors. It represents a good example of the usefulness and flexibility\\nof learning agents in multi-agent domains. However, the author does not offer\\na mathematical justification for the chosen individual learning algorithms, nor\\ndoes she explain why the agents were able to converge to the global behaviors.\\nOur research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[4]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents\\nlearning about agents [27, 36]. For example, Sen and Sekaran [28] show how\\nlearning agents in simple MAS converge to system-wide optimal behavior. Their\\nagents use Q-learning or modified classifier systems in order to learn. The\\nauthors implement these agents and compare the performance of the different\\nlearning algorithms for developing agent coordination. Hu and Wellman [14, 12]\\nhave studied reinforcement learning in market-base MASs, showing how certain\\ninitial learning biases can be self-fulfilling, and how learning can be useful but\\nis affected by an agent’s models of other agents. Claus and Boutilier [6] have\\nalso carried out experimental studies of the behavior of reinforcement learning\\nagents. We have been able to use the CLRI framework to predict some of their\\nexperimental results [33]. Other researchers [32, 20, 13] have extended the basic\\nQ-learning [35] algorithm for use with MASs in an effort to either improve or\\n\\n25\\n\\n\\n\\nprove convergence to the optimal behavior.\\nWe have also successfully experimented with reinforcement learning simula-\\n\\ntions [34], but we believe that the formal treatment elucidated in these pages\\nwill shed more light into the real nature of the problem and the relative im-\\nportance of the various parameters that describe the capabilities of an agent’s\\nlearning algorithm.\\n\\n11 Limitations and Future Work\\n\\nThe CLRI framework places some constraints on the type of systems it can\\nmodel, which limits its usability. However, it is important to understand that,\\nas we remove the limitations from the CLRI framework, the dynamics of the\\nsystem become much harder to predict. In the extreme, without any limitations\\non the agents’ abilities, the system becomes a complex adaptive system, as\\nstudied by Holland [11] and others in the field of complexity. The dynamic\\nbehavior of these systems continues to be studied by complexity researchers\\nwith only modest progress. It is only by placing limitations on the system that\\nwe were able to predict the expected error of agents in the systems modeled by\\nthe CLRI framework.\\n\\nOur ongoing work involves the relaxation of some of the constraints made by\\nthe CLRI framework so that it may become more easily and widely applicable,\\nwithout making the system dynamics impossible to analyze. We are targeting\\nthree specific constraints.\\n\\n1. The values of ci, li, ri, and Iij cannot, in all situations, be mathematically\\ndetermined from the system’s description. We have found that bounds for\\nthe ci, li, and ri values can often be determined when using reinforcement\\nlearning or supervised learning. However, the bounds are often very loose.\\n', metadata={'source': 'https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D'}), Document(page_content='The term \"Geographic Information Systems\" (GIS) has been added to MeSH in 2003, a step reflecting the importance and growing use of GIS in health and healthcare research and practices. GIS have much more to offer than the obvious digital cartography (map) functions. From a community health perspective, GIS could potentially act as powerful evidence-based practice tools for early problem detection and solving. When properly used, GIS can: inform and educate (professionals and the public); empower decision-making at all levels; help in planning and tweaking clinically and cost-effective actions, in predicting outcomes before making any financial commitments and ascribing priorities in a climate of finite resources; change practices; and continually monitor and analyse changes, as well as sentinel events. Yet despite all these potentials for GIS, they remain under-utilised in the UK National Health Service (NHS). This paper has the following objectives: (1) to illustrate with practical, real-world scenarios and examples from the literature the different GIS methods and uses to improve community health and healthcare practices, e.g., for improving hospital bed availability, in community health and bioterrorism surveillance services, and in the latest SARS outbreak; (2) to discuss challenges and problems currently hindering the wide-scale adoption of GIS across the NHS; and (3) to identify the most important requirements and ingredients for addressing these challenges, and realising GIS potential within the NHS, guided by related initiatives worldwide. The ultimate goal is to illuminate the road towards implementing a comprehensive national, multi-agency spatio-temporal health information infrastructure functioning proactively in real time. The concepts and principles presented in this paper can be also applied in other countries, and on regional (e.g., European Union) and global levels.', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D'}), Document(page_content='BACKGROUND: Sequence similarity searching is a powerful tool to help develop hypotheses in the quest to assign functional, structural and evolutionary information to DNA and protein sequences. As sequence databases continue to grow exponentially, it becomes increasingly important to repeat searches at frequent intervals, and similarity searches retrieve larger and larger sets of results. New and potentially significant results may be buried in a long list of previously obtained sequence hits from past searches. RESULTS: ReHAB (Recent Hits Acquired from BLAST) is a tool for finding new protein hits in repeated PSI-BLAST searches. ReHAB compares results from PSI-BLAST searches performed with two versions of a protein sequence database and highlights hits that are present only in the updated database. Results are presented in an easily comprehended table, or in a BLAST-like report, using colors to highlight the new hits. ReHAB is designed to handle large numbers of query sequences, such as whole genomes or sets of genomes. Advanced computer skills are not needed to use ReHAB; the graphics interface is simple to use and was designed with the bench biologist in mind. CONCLUSIONS: This software greatly simplifies the problem of evaluating the output of large numbers of protein database searches.', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D'})]\n"
     ]
    }
   ],
   "source": [
    "# Comment / Uncomment the below line if you want to inspect the ordered top_docs\n",
    "print(f\"Nr. of elements in top_docs: {len(top_docs)}. Here they are:\\n{top_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c26d7540-feb8-4581-849e-003f4bf2a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt token count: 2464\n",
      "Max Completion Token count: 2000\n",
      "Combined docs (context) token count: 2873\n",
      "--------\n",
      "Requested token count: 7337\n",
      "Token limit for gpt-35-turbo-16k : 16384\n",
      "Chain Type selected: stuff\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(top_docs)>0):\n",
    "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    prompt_tokens = num_tokens_from_string(COMBINE_CHAT_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
    "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
    "    \n",
    "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
    "    \n",
    "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
    "    \n",
    "    print(\"System prompt token count:\",prompt_tokens)\n",
    "    print(\"Max Completion Token count:\", COMPLETION_TOKENS)\n",
    "    print(\"Combined docs (context) token count:\",context_tokens)\n",
    "    print(\"--------\")\n",
    "    print(\"Requested token count:\",requested_tokens)\n",
    "    print(\"Token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Chain Type selected:\", chain_type)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb2e5d",
   "metadata": {},
   "source": [
    "## Using load_qa_with_sources_chain\n",
    "The *get_answer* function in the next cell is equivalent to the next code, in case **chain_type = map_reduce**:\n",
    "\n",
    "```\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from common.prompts import (COMBINE_PROMPT, COMBINE_QUESTION_PROMPT)\n",
    "\n",
    "llm35 = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
    "chain = load_qa_with_sources_chain(llm35, chain_type=\"map_reduce\",\n",
    "                                   combine_prompt = COMBINE_PROMPT,\n",
    "                                   question_prompt = COMBINE_QUESTION_PROMPT,\n",
    "                                   return_intermediate_steps = True)\n",
    "\n",
    "answer = chain( {\"input_documents\": top_docs, \"question\": QUESTION, \"language\": \"it\"}, return_only_outputs=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce6efa9-2b8f-4810-904d-5986b4ae0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Some use cases for reinforcement learning include:\n",
       "- Predictive strategies to eliminate chaotic global fluctuations in multi-robot domains [1]\n",
       "- Model-based learning to build models of other agents via observations [2]\n",
       "- Learning agents in multi-agent systems to converge to system-wide optimal behavior [2]\n",
       "- Reinforcement learning in market-based multi-agent systems [2]\n",
       "- Improving the synthesis of long DNA sequences by reducing random errors [3]\n",
       "- Using GIS (Geographic Information Systems) to inform and educate, empower decision-making, and monitor changes in community health and healthcare practices [4]\n",
       "- Finding new protein hits in repeated PSI-BLAST searches [5]\n",
       "\n",
       "References:\n",
       "[1] Source: https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\n",
       "[2] Source: https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\n",
       "[3] Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\n",
       "[4] Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\n",
       "[5] Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 ms, sys: 4.12 ms, total: 29.4 ms\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get the answer\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27501f1b-7db0-4ee3-9cb1-e609254ffa3d",
   "metadata": {},
   "source": [
    "And if we ask the follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf5b323-3b9c-479b-8502-acfc4f7915dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The main points of our conversation are the following:\n",
       "- The conversation was about the use of Geographic Information Systems (GIS) in health and healthcare research and practices, highlighting their potential benefits such as informing and educating professionals and the public, empowering decision-making, planning and predicting outcomes, changing practices, and monitoring changes and sentinel events. However, GIS remains under-utilized in the UK National Health Service (NHS) due to challenges and problems hindering its wide-scale adoption.\n",
       "- The conversation also discussed the importance of implementing a comprehensive national, multi-agency spatio-temporal health information infrastructure that functions proactively in real-time, and identified the requirements and ingredients for addressing the challenges and realizing the potential of GIS within the NHS.\n",
       "- The conversation mentioned specific examples and scenarios where GIS can be applied, such as improving hospital bed availability, community health and bioterrorism surveillance services, and responding to outbreaks like SARS.\n",
       "- The conversation emphasized the need for repeated similarity searches in sequence databases to identify new and potentially significant results, and introduced a tool called ReHAB (Recent Hits Acquired from BLAST) that compares results from PSI-BLAST searches performed with two versions of a protein sequence database and highlights hits that are present only in the updated database.\n",
       "\n",
       "References:\n",
       "- <sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\" target=\"_blank\">[1]</a></sup>\n",
       "- <sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\" target=\"_blank\">[2]</a></sup>\n",
       "\n",
       "Is there anything else I can help you with?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_answer(llm=llm, docs=top_docs,  query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fa6e6-226c-400f-a504-30255385f43b",
   "metadata": {},
   "source": [
    "You might get a different response from above, but it doesn't matter what response you get, it will be based on the context given, not on previous answers.\n",
    "\n",
    "Until now we just have the same as the prior Notebook 03: results from Azure Search enhanced by OpenAI model, with no memory\n",
    "\n",
    "**Now let's add memory to it:**\n",
    "\n",
    "Reference: https://python.langchain.com/docs/modules/memory/how_to/adding_memory_chain_multiple_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98b876e-d264-48ae-b5ed-9801d6a9152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has several use cases. Here are some examples:\n",
       "\n",
       "1. **Multi-robot domains**: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to produce certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>. This demonstrates the usefulness and flexibility of learning agents in multi-agent domains.\n",
       "\n",
       "2. **Model-based learning**: In model-based learning, agents build models of other agents via observations. For example, agents can learn finite state machine models of other agents by observing their actions<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>. This approach allows agents to learn and understand the behavior of other agents in a multi-agent system.\n",
       "\n",
       "3. **Agent coordination**: Learning agents in multi-agent systems can converge to system-wide optimal behavior through reinforcement learning. Different learning algorithms, such as Q-learning or modified classifier systems, can be used to develop agent coordination<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "4. **Healthcare research**: GIS (Geographic Information Systems) have been used as evidence-based practice tools for early problem detection and solving in community health and healthcare practices. GIS can inform and educate professionals and the public, empower decision-making, help in planning and predicting outcomes, and monitor and analyze changes in healthcare systems<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>.\n",
       "\n",
       "5. **Protein sequence analysis**: Reinforcement learning can be applied to sequence similarity searching to help assign functional, structural, and evolutionary information to DNA and protein sequences. It can assist in finding new protein hits in repeated searches and highlight hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\n",
       "\n",
       "These examples demonstrate the diverse applications of reinforcement learning in various domains. Let me know if you have any more questions.<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup><sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup><sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\")\n",
    "\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf28927b-d9ee-4412-bb07-13e055e832a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points we discussed:\n",
       "\n",
       "1. Reinforcement learning has several use cases, including:\n",
       "   - Multi-robot domains, where learning can enable robots to produce certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Model-based learning, where agents build models of other agents via observations<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Agent coordination in multi-agent systems, where learning agents can converge to system-wide optimal behavior<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "2. Reinforcement learning also has applications in other domains, such as:\n",
       "   - Healthcare research, where Geographic Information Systems (GIS) can be used for early problem detection, planning, predicting outcomes, and monitoring changes in healthcare systems<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>.\n",
       "   - Protein sequence analysis, where reinforcement learning can assist in finding functional, structural, and evolutionary information in DNA and protein sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\n",
       "\n",
       "These examples demonstrate the diverse applications of reinforcement learning in various domains. Let me know if you have any more questions.\n",
       "\n",
       "References:\n",
       "[1]<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>\n",
       "[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>\n",
       "[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3830b0b8-0ca2-4d0a-9747-f6273368002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points we discussed:\n",
       "\n",
       "1. Reinforcement learning has several use cases, including:\n",
       "   - Multi-robot domains, where learning can enable robots to produce certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Model-based learning, where agents build models of other agents via observations<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Agent coordination in multi-agent systems, where learning agents can converge to system-wide optimal behavior<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "2. Reinforcement learning also has applications in other domains, such as:\n",
       "   - Healthcare research, where Geographic Information Systems (GIS) can be used for early problem detection, planning, predicting outcomes, and monitoring changes in healthcare systems<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>.\n",
       "   - Protein sequence analysis, where reinforcement learning can assist in finding functional, structural, and evolutionary information in DNA and protein sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\n",
       "\n",
       "These examples demonstrate the diverse applications of reinforcement learning in various domains. Let me know if you have any more questions.\n",
       "\n",
       "References:\n",
       "[1]<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>\n",
       "[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>\n",
       "[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Translate to Italian\", language=\"Italian\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e732b-3c8c-4df3-8fcb-c3d01e7bec74",
   "metadata": {},
   "source": [
    "You might get a different answer on the above cell, and it is ok, this bot is not yet well configured to answer any question that is not related to its knowledge base, including salutations.\n",
    "\n",
    "Let's check our memory to see that it's keeping the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1279692c-7eb0-4300-8a66-c7025f02c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning\\nAI: Reinforcement learning has several use cases. Here are some examples:\\n\\n1. **Multi-robot domains**: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to produce certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>. This demonstrates the usefulness and flexibility of learning agents in multi-agent domains.\\n\\n2. **Model-based learning**: In model-based learning, agents build models of other agents via observations. For example, agents can learn finite state machine models of other agents by observing their actions<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>. This approach allows agents to learn and understand the behavior of other agents in a multi-agent system.\\n\\n3. **Agent coordination**: Learning agents in multi-agent systems can converge to system-wide optimal behavior through reinforcement learning. Different learning algorithms, such as Q-learning or modified classifier systems, can be used to develop agent coordination<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n4. **Healthcare research**: GIS (Geographic Information Systems) have been used as evidence-based practice tools for early problem detection and solving in community health and healthcare practices. GIS can inform and educate professionals and the public, empower decision-making, help in planning and predicting outcomes, and monitor and analyze changes in healthcare systems<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>.\\n\\n5. **Protein sequence analysis**: Reinforcement learning can be applied to sequence similarity searching to help assign functional, structural, and evolutionary information to DNA and protein sequences. It can assist in finding new protein hits in repeated searches and highlight hits that are present only in the updated database<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\\n\\nThese examples demonstrate the diverse applications of reinforcement learning in various domains. Let me know if you have any more questions.<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup><sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup><sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>\\nHuman: Give me the main points of our conversation\\nAI: Based on our conversation, here are the main points we discussed:\\n\\n1. Reinforcement learning has several use cases, including:\\n   - Multi-robot domains, where learning can enable robots to produce certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Model-based learning, where agents build models of other agents via observations<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Agent coordination in multi-agent systems, where learning agents can converge to system-wide optimal behavior<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n2. Reinforcement learning also has applications in other domains, such as:\\n   - Healthcare research, where Geographic Information Systems (GIS) can be used for early problem detection, planning, predicting outcomes, and monitoring changes in healthcare systems<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>.\\n   - Protein sequence analysis, where reinforcement learning can assist in finding functional, structural, and evolutionary information in DNA and protein sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\\n\\nThese examples demonstrate the diverse applications of reinforcement learning in various domains. Let me know if you have any more questions.\\n\\nReferences:\\n[1]<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>\\n[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>\\n[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>\\nHuman: Thank you\\nAI: Based on our conversation, here are the main points we discussed:\\n\\n1. Reinforcement learning has several use cases, including:\\n   - Multi-robot domains, where learning can enable robots to produce certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Model-based learning, where agents build models of other agents via observations<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Agent coordination in multi-agent systems, where learning agents can converge to system-wide optimal behavior<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n2. Reinforcement learning also has applications in other domains, such as:\\n   - Healthcare research, where Geographic Information Systems (GIS) can be used for early problem detection, planning, predicting outcomes, and monitoring changes in healthcare systems<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>.\\n   - Protein sequence analysis, where reinforcement learning can assist in finding functional, structural, and evolutionary information in DNA and protein sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\\n\\nThese examples demonstrate the diverse applications of reinforcement learning in various domains. Let me know if you have any more questions.\\n\\nReferences:\\n[1]<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>\\n[3]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[3]</a></sup>\\n[4]<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC549547/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87405173",
   "metadata": {},
   "source": [
    "## Using CosmosDB as persistent memory\n",
    "\n",
    "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wisg to provide recommendations. \n",
    "\n",
    "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
    "We will use a class in LangChain use CosmosDBChatMessageHistory, see [HERE](https://python.langchain.com/en/latest/_modules/langchain/memory/chat_message_histories/cosmos_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7131daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CosmosDB instance from langchain cosmos class.\n",
    "cosmos = CosmosDBChatMessageHistory(\n",
    "    cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "    cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
    "    cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "    connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "    session_id=\"Agent-Test-Session\" + str(random.randint(1, 1000)),\n",
    "    user_id=\"Agent-Test-User\" + str(random.randint(1, 1000))\n",
    "    )\n",
    "\n",
    "# prepare the cosmosdb instance\n",
    "cosmos.prepare_cosmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Memory Object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\",chat_memory=cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27ceb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has several use cases. Here are some examples:\n",
       "\n",
       "1. **Multi-robot domains**: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "2. **Model-based learning**: Agents can build models of other agents through observations, and reinforcement learning algorithms can be used to effectively learn these models<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "3. **Market-based multi-agent systems**: Reinforcement learning has been studied in market-based multi-agent systems, where certain initial learning biases can be self-fulfilling and learning can be useful for agent coordination<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "4. **Agent learning in healthcare**: GIS (Geographic Information Systems) can be used as evidence-based practice tools in community health and healthcare practices. GIS can inform and educate professionals and the public, empower decision-making, help in planning and prioritizing actions, and continually monitor and analyze changes in healthcare<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\n",
       "\n",
       "These are just a few examples of the use cases for reinforcement learning. It is a versatile approach that can be applied to various domains and problems. If you need more information or have any other questions, feel free to ask!\n",
       "\n",
       "References:\n",
       "1. [Reinforcement Learning in Multi-Robot Domains](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "2. [Reinforcement Learning in Market-Based Multiagent Systems](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "3. [Reinforcement Learning in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing using our Question\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a5ff826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points:\n",
       "\n",
       "1. Reinforcement learning has several use cases, including:\n",
       "   - Multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Model-based learning, where agents build models of other agents through observations and reinforcement learning algorithms are used to learn these models<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Market-based multi-agent systems, where reinforcement learning is studied and learning can be useful for agent coordination<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "2. Reinforcement learning can also be applied to agent learning in healthcare, where Geographic Information Systems (GIS) can be used as evidence-based practice tools in community health and healthcare practices. GIS can inform and educate professionals and the public, empower decision-making, help in planning and prioritizing actions, and continually monitor and analyze changes in healthcare<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\n",
       "\n",
       "If you have any more questions or need further information, feel free to ask!\n",
       "\n",
       "References:\n",
       "1. [Reinforcement Learning in Multi-Robot Domains](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "2. [Reinforcement Learning in Market-Based Multiagent Systems](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "3. [Reinforcement Learning in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "4. [GIS in Health and Healthcare Practices: Where Are We Now?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be1620fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ecco i punti principali della nostra conversazione:\n",
       "\n",
       "1. L'apprendimento per rinforzo ha diverse applicazioni, tra cui:\n",
       "   - Domini multi-robot, dove l'apprendimento può generare comportamenti sociali e consentire ai robot di programmare individualmente determinati comportamenti di gruppo<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Apprendimento basato su modelli, in cui gli agenti possono costruire modelli di altri agenti attraverso osservazioni e algoritmi di apprendimento per rinforzo possono essere utilizzati per apprendere efficacemente questi modelli<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "   - Sistemi multi-agenti basati sul mercato, dove l'apprendimento per rinforzo è stato studiato e l'apprendimento può essere utile per il coordinamento degli agenti<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\n",
       "\n",
       "2. L'apprendimento per rinforzo può essere applicato anche all'apprendimento degli agenti nel settore sanitario, in cui i sistemi di informazione geografica (GIS) possono essere utilizzati come strumenti basati su evidenze per la pratica basata sulle comunità sanitarie e le pratiche sanitarie. I GIS possono informare ed educare professionisti e pubblico, facilitare la presa di decisioni, aiutare nella pianificazione e nell'assegnazione delle priorità, monitorare e analizzare continuamente i cambiamenti nel settore sanitario<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\n",
       "\n",
       "Se hai altre domande o hai bisogno di ulteriori informazioni, chiedi pure!\n",
       "\n",
       "Riferimenti:\n",
       "1. [Reinforcement Learning in Multi-Robot Domains](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "2. [Reinforcement Learning in Market-Based Multiagent Systems](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "3. [Reinforcement Learning in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\n",
       "4. [GIS in Health and Healthcare Practices: Where Are We Now?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Translate to Italian\", language=\"Italian\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5ac98",
   "metadata": {},
   "source": [
    "Let's check our Azure CosmosDB to see the whole conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1d7688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me some use cases for reinforcement learning'),\n",
       " AIMessage(content='Reinforcement learning has several use cases. Here are some examples:\\n\\n1. **Multi-robot domains**: Reinforcement learning has been studied in multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n2. **Model-based learning**: Agents can build models of other agents through observations, and reinforcement learning algorithms can be used to effectively learn these models<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n3. **Market-based multi-agent systems**: Reinforcement learning has been studied in market-based multi-agent systems, where certain initial learning biases can be self-fulfilling and learning can be useful for agent coordination<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n4. **Agent learning in healthcare**: GIS (Geographic Information Systems) can be used as evidence-based practice tools in community health and healthcare practices. GIS can inform and educate professionals and the public, empower decision-making, help in planning and prioritizing actions, and continually monitor and analyze changes in healthcare<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\\n\\nThese are just a few examples of the use cases for reinforcement learning. It is a versatile approach that can be applied to various domains and problems. If you need more information or have any other questions, feel free to ask!\\n\\nReferences:\\n1. [Reinforcement Learning in Multi-Robot Domains](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n2. [Reinforcement Learning in Market-Based Multiagent Systems](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n3. [Reinforcement Learning in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00%3A00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)'),\n",
       " HumanMessage(content='Give me the main points of our conversation'),\n",
       " AIMessage(content='Based on our conversation, here are the main points:\\n\\n1. Reinforcement learning has several use cases, including:\\n   - Multi-robot domains, where learning can give rise to social behaviors and enable robots to individually program certain group behaviors<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10%3A02Z&se=2030-12-16T11%3A00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Model-based learning, where agents build models of other agents through observations and reinforcement learning algorithms are used to learn these models<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Market-based multi-agent systems, where reinforcement learning is studied and learning can be useful for agent coordination<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n2. Reinforcement learning can also be applied to agent learning in healthcare, where Geographic Information Systems (GIS) can be used as evidence-based practice tools in community health and healthcare practices. GIS can inform and educate professionals and the public, empower decision-making, help in planning and prioritizing actions, and continually monitor and analyze changes in healthcare<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\\n\\nIf you have any more questions or need further information, feel free to ask!\\n\\nReferences:\\n1. [Reinforcement Learning in Multi-Robot Domains](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n2. [Reinforcement Learning in Market-Based Multiagent Systems](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n3. [Reinforcement Learning in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n4. [GIS in Health and Healthcare Practices: Where Are We Now?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)'),\n",
       " HumanMessage(content='Translate to Italian'),\n",
       " AIMessage(content='Ecco i punti principali della nostra conversazione:\\n\\n1. L\\'apprendimento per rinforzo ha diverse applicazioni, tra cui:\\n   - Domini multi-robot, dove l\\'apprendimento può generare comportamenti sociali e consentire ai robot di programmare individualmente determinati comportamenti di gruppo<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Apprendimento basato su modelli, in cui gli agenti possono costruire modelli di altri agenti attraverso osservazioni e algoritmi di apprendimento per rinforzo possono essere utilizzati per apprendere efficacemente questi modelli<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n   - Sistemi multi-agenti basati sul mercato, dove l\\'apprendimento per rinforzo è stato studiato e l\\'apprendimento può essere utile per il coordinamento degli agenti<sup><a href=\"https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[1]</a></sup>.\\n\\n2. L\\'apprendimento per rinforzo può essere applicato anche all\\'apprendimento degli agenti nel settore sanitario, in cui i sistemi di informazione geografica (GIS) possono essere utilizzati come strumenti basati su evidenze per la pratica basata sulle comunità sanitarie e le pratiche sanitarie. I GIS possono informare ed educare professionisti e pubblico, facilitare la presa di decisioni, aiutare nella pianificazione e nell\\'assegnazione delle priorità, monitorare e analizzare continuamente i cambiamenti nel settore sanitario<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D\">[4]</a></sup>.\\n\\nSe hai altre domande o hai bisogno di ulteriori informazioni, chiedi pure!\\n\\nRiferimenti:\\n1. [Reinforcement Learning in Multi-Robot Domains](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n2. [Reinforcement Learning in Market-Based Multiagent Systems](https://blobstoragey24bi577jszf6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n3. [Reinforcement Learning in Healthcare](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)\\n4. [GIS in Health and Healthcare Practices: Where Are We Now?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC343292/?sv=2023-01-03&ss=btqf&srt=sco&st=2023-12-15T13%3A10:02Z&se=2030-12-16T11:00:00Z&sp=rl&sig=csDdBE4LlNa0VmLDMUbbY6pcly9wKr5f8XKFpKRfq64%3D)')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load message from cosmosdb\n",
    "cosmos.load_messages()\n",
    "cosmos.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
   "metadata": {},
   "source": [
    "![CosmosDB Memory](./images/cosmos-chathistory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
    "\n",
    "We added persitent memory using CosmosDB.\n",
    "\n",
    "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, regardless of the input and it struggles to respond to prompts like: Hello, Thank you, Bye, What's your name, What's the weather and any other task that is not search in the knowledge base.\n",
    "\n",
    "\n",
    "## <u>Important Note</u>:<br>\n",
    "As we proceed, while all the code will remain compatible with GPT-3.5 models, we highly recommend transitioning to GPT-4. Here's why:\n",
    "\n",
    "**GPT-3.5-Turbo** can be likened to a 7-year-old child. You can provide it with concise instructions, but it frequently struggles to follow them accurately. Additionally, its limited memory can make sustained conversations challenging.\n",
    "\n",
    "**GPT-3.5-Turbo-16k** resembles the same 7-year-old, but with an increased attention span for longer instructions. However, it still faces difficulties accurately executing them about half the time.\n",
    "\n",
    "**GPT-4** exhibits the capabilities of a 10-12-year-old child. It possesses enhanced reasoning skills and more consistently adheres to instructions. While its memory retention for instructions is moderate, it excels at following them.\n",
    "\n",
    "**GPT-4-32k** is akin to the 10-12-year-old child with an extended memory. It comprehends lengthy sets of instructions and engages in meaningful conversations. Thanks to its robust memory, it offers detailed responses.\n",
    "\n",
    "Understanding this analogy above will become clearer as you complete the final notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
    "\n",
    "But, does this solve all the possible scenarios that a virtual assistant will require?  **What about if the answer to the Smart Search Engine is not related to text, but instead requires to look into tabular data?** The next notebook explains and solves the tabular problem and the concept of Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivbd_20231215",
   "language": "python",
   "name": "openaivbd_20231215"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
