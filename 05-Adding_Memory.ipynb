{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
   "metadata": {},
   "source": [
    "# Understanding Memory in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
   "metadata": {},
   "source": [
    "In the previous Notebook, we successfully explored how OpenAI models can enhance the results from Azure Cognitive Search. \n",
    "\n",
    "However, we have yet to discover how to engage in a conversation with the LLM. With [Bing Chat](http://chat.bing.com/), for example, this is possible, as it can understand and reference the previous responses.\n",
    "\n",
    "There is a common misconception that GPT models have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
    "\n",
    "In this Notebook, our goal is to illustrate how we can effectively \"endow the LLM with memory\" by employing prompts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from openai.error import OpenAIError\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import CosmosDBChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import (\n",
    "    get_search_results,\n",
    "    update_vector_indexes,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    num_tokens_from_string,\n",
    "    get_answer,\n",
    ")\n",
    "\n",
    "from common.prompts import COMBINE_CHAT_PROMPT_TEMPLATE\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials (my).env\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"]    = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"]     = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"]    = os.environ[\"OPENAI_API_TYPE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
   "metadata": {},
   "source": [
    "### Let's start with the basics\n",
    "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Tell me some use cases for reinforcement learning?\"\n",
    "FOLLOW_UP_QUESTION = \"Give me the main points of our conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = os.environ[\"COMPLETION3516_DEPLOYMENT\"] # we DO have chatgpt-4, but not needed in this case\n",
    "COMPLETION_TOKENS = 500\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201b3a82-3d28-444f-a37f-51beb28ecf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-35-turbo-16k'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning can be applied to various real-world scenarios. Here are some use cases:\n",
       "\n",
       "1. Game Playing: Reinforcement learning has been successfully used to train agents that can play complex games like chess, Go, poker, and video games. For example, AlphaGo, developed by DeepMind, defeated the world champion Go player using reinforcement learning.\n",
       "\n",
       "2. Robotics: Reinforcement learning enables robots to learn how to perform tasks in dynamic and uncertain environments. It can be used for tasks like grasping objects, locomotion, navigation, and manipulation.\n",
       "\n",
       "3. Autonomous Vehicles: Reinforcement learning can help train autonomous vehicles to make decisions in complex traffic scenarios, such as lane changing, merging, and parking.\n",
       "\n",
       "4. Recommendation Systems: Reinforcement learning algorithms can be used to optimize recommendation systems by learning user preferences and making personalized recommendations in real-time.\n",
       "\n",
       "5. Energy Management: Reinforcement learning can optimize energy consumption in buildings by learning to control heating, cooling, and lighting systems based on user comfort and energy efficiency.\n",
       "\n",
       "6. Finance and Trading: Reinforcement learning can be applied to algorithmic trading to learn optimal trading strategies based on market conditions and maximize profits.\n",
       "\n",
       "7. Healthcare: Reinforcement learning can assist in medical treatment planning, personalized drug dosage determination, and optimizing resource allocation in hospitals.\n",
       "\n",
       "8. Supply Chain Management: Reinforcement learning can optimize inventory management, demand forecasting, and logistics planning to minimize costs and maximize efficiency.\n",
       "\n",
       "9. Advertising and Marketing: Reinforcement learning can optimize online advertising campaigns by learning to target the right audience, determining optimal bidding strategies, and maximizing conversions.\n",
       "\n",
       "10. Industrial Automation: Reinforcement learning can be used to optimize control systems in manufacturing processes, quality control, and predictive maintenance.\n",
       "\n",
       "These are just a few examples, and the potential applications of reinforcement learning are vast and diverse."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "response = chain.run(QUESTION)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. The conversation revolved around the topic of artificial intelligence and its impact on society.\\n2. We discussed the potential benefits of AI, such as increased efficiency and productivity in various industries.\\n3. We also touched upon the concerns and ethical considerations surrounding AI, including job displacement and privacy issues.\\n4. The conversation highlighted the need for proper regulation and responsible development of AI technologies.\\n5. We acknowledged the importance of public awareness and education about AI to ensure its responsible and ethical use.\\n6. Overall, the conversation emphasized the need for a balanced approach to AI, considering both its potential benefits and potential risks.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "chain.run(FOLLOW_UP_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As you can see, it doesn't remember what it just responded, sometimes it responds based only on the system prompt, or just randomly. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "hist_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "                {history}\n",
    "                Human: {question}\n",
    "                AI:\n",
    "            \"\"\"\n",
    "    )\n",
    "chain = LLMChain(llm=llm, prompt=hist_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Reinforcement learning can be applied to various real-world scenarios.\n",
       "- Some use cases of reinforcement learning include game playing, robotics, autonomous vehicles, recommendation systems, energy management, finance and trading, healthcare, supply chain management, advertising and marketing, and industrial automation.\n",
       "- Reinforcement learning can optimize decision-making, maximize profits, minimize costs, and improve efficiency in these domains.\n",
       "- The potential applications of reinforcement learning are vast and diverse."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain.run({\"history\": conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
   "metadata": {},
   "source": [
    "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
   "metadata": {},
   "source": [
    "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba257e86-fd90-4a51-a72d-27000913e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Memory adds tokens to the prompt, we would need a better model that allows more space on the prompt\n",
    "COMPLETION_TOKENS = 1000\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
    "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name    = \"cogsrch-index-files\"\n",
    "index2_name    = \"cogsrch-index-csv\"\n",
    "index3_name    = \"cogsrch-index-books-vector\"\n",
    "text_indexes   = [index1_name, index2_name]\n",
    "vector_indexes = [index+\"-vector\" for index in text_indexes] + [index3_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b01852c2-6192-496c-adff-4270f9380469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 5\n",
      "CPU times: total: 125 ms\n",
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Search in text-based indexes first and update vector indexes\n",
    "k=10 # Top k results per each text-based index\n",
    "ordered_results = get_search_results(QUESTION, text_indexes, k=k, reranker_threshold=1, vector_search=False)\n",
    "update_vector_indexes(ordered_search_results=ordered_results, embedder=embedder)\n",
    "\n",
    "# Search in all vector-based indexes available\n",
    "similarity_k = 5 # top results from multi-vector-index similarity search\n",
    "ordered_results = get_search_results(QUESTION, vector_indexes, k=k, vector_search=True,\n",
    "                                        similarity_k=similarity_k,\n",
    "                                        query_vector = embedder.embed_query(QUESTION))\n",
    "print(\"Number of results:\", len(ordered_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca500dd8-148c-4d8a-b58b-2df4c957459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('aHR0cHM6Ly9ibG9ic3RvcmFnZXloeXVmb3FoNnlqcjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA4djIucGRm0_12',\n",
       "              {'title': 'arXiv:cs/0001008v2  [cs.MA]  17 Jan 2000_chunk_12',\n",
       "               'name': '0001008v2.pdf',\n",
       "               'location': 'https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf',\n",
       "               'caption': 'Matarić [1995] has studied reinforcement learning in multi-robot domains. She notes, for example, how learning can give rise to social behaviors (Matarić [1997]). The work shows how robots can be individually programmed to pro- duce certain group behaviors.',\n",
       "               'index': 'cogsrch-index-files-vector',\n",
       "               'chunk': 'predictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [1995] has studied reinforcement learning in multi-robot domains.\\nShe notes, for example, how learning can give rise to social behaviors (Matarić\\n[1997]). The work shows how robots can be individually programmed to pro-\\nduce certain group behaviors. It represents a good example of the usefulness\\nand flexibility of learning agents in multi-agent domains. However, the author\\ndoes not offer a mathematical justification for the chosen individual learning\\nalgorithms, nor does she explain why the agents were able to converge to the\\nglobal behaviors. Our research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[1997]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents learn-\\ning about agents (Sen [1996], Weiß [1997]). For example, Sen and Sekaran [1998]\\nshow how learning agents in simple MAS converge to system-wide optimal be-\\nhavior. Their agents use Q-learning or modified classifier systems in order to\\nlearn. The authors implement these agents and compare the performance of the\\ndifferent learning algorithms for developing agent coordination. Hu and Well-\\nman [1998b, 1996] have studied reinforcement learning in market-base MASs,\\nshowing how certain initial learning biases can be self-fulfilling, and how learn-\\ning can be useful but is affected by an agent’s models of other agents. Claus\\nand Boutilier [1997] have also carried out experimental studies of the behavior of\\nreinforcement learning agents. We have been able to use the CLRI framework\\nto predict some of their experimental results Vidal [1998]. Other researchers\\nsuch as Stone and Veloso [1999], Littman [1994], and Hu and Wellman [1998a]\\nhave extended the basic Q-learning Watkins and Dayan [1992] algorithm for use\\nwith MASs in an effort to either improve or prove convergence to the optimal\\nbehavior.\\n\\nWe have also successfully experimented with reinforcement learning simu-\\nlations (Vidal and Durfee [1998]), but we believe that the formal treatment\\nelucidated in these pages will shed more light into the real nature of the prob-\\nlem and the relative importance of the various parameters that describe the\\n\\n25\\n\\n\\n\\ncapabilities of an agent’s learning algorithm.\\n\\n11 Summary\\n\\nWe have presented a framework for studying and predicting the behavior of\\nMASs composed of learning agents. We believe that this framework captures the\\nmost important parameters that describe an agents’ learning and the system’s\\nrules of encounter. Various comparisons between the framework’s predictions\\nand experimental results were given. These comparisons showed that the theo-\\nretical predictions closely match our experimental results and the experimental\\nresults published by others. Our success in reproducing these results allows us\\nto confidently state the effectiveness and accuracy of our theory in predicting\\nthe expected error of machine learning agents in MASs.\\n\\nSince our theory describes an agent’s behavior at a high-level (i.e., the agent’s\\nerror), it is not capable of making system-specific predictions (e.g., predicting\\nthe particular actions that are favored). These types of system-specific predic-\\ntions can only be arrived at by the traditional method of implementing popula-\\ntions of such agents and testing their behaviors. However, we expect that there\\nwill be times when the predictions from our theory will be enough to answer a\\ndesigner’s questions. A MAS designer that only needs to determine how “good”\\nthe agent’s behavior will be could probably use the CLRI framework. A de-\\nsigner that needs to know which particular emergent behaviors will be favored\\nby his agents will need to implement the agents.\\n\\nFinally, while we have given some examples as to how learning rates can be\\ndetermined for particular machine learning implementations, we do not have\\nany general method for determining these rates. However, we showed how to\\nuse the sample complexity of a learning problem to determine a lower bound on\\nthe learning rate of a consistent learning agent. This bound is useful for quickly\\n',\n",
       "               'score': 0.03333333507180214}),\n",
       "             ('aHR0cHM6Ly9ibG9ic3RvcmFnZXloeXVmb3FoNnlqcjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA4djMucGRm0_12',\n",
       "              {'title': 'Predicting the Expected Behavior of Agents that Learn About Agents: The CLRI Framework_chunk_12',\n",
       "               'name': '0001008v3.pdf',\n",
       "               'location': 'https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf',\n",
       "               'caption': '1. The values of ci, li, ri, and Iij cannot, in all situations, be mathematically determined from the system’s description. We have found that bounds for the ci, li, and ri values can often be determined when using reinforcement learning or supervised learning. However, the bounds are often very loose. .\\x00',\n",
       "               'index': 'cogsrch-index-files-vector',\n",
       "               'chunk': 'partially relevant to our work. Our emphasis is on finding ways to predict the\\nbehavior of MASs composed of machine-learning agents. We are only concerned\\nwith the behavior of simpler artificial programmable agents, rather than the\\ncomplex behavior of humans or the unpredictable behavior of animals.\\n\\nThe dynamics of MASs have also been studied by Kephart et. al. [18]. In\\nthis work the authors show how simple predictive agents can lead to globally\\ncyclic or chaotic behaviors. As the authors explain, the chaotic behaviors were a\\nresult of the simple predictive strategies used by the agents. Unlike our agents,\\nmost of their agents are not engaged in learning, instead they use simple fixed\\npredictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [22] has studied reinforcement learning in multi-robot domains. She\\nnotes, for example, how learning can give rise to social behaviors [23]. The\\nwork shows how robots can be individually programmed to produce certain\\ngroup behaviors. It represents a good example of the usefulness and flexibility\\nof learning agents in multi-agent domains. However, the author does not offer\\na mathematical justification for the chosen individual learning algorithms, nor\\ndoes she explain why the agents were able to converge to the global behaviors.\\nOur research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[4]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents\\nlearning about agents [27, 36]. For example, Sen and Sekaran [28] show how\\nlearning agents in simple MAS converge to system-wide optimal behavior. Their\\nagents use Q-learning or modified classifier systems in order to learn. The\\nauthors implement these agents and compare the performance of the different\\nlearning algorithms for developing agent coordination. Hu and Wellman [14, 12]\\nhave studied reinforcement learning in market-base MASs, showing how certain\\ninitial learning biases can be self-fulfilling, and how learning can be useful but\\nis affected by an agent’s models of other agents. Claus and Boutilier [6] have\\nalso carried out experimental studies of the behavior of reinforcement learning\\nagents. We have been able to use the CLRI framework to predict some of their\\nexperimental results [33]. Other researchers [32, 20, 13] have extended the basic\\nQ-learning [35] algorithm for use with MASs in an effort to either improve or\\n\\n25\\n\\n\\n\\nprove convergence to the optimal behavior.\\nWe have also successfully experimented with reinforcement learning simula-\\n\\ntions [34], but we believe that the formal treatment elucidated in these pages\\nwill shed more light into the real nature of the problem and the relative im-\\nportance of the various parameters that describe the capabilities of an agent’s\\nlearning algorithm.\\n\\n11 Limitations and Future Work\\n\\nThe CLRI framework places some constraints on the type of systems it can\\nmodel, which limits its usability. However, it is important to understand that,\\nas we remove the limitations from the CLRI framework, the dynamics of the\\nsystem become much harder to predict. In the extreme, without any limitations\\non the agents’ abilities, the system becomes a complex adaptive system, as\\nstudied by Holland [11] and others in the field of complexity. The dynamic\\nbehavior of these systems continues to be studied by complexity researchers\\nwith only modest progress. It is only by placing limitations on the system that\\nwe were able to predict the expected error of agents in the systems modeled by\\nthe CLRI framework.\\n\\nOur ongoing work involves the relaxation of some of the constraints made by\\nthe CLRI framework so that it may become more easily and widely applicable,\\nwithout making the system dynamics impossible to analyze. We are targeting\\nthree specific constraints.\\n\\n1. The values of ci, li, ri, and Iij cannot, in all situations, be mathematically\\ndetermined from the system’s description. We have found that bounds for\\nthe ci, li, and ri values can often be determined when using reinforcement\\nlearning or supervised learning. However, the bounds are often very loose.\\n',\n",
       "               'score': 0.032522473484277725}),\n",
       "             ('aHR0cHM6Ly9ibG9ic3RvcmFnZXloeXVmb3FoNnlqcjYuYmxvYi5jb3JlLndpbmRvd3MubmV0L2FyeGl2Y3MvMDAwMS8wMDAxMDA4djMucGRm0_8',\n",
       "              {'title': 'Predicting the Expected Behavior of Agents that Learn About Agents: The CLRI Framework_chunk_8',\n",
       "               'name': '0001008v3.pdf',\n",
       "               'location': 'https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf',\n",
       "               'caption': 'The sellers use a reinforcement learning algorithm with their reinforcements being the profit the agent achieved in each round, or 0 if it did not sell the good at the time.',\n",
       "               'index': 'cogsrch-index-files-vector',\n",
       "               'chunk': 'In each time step the sellers post a price and the buyer\\ndecides which of the sellers to buy from, namely, the one with the lowest bid.\\nThe sellers can bid any one of 20 prices in an effort to maximize their profits.\\nThe sellers use a reinforcement learning algorithm with their reinforcements\\nbeing the profit the agent achieved in each round, or 0 if it did not sell the good\\nat the time. In this system we had one good being sold (|W | = 1).As predicted\\nby economic theory, the price in this system settles to the sellers’ marginal cost,\\nbut it takes time to get there due to the learning inefficiencies.\\n\\nWe experimented with different αj rates2 for the reinforcement learning of\\nagent j, while keeping αi = .1 fixed, and plotted the running average of the\\nerror of agent i. A comparison is shown in Figure 9. Figure 9(a) gives the\\nexperimental results for three different values of αj . It shows i’s average error,\\nover 100 runs, as a function of time. Since both sellers start with no knowledge,\\ntheir initial actions are completely random which makes their error equal to .5.\\n\\n2\\nα is the relative weight the algorithm gives to the most recent payoff. α = 1 means that\\n\\nit will forget all previous experience and use only the latest payoff to determine what action\\nto take.\\n\\n17\\n\\n\\n\\nThen, depending on αj , i’s error will either start to go down from there or will\\nfirst go up some and then down. Eventually, i’s error gets very close to 0, as\\nthe system reaches a market equilibrium.\\n\\nWe can predict this behavior using Eq. (27). Based on the game description,\\nwe set |Ai| = |Aj | = 20, since there were 20 possible actions. We let ri =\\nrj = 1 because reinforcement learning with fixed payoffs enforces the condition\\nthat once an agent is taking the correct action it will never change its decision\\nfunction to take a different action. The agent might, however, still take a wrong\\naction but only when its exploration rate dictates it.\\n\\nWe then let Iij = Iji = .17 based on the rough calculation that each agent\\nhas an equal probability of bidding any one of the 20 prices. If ∆t\\n\\ni = 20 then\\nIji for this situation is the probability that j was also bidding 20 or above,\\ni.e., 1/20, times the probability that j’s new price is lower than 20, i.e. 19/20.\\nSimilarly, if ∆t\\n\\ni = 19 then Iji is equal to 2/20 times 18/20. The average of all\\nof these probabilities is .17. A more precise calculation of the impact would\\nrequire us to find it via experimentation by actually running the system.\\n\\nFinally, we chose li = lj = ci = cj = .005 for the first curve (i.e., the one that\\ncompares with αj = .1). We knew that for such a low αj the learning and change\\nrate should be the same. The actual value was chosen via experimentation. The\\nresulting curve is shown in Figure 9(b). At this moment, we do not possess a\\nformal way of deriving learning and change rates from α-rates.\\n\\nFor the second curve (αj = .3) we knew that, since only αj had changed\\nfrom the first experiment, we should only change lj and cj . In fact, these\\ntwo values should only be increased. We found their exact values, again by\\nexperimentation, to be lj = .04, cj = .4. For the third curve we found the\\nvalues to be lj = .055, cj = .8.\\n\\nOne difference we notice between the experimental and the theoretical results\\nis that the experimental results show a longer delay before the error starts to\\ndecrease. We attribute this delay to the agent’s initially high exploration rate.\\nThat is, the agents initially start by taking all random actions but progressively\\nreduce this rate of exploration. As the exploration rate decreases the discrepancy\\nbetween our theoretical predictions and experimental results is reduced.\\n\\nIn summary, while it is true that we found lj and cj by experimentation,\\nall the other values were calculated from the description of the problem. Even\\nthe relative values of lj and cj follow the intuitive relation with αj that, as\\nαj increases so does lj and (even more) cj . Section 9 shows how to calculate\\nlower bounds on the learning rate. We believe that this experiment provides\\nsolid evidence that our theory can be used to approximately determine the\\nquantitative behaviors of MASs with learning agents.\\n\\n18\\n\\n\\n\\n0.5\\n\\n0.55\\n\\n0.6\\n\\n0.65\\n\\n0.7\\n\\n0.75\\n\\n0.8\\n\\n0.85\\n\\n0.9\\n\\n0.95\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\nP\\nro\\n\\nba\\nbi\\n\\nlit\\ny \\n\\nof\\n c\\n\\nho\\nos\\n\\nin\\ng \\n\\nan\\n o\\n\\npt\\nim\\n\\nal\\n jo\\n\\nin\\nt a\\n\\nct\\nio\\n\\nn\\n\\nNumber of Interactions\\n\\nChoosing optimal joint actions\\n\\nIndependent learners\\nJoint action learners\\n\\n(a) Experiment\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\ntime\\n\\n1\\n-\\ne\\nr\\nr\\no\\nr\\n\\nli = .1\\n\\n(b) Theory\\n\\nFigure 10: Comparing theory (b) with results from [6] (a).\\n\\n8 Application of our Theory to Experiments in\\n\\nthe Literature\\n\\nIn this section we show how we can apply our theory to experimental results\\nfound in the AI and MAS literature. While we will often not be able to com-\\npletely reproduce the authors’ results exactly, we believe that being able to\\n',\n",
       "               'score': 0.03125}),\n",
       "             ('snqdma0s_0',\n",
       "              {'title': 'Correcting errors in synthetic DNA through consensus shuffling_chunk_0',\n",
       "               'name': 'metadata.csv',\n",
       "               'location': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/',\n",
       "               'caption': 'Although efficient methods exist to assemble synthetic oligonucleotides into genes and genomes, these suffer from the presence of 1–3 random errors/kb of DNA. Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA.',\n",
       "               'index': 'cogsrch-index-csv-vector',\n",
       "               'chunk': 'Although efficient methods exist to assemble synthetic oligonucleotides into genes and genomes, these suffer from the presence of 1–3 random errors/kb of DNA. Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population. The DNA is fragmented, and mismatched fragments are removed upon binding to an immobilized mismatch binding protein (MutS). PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population. We show that two iterations of consensus shuffling improved a population of synthetic green fluorescent protein (GFPuv) clones from ∼60 to >90% fluorescent, and decreased errors 3.5- to 4.3-fold to final values of ∼1 error per 3500 bp. In addition, two iterations of consensus shuffling corrected a population of GFPuv clones where all members were non-functional, to a population where 82% of clones were fluorescent. Consensus shuffling should facilitate the rapid and accurate synthesis of long DNA sequences.',\n",
       "               'score': 0.03030998818576336}),\n",
       "             ('TWFkZV9Ub19TdGljay5wZGYxODE=',\n",
       "              {'title': 'Made_To_Stick.pdf_page_181',\n",
       "               'name': 'Made_To_Stick.pdf',\n",
       "               'location': 'https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf',\n",
       "               'caption': 'WIIFY. Cable television in Tempe: Visualizing what it could do for you. Avoid Maslow\\'s basement: our false assumption that other people are  baser than we are. Floyd Lee and his Iraq mess tent : \"I\\'m in charge of  morale.\" Appeal to identity. The firemen who rejected the popcom popper. Underst and how people  make decisions based on identity.',\n",
       "               'index': 'cogsrch-index-books-vector',\n",
       "               'chunk': '256                                   EASY REFERENC E GUIDE  \\nsmoking campaign: What made kids care was not healt h concerns but \\nanticorporate rebellion.  \\nUse the power of association. \\nThe need to fight semantic stretch: the diluted mea ning of \"relativity\" \\nand why \"unique\" isn\\'t unique anymore. Transforming  \"sportsmanship\" \\ninto \"honoring the game.\"  \\nAppeal to self-interest (and not just base self-int erest). \\nMail-order ads — \"They laughed when I sat down at the piano. . . .\" \\nWIIFY. Cable television in Tempe: Visualizing what it could do for you. \\nAvoid Maslow\\'s basement: our false assumption that other people are \\nbaser than we are. Floyd Lee and his Iraq mess tent : \"I\\'m in charge of \\nmorale.\"  \\nAppeal to identity. \\nThe firemen who rejected the popcom popper. Underst and how people \\nmake decisions based on identity. (Who am I? What k ind of situation is \\nthis? And what do people like me do in this kind of  situation?) Clinic: \\nWhy study algebra? Don\\'t mess with Texas: Texans don\\'t litter. Don\\'t \\nforget the Curse of Knowledge—don\\'t assume, like th e defenders of the \\nduo piano, that others care at the same level that you do.  \\n6. Stories \\nGet people to act. \\nStories as simulation (tell people how to act). \\nThe day the heart monitor lied: how the nurse acted . Shop talk at Xerox: \\nhow the repairman acted. Visualizing \"how I got her e\": simulating prob- \\nlems to solve them. Use stories as flight simulator s. Clinic: Dealing with \\nproblem students. \\nEASY REFERENCE GUIDE                                    257  \\nStories as inspiration (give people energy to act).  \\nJared, the 425-pound fast-food dieter. How to spot inspiring stories. Look \\nfor three key plots: Challenge (to overcome obstacles), Connection (to g et \\nalong or reconnect), Creativity (to inspire a new w ay of thinking). Tell a \\nspringboard story: a story that helps people see how an existing probl em \\nmight change. Stephen Denning at the World Bank: a health worker in \\nZambia. You can extract a moral from a story, but y ou can\\'t extract a \\nstory from a moral. Why speakers got mad when peopl e boiled down \\ntheir presentations to stories.  \\nWhat Sticks. ',\n",
       "               'score': 0.027970679104328156})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment the below line if you want to inspect the ordered results\n",
    "ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b2a3595-c3b7-4376-b9c5-0db7a42b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "top_docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location+os.environ['BLOB_SAS_TOKEN']}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(top_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db52ccb4-3112-4c48-ab95-5b6816dd51c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of elements in top_docs: 5. Here they are:\n",
      "[Document(page_content='predictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [1995] has studied reinforcement learning in multi-robot domains.\\nShe notes, for example, how learning can give rise to social behaviors (Matarić\\n[1997]). The work shows how robots can be individually programmed to pro-\\nduce certain group behaviors. It represents a good example of the usefulness\\nand flexibility of learning agents in multi-agent domains. However, the author\\ndoes not offer a mathematical justification for the chosen individual learning\\nalgorithms, nor does she explain why the agents were able to converge to the\\nglobal behaviors. Our research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[1997]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents learn-\\ning about agents (Sen [1996], Weiß [1997]). For example, Sen and Sekaran [1998]\\nshow how learning agents in simple MAS converge to system-wide optimal be-\\nhavior. Their agents use Q-learning or modified classifier systems in order to\\nlearn. The authors implement these agents and compare the performance of the\\ndifferent learning algorithms for developing agent coordination. Hu and Well-\\nman [1998b, 1996] have studied reinforcement learning in market-base MASs,\\nshowing how certain initial learning biases can be self-fulfilling, and how learn-\\ning can be useful but is affected by an agent’s models of other agents. Claus\\nand Boutilier [1997] have also carried out experimental studies of the behavior of\\nreinforcement learning agents. We have been able to use the CLRI framework\\nto predict some of their experimental results Vidal [1998]. Other researchers\\nsuch as Stone and Veloso [1999], Littman [1994], and Hu and Wellman [1998a]\\nhave extended the basic Q-learning Watkins and Dayan [1992] algorithm for use\\nwith MASs in an effort to either improve or prove convergence to the optimal\\nbehavior.\\n\\nWe have also successfully experimented with reinforcement learning simu-\\nlations (Vidal and Durfee [1998]), but we believe that the formal treatment\\nelucidated in these pages will shed more light into the real nature of the prob-\\nlem and the relative importance of the various parameters that describe the\\n\\n25\\n\\n\\n\\ncapabilities of an agent’s learning algorithm.\\n\\n11 Summary\\n\\nWe have presented a framework for studying and predicting the behavior of\\nMASs composed of learning agents. We believe that this framework captures the\\nmost important parameters that describe an agents’ learning and the system’s\\nrules of encounter. Various comparisons between the framework’s predictions\\nand experimental results were given. These comparisons showed that the theo-\\nretical predictions closely match our experimental results and the experimental\\nresults published by others. Our success in reproducing these results allows us\\nto confidently state the effectiveness and accuracy of our theory in predicting\\nthe expected error of machine learning agents in MASs.\\n\\nSince our theory describes an agent’s behavior at a high-level (i.e., the agent’s\\nerror), it is not capable of making system-specific predictions (e.g., predicting\\nthe particular actions that are favored). These types of system-specific predic-\\ntions can only be arrived at by the traditional method of implementing popula-\\ntions of such agents and testing their behaviors. However, we expect that there\\nwill be times when the predictions from our theory will be enough to answer a\\ndesigner’s questions. A MAS designer that only needs to determine how “good”\\nthe agent’s behavior will be could probably use the CLRI framework. A de-\\nsigner that needs to know which particular emergent behaviors will be favored\\nby his agents will need to implement the agents.\\n\\nFinally, while we have given some examples as to how learning rates can be\\ndetermined for particular machine learning implementations, we do not have\\nany general method for determining these rates. However, we showed how to\\nuse the sample complexity of a learning problem to determine a lower bound on\\nthe learning rate of a consistent learning agent. This bound is useful for quickly\\n', metadata={'source': 'https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D'}), Document(page_content='partially relevant to our work. Our emphasis is on finding ways to predict the\\nbehavior of MASs composed of machine-learning agents. We are only concerned\\nwith the behavior of simpler artificial programmable agents, rather than the\\ncomplex behavior of humans or the unpredictable behavior of animals.\\n\\nThe dynamics of MASs have also been studied by Kephart et. al. [18]. In\\nthis work the authors show how simple predictive agents can lead to globally\\ncyclic or chaotic behaviors. As the authors explain, the chaotic behaviors were a\\nresult of the simple predictive strategies used by the agents. Unlike our agents,\\nmost of their agents are not engaged in learning, instead they use simple fixed\\npredictive strategies, such as “if the state of the world was x ten time units\\nbefore, then it will be x next time so take action a”. The authors later show\\nhow learning can be used to eliminate these chaotic global fluctuations.\\n\\nMatarić [22] has studied reinforcement learning in multi-robot domains. She\\nnotes, for example, how learning can give rise to social behaviors [23]. The\\nwork shows how robots can be individually programmed to produce certain\\ngroup behaviors. It represents a good example of the usefulness and flexibility\\nof learning agents in multi-agent domains. However, the author does not offer\\na mathematical justification for the chosen individual learning algorithms, nor\\ndoes she explain why the agents were able to converge to the global behaviors.\\nOur research hopes to provide the first steps in this direction.\\n\\nOne particularly interesting approach is taken by Carmel and Markovitch\\n[4]. They work on model-based learning, that is, agents build models of other\\nagents via observations. They use models based on finite state machines. The\\nauthors show how some of these models can be effectively learned via observation\\nof the other agent’s actions. The authors concentrate on the development of\\nlearning algorithms that would let one agent learn a finite-state machine model\\nof another agent. They have not considered the case where two or more agents\\nare simultaneously learning about each other, which we study in this article.\\nHowever, their work is more general in the sense that they model agents as\\nstate machines, rather than the state-action pairs we use.\\n\\nFinally, a lot of experimental work has been done in the area of agents\\nlearning about agents [27, 36]. For example, Sen and Sekaran [28] show how\\nlearning agents in simple MAS converge to system-wide optimal behavior. Their\\nagents use Q-learning or modified classifier systems in order to learn. The\\nauthors implement these agents and compare the performance of the different\\nlearning algorithms for developing agent coordination. Hu and Wellman [14, 12]\\nhave studied reinforcement learning in market-base MASs, showing how certain\\ninitial learning biases can be self-fulfilling, and how learning can be useful but\\nis affected by an agent’s models of other agents. Claus and Boutilier [6] have\\nalso carried out experimental studies of the behavior of reinforcement learning\\nagents. We have been able to use the CLRI framework to predict some of their\\nexperimental results [33]. Other researchers [32, 20, 13] have extended the basic\\nQ-learning [35] algorithm for use with MASs in an effort to either improve or\\n\\n25\\n\\n\\n\\nprove convergence to the optimal behavior.\\nWe have also successfully experimented with reinforcement learning simula-\\n\\ntions [34], but we believe that the formal treatment elucidated in these pages\\nwill shed more light into the real nature of the problem and the relative im-\\nportance of the various parameters that describe the capabilities of an agent’s\\nlearning algorithm.\\n\\n11 Limitations and Future Work\\n\\nThe CLRI framework places some constraints on the type of systems it can\\nmodel, which limits its usability. However, it is important to understand that,\\nas we remove the limitations from the CLRI framework, the dynamics of the\\nsystem become much harder to predict. In the extreme, without any limitations\\non the agents’ abilities, the system becomes a complex adaptive system, as\\nstudied by Holland [11] and others in the field of complexity. The dynamic\\nbehavior of these systems continues to be studied by complexity researchers\\nwith only modest progress. It is only by placing limitations on the system that\\nwe were able to predict the expected error of agents in the systems modeled by\\nthe CLRI framework.\\n\\nOur ongoing work involves the relaxation of some of the constraints made by\\nthe CLRI framework so that it may become more easily and widely applicable,\\nwithout making the system dynamics impossible to analyze. We are targeting\\nthree specific constraints.\\n\\n1. The values of ci, li, ri, and Iij cannot, in all situations, be mathematically\\ndetermined from the system’s description. We have found that bounds for\\nthe ci, li, and ri values can often be determined when using reinforcement\\nlearning or supervised learning. However, the bounds are often very loose.\\n', metadata={'source': 'https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D'}), Document(page_content='In each time step the sellers post a price and the buyer\\ndecides which of the sellers to buy from, namely, the one with the lowest bid.\\nThe sellers can bid any one of 20 prices in an effort to maximize their profits.\\nThe sellers use a reinforcement learning algorithm with their reinforcements\\nbeing the profit the agent achieved in each round, or 0 if it did not sell the good\\nat the time. In this system we had one good being sold (|W | = 1).As predicted\\nby economic theory, the price in this system settles to the sellers’ marginal cost,\\nbut it takes time to get there due to the learning inefficiencies.\\n\\nWe experimented with different αj rates2 for the reinforcement learning of\\nagent j, while keeping αi = .1 fixed, and plotted the running average of the\\nerror of agent i. A comparison is shown in Figure 9. Figure 9(a) gives the\\nexperimental results for three different values of αj . It shows i’s average error,\\nover 100 runs, as a function of time. Since both sellers start with no knowledge,\\ntheir initial actions are completely random which makes their error equal to .5.\\n\\n2\\nα is the relative weight the algorithm gives to the most recent payoff. α = 1 means that\\n\\nit will forget all previous experience and use only the latest payoff to determine what action\\nto take.\\n\\n17\\n\\n\\n\\nThen, depending on αj , i’s error will either start to go down from there or will\\nfirst go up some and then down. Eventually, i’s error gets very close to 0, as\\nthe system reaches a market equilibrium.\\n\\nWe can predict this behavior using Eq. (27). Based on the game description,\\nwe set |Ai| = |Aj | = 20, since there were 20 possible actions. We let ri =\\nrj = 1 because reinforcement learning with fixed payoffs enforces the condition\\nthat once an agent is taking the correct action it will never change its decision\\nfunction to take a different action. The agent might, however, still take a wrong\\naction but only when its exploration rate dictates it.\\n\\nWe then let Iij = Iji = .17 based on the rough calculation that each agent\\nhas an equal probability of bidding any one of the 20 prices. If ∆t\\n\\ni = 20 then\\nIji for this situation is the probability that j was also bidding 20 or above,\\ni.e., 1/20, times the probability that j’s new price is lower than 20, i.e. 19/20.\\nSimilarly, if ∆t\\n\\ni = 19 then Iji is equal to 2/20 times 18/20. The average of all\\nof these probabilities is .17. A more precise calculation of the impact would\\nrequire us to find it via experimentation by actually running the system.\\n\\nFinally, we chose li = lj = ci = cj = .005 for the first curve (i.e., the one that\\ncompares with αj = .1). We knew that for such a low αj the learning and change\\nrate should be the same. The actual value was chosen via experimentation. The\\nresulting curve is shown in Figure 9(b). At this moment, we do not possess a\\nformal way of deriving learning and change rates from α-rates.\\n\\nFor the second curve (αj = .3) we knew that, since only αj had changed\\nfrom the first experiment, we should only change lj and cj . In fact, these\\ntwo values should only be increased. We found their exact values, again by\\nexperimentation, to be lj = .04, cj = .4. For the third curve we found the\\nvalues to be lj = .055, cj = .8.\\n\\nOne difference we notice between the experimental and the theoretical results\\nis that the experimental results show a longer delay before the error starts to\\ndecrease. We attribute this delay to the agent’s initially high exploration rate.\\nThat is, the agents initially start by taking all random actions but progressively\\nreduce this rate of exploration. As the exploration rate decreases the discrepancy\\nbetween our theoretical predictions and experimental results is reduced.\\n\\nIn summary, while it is true that we found lj and cj by experimentation,\\nall the other values were calculated from the description of the problem. Even\\nthe relative values of lj and cj follow the intuitive relation with αj that, as\\nαj increases so does lj and (even more) cj . Section 9 shows how to calculate\\nlower bounds on the learning rate. We believe that this experiment provides\\nsolid evidence that our theory can be used to approximately determine the\\nquantitative behaviors of MASs with learning agents.\\n\\n18\\n\\n\\n\\n0.5\\n\\n0.55\\n\\n0.6\\n\\n0.65\\n\\n0.7\\n\\n0.75\\n\\n0.8\\n\\n0.85\\n\\n0.9\\n\\n0.95\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\nP\\nro\\n\\nba\\nbi\\n\\nlit\\ny \\n\\nof\\n c\\n\\nho\\nos\\n\\nin\\ng \\n\\nan\\n o\\n\\npt\\nim\\n\\nal\\n jo\\n\\nin\\nt a\\n\\nct\\nio\\n\\nn\\n\\nNumber of Interactions\\n\\nChoosing optimal joint actions\\n\\nIndependent learners\\nJoint action learners\\n\\n(a) Experiment\\n\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n0 5 10 15 20 25 30 35 40 45 50\\n\\ntime\\n\\n1\\n-\\ne\\nr\\nr\\no\\nr\\n\\nli = .1\\n\\n(b) Theory\\n\\nFigure 10: Comparing theory (b) with results from [6] (a).\\n\\n8 Application of our Theory to Experiments in\\n\\nthe Literature\\n\\nIn this section we show how we can apply our theory to experimental results\\nfound in the AI and MAS literature. While we will often not be able to com-\\npletely reproduce the authors’ results exactly, we believe that being able to\\n', metadata={'source': 'https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D'}), Document(page_content='Although efficient methods exist to assemble synthetic oligonucleotides into genes and genomes, these suffer from the presence of 1–3 random errors/kb of DNA. Here, we introduce a new method termed consensus shuffling and demonstrate its use to significantly reduce random errors in synthetic DNA. In this method, errors are revealed as mismatches by re-hybridization of the population. The DNA is fragmented, and mismatched fragments are removed upon binding to an immobilized mismatch binding protein (MutS). PCR assembly of the remaining fragments yields a new population of full-length sequences enriched for the consensus sequence of the input population. We show that two iterations of consensus shuffling improved a population of synthetic green fluorescent protein (GFPuv) clones from ∼60 to >90% fluorescent, and decreased errors 3.5- to 4.3-fold to final values of ∼1 error per 3500 bp. In addition, two iterations of consensus shuffling corrected a population of GFPuv clones where all members were non-functional, to a population where 82% of clones were fluorescent. Consensus shuffling should facilitate the rapid and accurate synthesis of long DNA sequences.', metadata={'source': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D'}), Document(page_content='256                                   EASY REFERENC E GUIDE  \\nsmoking campaign: What made kids care was not healt h concerns but \\nanticorporate rebellion.  \\nUse the power of association. \\nThe need to fight semantic stretch: the diluted mea ning of \"relativity\" \\nand why \"unique\" isn\\'t unique anymore. Transforming  \"sportsmanship\" \\ninto \"honoring the game.\"  \\nAppeal to self-interest (and not just base self-int erest). \\nMail-order ads — \"They laughed when I sat down at the piano. . . .\" \\nWIIFY. Cable television in Tempe: Visualizing what it could do for you. \\nAvoid Maslow\\'s basement: our false assumption that other people are \\nbaser than we are. Floyd Lee and his Iraq mess tent : \"I\\'m in charge of \\nmorale.\"  \\nAppeal to identity. \\nThe firemen who rejected the popcom popper. Underst and how people \\nmake decisions based on identity. (Who am I? What k ind of situation is \\nthis? And what do people like me do in this kind of  situation?) Clinic: \\nWhy study algebra? Don\\'t mess with Texas: Texans don\\'t litter. Don\\'t \\nforget the Curse of Knowledge—don\\'t assume, like th e defenders of the \\nduo piano, that others care at the same level that you do.  \\n6. Stories \\nGet people to act. \\nStories as simulation (tell people how to act). \\nThe day the heart monitor lied: how the nurse acted . Shop talk at Xerox: \\nhow the repairman acted. Visualizing \"how I got her e\": simulating prob- \\nlems to solve them. Use stories as flight simulator s. Clinic: Dealing with \\nproblem students. \\nEASY REFERENCE GUIDE                                    257  \\nStories as inspiration (give people energy to act).  \\nJared, the 425-pound fast-food dieter. How to spot inspiring stories. Look \\nfor three key plots: Challenge (to overcome obstacles), Connection (to g et \\nalong or reconnect), Creativity (to inspire a new w ay of thinking). Tell a \\nspringboard story: a story that helps people see how an existing probl em \\nmight change. Stephen Denning at the World Bank: a health worker in \\nZambia. You can extract a moral from a story, but y ou can\\'t extract a \\nstory from a moral. Why speakers got mad when peopl e boiled down \\ntheir presentations to stories.  \\nWhat Sticks. ', metadata={'source': 'https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D'})]\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the below line if you want to inspect the ordered top_docs\n",
    "print(f\"Nr. of elements in top_docs: {len(top_docs)}. Here they are:\\n{top_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c26d7540-feb8-4581-849e-003f4bf2a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt token count: 2464\n",
      "Max Completion Token count: 1000\n",
      "Combined docs (context) token count: 4080\n",
      "--------\n",
      "Requested token count: 7544\n",
      "Token limit for gpt-35-turbo-16k : 16384\n",
      "Chain Type selected: stuff\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(top_docs)>0):\n",
    "    tokens_limit   = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    prompt_tokens  = num_tokens_from_string(COMBINE_CHAT_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
    "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
    "    \n",
    "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
    "    \n",
    "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
    "    \n",
    "    print(\"System prompt token count:\", prompt_tokens)\n",
    "    print(\"Max Completion Token count:\", COMPLETION_TOKENS)\n",
    "    print(\"Combined docs (context) token count:\", context_tokens)\n",
    "    print(\"--------\")\n",
    "    print(\"Requested token count:\", requested_tokens)\n",
    "    print(\"Token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Chain Type selected:\", chain_type)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcc376-6ee2-4812-b182-52571cd01241",
   "metadata": {},
   "source": [
    "## Using load_qa_with_sources_chain\n",
    "The *get_answer* function in the next cell is equivalent to the next code, in case **chain_type = map_reduce**:\n",
    "\n",
    "```\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from common.prompts import (COMBINE_PROMPT, COMBINE_QUESTION_PROMPT)\n",
    "\n",
    "llm35 = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
    "chain = load_qa_with_sources_chain(llm35, chain_type=\"map_reduce\",\n",
    "                                   combine_prompt = COMBINE_PROMPT,\n",
    "                                   question_prompt = COMBINE_QUESTION_PROMPT,\n",
    "                                   return_intermediate_steps = True)\n",
    "\n",
    "answer = chain( {\"input_documents\": top_docs, \"question\": QUESTION, \"language\": \"it\"}, return_only_outputs=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce6efa9-2b8f-4810-904d-5986b4ae0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Some use cases for reinforcement learning include:\n",
       "1. Predictive strategies to eliminate chaotic global fluctuations [1].\n",
       "2. Learning in multi-robot domains to give rise to social behaviors and program robots for certain group behaviors [2].\n",
       "3. Model-based learning, where agents build models of other agents via observations [3].\n",
       "4. Learning agents in multi-agent systems to converge to system-wide optimal behavior [4].\n",
       "5. Reinforcement learning in market-based multi-agent systems [4].\n",
       "6. Correcting errors in synthetic DNA sequences through consensus shuffling [5].\n",
       "7. Using storytelling to inspire and motivate action [6].\n",
       "\n",
       "References:\n",
       "[1] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[2] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[3] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[4] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[5] Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[6] Source: https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get the answer\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafbbb47-d31a-4720-b6bb-0febb83143fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Some use cases for reinforcement learning include:\n",
       "1. Predictive strategies to eliminate chaotic global fluctuations [1].\n",
       "2. Learning in multi-robot domains to give rise to social behaviors and program robots for certain group behaviors [2].\n",
       "3. Model-based learning, where agents build models of other agents via observations [3].\n",
       "4. Learning agents in multi-agent systems to converge to system-wide optimal behavior [4].\n",
       "5. Reinforcement learning in market-based multi-agent systems [4].\n",
       "6. Correcting errors in synthetic DNA sequences through consensus shuffling [5].\n",
       "7. Using storytelling to inspire and motivate action [6].\n",
       "\n",
       "References:\n",
       "[1] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[2] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[3] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[4] Source: https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v3.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[5] Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\n",
       "[6] Source: https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27501f1b-7db0-4ee3-9cb1-e609254ffa3d",
   "metadata": {},
   "source": [
    "And if we ask the follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cf5b323-3b9c-479b-8502-acfc4f7915dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I couldn't find any information related to the main points of our conversation in the provided extracts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# follow-up questions don't work!\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fa6e6-226c-400f-a504-30255385f43b",
   "metadata": {},
   "source": [
    "You might get a different response from above, but it doesn't matter what response you get, it will be based on the context given, not on previous answers.\n",
    "\n",
    "Until now we just have the same as the prior Notebook 03: results from Azure Search enhanced by OpenAI model, with no memory\n",
    "\n",
    "**Now let's add memory to it:**\n",
    "\n",
    "Reference: https://python.langchain.com/docs/modules/memory/how_to/adding_memory_chain_multiple_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d98b876e-d264-48ae-b5ed-9801d6a9152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has several use cases. Here are some examples:\n",
       "\n",
       "1. Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where robots can be individually programmed to produce certain group behaviors. This allows for the emergence of social behaviors among the robots<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "2. Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "3. Market-based multi-agent systems: Reinforcement learning has been studied in market-based multi-agent systems, where agents learn to coordinate their actions to achieve system-wide optimal behavior. Different learning algorithms, such as Q-learning and modified classifier systems, have been used in these systems<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "These are just a few examples of the use cases for reinforcement learning. It is a versatile approach that can be applied in various domains to enable learning and decision-making in autonomous agents. \n",
       "\n",
       "Let me know if there's anything else I can help with!\n",
       "\n",
       "References:\n",
       "[1]<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">Source</a></sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\")\n",
    "\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type,\n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b5d4e63-534b-4a0e-8791-7e543a2a0bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning?\\nAI: Reinforcement learning has several use cases. Here are some examples:\\n\\n1. Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where robots can be individually programmed to produce certain group behaviors. This allows for the emergence of social behaviors among the robots<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n2. Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n3. Market-based multi-agent systems: Reinforcement learning has been studied in market-based multi-agent systems, where agents learn to coordinate their actions to achieve system-wide optimal behavior. Different learning algorithms, such as Q-learning and modified classifier systems, have been used in these systems<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\nThese are just a few examples of the use cases for reinforcement learning. It is a versatile approach that can be applied in various domains to enable learning and decision-making in autonomous agents. \\n\\nLet me know if there\\'s anything else I can help with!\\n\\nReferences:\\n[1]<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">Source</a></sup>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf28927b-d9ee-4412-bb07-13e055e832a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, we discussed the use cases for reinforcement learning<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>. Here are the main points of our conversation:\n",
       "\n",
       "- Reinforcement learning has been studied in multi-robot domains, where robots can be individually programmed to produce certain group behaviors. This allows for the emergence of social behaviors among the robots.\n",
       "- Model-based learning is another use case, where agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines.\n",
       "- Reinforcement learning has also been studied in market-based multi-agent systems, where agents learn to coordinate their actions to achieve system-wide optimal behavior. Different learning algorithms, such as Q-learning and modified classifier systems, have been used in these systems.\n",
       "\n",
       "These are just a few examples of the use cases for reinforcement learning. It is a versatile approach that can be applied in various domains to enable learning and decision-making in autonomous agents.\n",
       "\n",
       "References:\n",
       "[1]<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">Source</a></sup>\n",
       "\n",
       "Let me know if there's anything else I can help with!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3830b0b8-0ca2-4d0a-9747-f6273368002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I couldn't find any relevant information from the extracted parts to answer your question. Is there anything else I can help you with?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e732b-3c8c-4df3-8fcb-c3d01e7bec74",
   "metadata": {},
   "source": [
    "You might get a different answer on the above cell, and it is ok, this bot is not yet well configured to answer any question that is not related to its knowledge base, including salutations.\n",
    "\n",
    "Let's check our memory to see that it's keeping the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1279692c-7eb0-4300-8a66-c7025f02c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning?\\nAI: Reinforcement learning has several use cases. Here are some examples:\\n\\n1. Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where robots can be individually programmed to produce certain group behaviors. This allows for the emergence of social behaviors among the robots<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n2. Model-based learning: In model-based learning, agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n3. Market-based multi-agent systems: Reinforcement learning has been studied in market-based multi-agent systems, where agents learn to coordinate their actions to achieve system-wide optimal behavior. Different learning algorithms, such as Q-learning and modified classifier systems, have been used in these systems<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\nThese are just a few examples of the use cases for reinforcement learning. It is a versatile approach that can be applied in various domains to enable learning and decision-making in autonomous agents. \\n\\nLet me know if there\\'s anything else I can help with!\\n\\nReferences:\\n[1]<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">Source</a></sup>\\nHuman: Give me the main points of our conversation\\nAI: Based on our conversation, we discussed the use cases for reinforcement learning<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>. Here are the main points of our conversation:\\n\\n- Reinforcement learning has been studied in multi-robot domains, where robots can be individually programmed to produce certain group behaviors. This allows for the emergence of social behaviors among the robots.\\n- Model-based learning is another use case, where agents build models of other agents via observations. This approach has been used to effectively learn models based on finite state machines.\\n- Reinforcement learning has also been studied in market-based multi-agent systems, where agents learn to coordinate their actions to achieve system-wide optimal behavior. Different learning algorithms, such as Q-learning and modified classifier systems, have been used in these systems.\\n\\nThese are just a few examples of the use cases for reinforcement learning. It is a versatile approach that can be applied in various domains to enable learning and decision-making in autonomous agents.\\n\\nReferences:\\n[1]<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">Source</a></sup>\\n\\nLet me know if there\\'s anything else I can help with!\\nHuman: Thank you\\nAI: I\\'m sorry, but I couldn\\'t find any relevant information from the extracted parts to answer your question. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87405173",
   "metadata": {},
   "source": [
    "## Using CosmosDB as persistent memory\n",
    "\n",
    "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wisg to provide recommendations. \n",
    "\n",
    "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
    "We will use a class in LangChain use CosmosDBChatMessageHistory, see [HERE](https://python.langchain.com/en/latest/_modules/langchain/memory/chat_message_histories/cosmos_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7131daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CosmosDB instance from langchain cosmos class.\n",
    "cosmos = CosmosDBChatMessageHistory(\n",
    "    cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "    cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
    "    cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "    connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "    session_id=\"Agent-Test-Session\" + str(random.randint(1, 1000)),\n",
    "    user_id=\"Agent-Test-User\" + str(random.randint(1, 1000))\n",
    "    )\n",
    "\n",
    "# prepare the cosmosdb instance\n",
    "cosmos.prepare_cosmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Memory Object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\",chat_memory=cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27ceb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has various use cases in different domains. Here are some examples:\n",
       "\n",
       "1. Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where robots can be individually programmed to produce certain group behaviors. This can lead to social behaviors among robots and enhance their coordination and cooperation<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "2. Model-based learning: In model-based learning, agents build models of other agents through observations. This approach allows agents to effectively learn and predict the actions of other agents. For example, agents can learn a finite-state machine model of another agent through observation of its actions<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "3. Market-based multi-agent systems: Reinforcement learning has been applied to market-based multi-agent systems, where agents learn to make optimal decisions in market environments. This can include learning coordination strategies and adapting to changing market conditions<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "4. Synthetic DNA synthesis: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. The consensus shuffling method, which involves re-hybridization and removal of mismatched fragments, has been shown to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[2]</a></sup>.\n",
       "\n",
       "5. Communication and persuasion: Reinforcement learning principles can be applied to communication and persuasion techniques. For example, understanding the power of association, appealing to self-interest, and using stories as simulation and inspiration can help in effectively delivering messages and influencing behavior<sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[3]</a></sup>.\n",
       "\n",
       "These examples highlight the versatility and potential applications of reinforcement learning across different domains. Let me know if there's anything else I can help with!<"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing using our Question\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a5ff826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the main points of our conversation:\n",
       "\n",
       "1. Reinforcement learning has various use cases in different domains, such as multi-robot domains, model-based learning, market-based multi-agent systems, synthetic DNA synthesis, and communication and persuasion<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00:00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "2. In multi-robot domains, reinforcement learning can lead to social behaviors among robots and enhance their coordination and cooperation.\n",
       "\n",
       "3. Model-based learning involves agents building models of other agents through observations, allowing them to effectively learn and predict the actions of other agents.\n",
       "\n",
       "4. Reinforcement learning has been applied to market-based multi-agent systems, where agents learn to make optimal decisions in market environments.\n",
       "\n",
       "5. Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis through a method called consensus shuffling.\n",
       "\n",
       "6. Reinforcement learning principles can be applied to communication and persuasion techniques, such as understanding the power of association, appealing to self-interest, and using stories as simulation and inspiration.\n",
       "\n",
       "These points highlight the versatility and potential applications of reinforcement learning in various domains. Let me know if there's anything else I can assist you with!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be1620fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The main points of our conversation are:\n",
       "\n",
       "1. Reinforcement learning has various use cases in different domains, such as multi-robot domains, model-based learning, market-based multi-agent systems, synthetic DNA synthesis, and communication and persuasion<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00:00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\n",
       "\n",
       "2. In multi-robot domains, reinforcement learning can lead to social behaviors among robots and enhance their coordination and cooperation.\n",
       "\n",
       "3. Model-based learning involves agents building models of other agents through observations, allowing them to effectively learn and predict the actions of other agents.\n",
       "\n",
       "4. Reinforcement learning has been applied to market-based multi-agent systems, where agents learn to make optimal decisions in market environments.\n",
       "\n",
       "5. Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis through a method called consensus shuffling<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[2]</a></sup>.\n",
       "\n",
       "6. Reinforcement learning principles can be applied to communication and persuasion techniques, such as understanding the power of association, appealing to self-interest, and using stories as simulation and inspiration<sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00:00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[3]</a></sup>.\n",
       "\n",
       "These points highlight the versatility and potential applications of reinforcement learning in various domains. Let me know if there's anything else I can assist you with!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5ac98",
   "metadata": {},
   "source": [
    "Let's check our Azure CosmosDB to see the whole conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1d7688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me some use cases for reinforcement learning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Reinforcement learning has various use cases in different domains. Here are some examples:\\n\\n1. Multi-robot domains: Reinforcement learning has been studied in multi-robot domains, where robots can be individually programmed to produce certain group behaviors. This can lead to social behaviors among robots and enhance their coordination and cooperation<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n2. Model-based learning: In model-based learning, agents build models of other agents through observations. This approach allows agents to effectively learn and predict the actions of other agents. For example, agents can learn a finite-state machine model of another agent through observation of its actions<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n3. Market-based multi-agent systems: Reinforcement learning has been applied to market-based multi-agent systems, where agents learn to make optimal decisions in market environments. This can include learning coordination strategies and adapting to changing market conditions<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n4. Synthetic DNA synthesis: Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis. The consensus shuffling method, which involves re-hybridization and removal of mismatched fragments, has been shown to improve the accuracy and efficiency of synthesizing long DNA sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[2]</a></sup>.\\n\\n5. Communication and persuasion: Reinforcement learning principles can be applied to communication and persuasion techniques. For example, understanding the power of association, appealing to self-interest, and using stories as simulation and inspiration can help in effectively delivering messages and influencing behavior<sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[3]</a></sup>.\\n\\nThese examples highlight the versatility and potential applications of reinforcement learning across different domains. Let me know if there\\'s anything else I can help with!<', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Give me the main points of our conversation', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Here are the main points of our conversation:\\n\\n1. Reinforcement learning has various use cases in different domains, such as multi-robot domains, model-based learning, market-based multi-agent systems, synthetic DNA synthesis, and communication and persuasion<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00:00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n2. In multi-robot domains, reinforcement learning can lead to social behaviors among robots and enhance their coordination and cooperation.\\n\\n3. Model-based learning involves agents building models of other agents through observations, allowing them to effectively learn and predict the actions of other agents.\\n\\n4. Reinforcement learning has been applied to market-based multi-agent systems, where agents learn to make optimal decisions in market environments.\\n\\n5. Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis through a method called consensus shuffling.\\n\\n6. Reinforcement learning principles can be applied to communication and persuasion techniques, such as understanding the power of association, appealing to self-interest, and using stories as simulation and inspiration.\\n\\nThese points highlight the versatility and potential applications of reinforcement learning in various domains. Let me know if there\\'s anything else I can assist you with!', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Thank you', additional_kwargs={}, example=False),\n",
       " AIMessage(content='The main points of our conversation are:\\n\\n1. Reinforcement learning has various use cases in different domains, such as multi-robot domains, model-based learning, market-based multi-agent systems, synthetic DNA synthesis, and communication and persuasion<sup><a href=\"https://blobstorageyhyufoqh6yjr6.blob.core.windows.net/arxivcs/0001/0001008v2.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00:00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[1]</a></sup>.\\n\\n2. In multi-robot domains, reinforcement learning can lead to social behaviors among robots and enhance their coordination and cooperation.\\n\\n3. Model-based learning involves agents building models of other agents through observations, allowing them to effectively learn and predict the actions of other agents.\\n\\n4. Reinforcement learning has been applied to market-based multi-agent systems, where agents learn to make optimal decisions in market environments.\\n\\n5. Reinforcement learning has been used to significantly reduce random errors in synthetic DNA synthesis through a method called consensus shuffling<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1072806/?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00%3A00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[2]</a></sup>.\\n\\n6. Reinforcement learning principles can be applied to communication and persuasion techniques, such as understanding the power of association, appealing to self-interest, and using stories as simulation and inspiration<sup><a href=\"https://demodatasetsp.blob.core.windows.net/books/Made_To_Stick.pdf?sv=2021-10-04&ss=btqf&srt=sco&st=2023-09-03T12%3A36%3A56Z&se=2030-09-04T10%3A00:00Z&sp=rl&sig=OK69n%2FVXw0MGQ%2BPPOXDLO75l5r21vymd40w1pOOu4kk%3D\">[3]</a></sup>.\\n\\nThese points highlight the versatility and potential applications of reinforcement learning in various domains. Let me know if there\\'s anything else I can assist you with!', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load message from cosmosdb\n",
    "cosmos.load_messages()\n",
    "cosmos.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
   "metadata": {},
   "source": [
    "![CosmosDB Memory](./images/cosmos-chathistory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
    "\n",
    "We added persitent memory using CosmosDB.\n",
    "\n",
    "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, regardless of the input and it struggles to respond to prompts like: Hello, Thank you, Bye, What's your name, What's the weather and any other task that is not search in the knowledge base.\n",
    "\n",
    "\n",
    "## <u>Important Note</u>:<br>\n",
    "As we proceed, while all the code will remain compatible with GPT-3.5 models, we highly recommend transitioning to GPT-4. Here's why:\n",
    "\n",
    "**GPT-3.5-Turbo** can be likened to a 7-year-old child. You can provide it with concise instructions, but it frequently struggles to follow them accurately. Additionally, its limited memory can make sustained conversations challenging.\n",
    "\n",
    "**GPT-3.5-Turbo-16k** resembles the same 7-year-old, but with an increased attention span for longer instructions. However, it still faces difficulties accurately executing them about half the time.\n",
    "\n",
    "**GPT-4** exhibits the capabilities of a 10-12-year-old child. It possesses enhanced reasoning skills and more consistently adheres to instructions. While its memory retention for instructions is moderate, it excels at following them.\n",
    "\n",
    "**GPT-4-32k** is akin to the 10-12-year-old child with an extended memory. It comprehends lengthy sets of instructions and engages in meaningful conversations. Thanks to its robust memory, it offers detailed responses.\n",
    "\n",
    "Understanding this analogy above will become clearer as you complete the final notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
    "\n",
    "But, does this solve all the possible scenarios that a virtual assistant will require?  **What about if the answer to the Smart Search Engine is not related to text, but instead requires to look into tabular data?** The next notebook explains and solves the tabular problem and the concept of Agents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivbd",
   "language": "python",
   "name": "openaivbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
